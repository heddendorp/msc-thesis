Automatically Detecting Flaky End-to-End Tests in Multi-Language Systems Using Code Coverage

Comment: Actually should be Detecting Flaky Failures of End-to-End Tests as what is really detected is if a specific failure is flaky and not only the test that failed.

Context
When testing software, we assume that a test passes if the code under test is free from faults
for the input defined in the test and assume that the test fails if not. In other words, we expect a
test to be deterministic, which doesnâ€™t always hold true. This kind of tests, that fail inconsistently
without changes to the code under test or the test itself are called flaky tests [1 ]. According to a
keynote from 2017, almost 16% of all tests at Google are flaky and they spend between 2 and
16% of their compute resources only on re-running flaky tests [2].
The problem of flakiness is particularly significant in User Interface (UI) tests because (1)
these types of tests are often quite large and (2) involve many highly asynchronous actions like
handling user input and downloading multiple resources required by the interface [ 3]. Although
this type of testing is so problematic, it has not been well researched to date.
Developers of the interactive learning platform Artemis recently complain about frequent
flickering of UI tests. As Artemis is a non-commercial project, resources for Continuous
Integration (CI) are limited, so the standard approach of rerunning failing flaky tests in the
CI is not appropriate. Also, it is assumed that the limited computing power in combination
with the asynchronous nature of UI tests is the reason for the high amount of test flickerings.
Nevertheless, computing power is available at night and on weekends for the analysis of test
flakiness.
Artemis uses the popular tech stack with Java on the backend and JavaScript on the
frontend, making it an ideal subject of study for research in this field.

Comment: During the working time of the thesis it showed that Artemis and their CI setup are too unstable to be used for bigger evaluations.
A second open source project, n8n was considered, this project uses typescript on the backend and also JavaScript on the frontend.

Goal
The goal of this thesis is (1) to build a tool to collect coverage information for end-to-end tests
and (2) to decide if failing tests could be flaky based on the new information and changes.
For part 1, a solution has to be found that allows collecting coverage for e2e tests across
both the client and server part of Artemis. For this, first existing solutions will be considered
and if they should not be usable for our problem, the chairs new approach will be used.

Comment: It turns out that existing solutions are mostly not suitable as they don't collect coverage on a per-test basis.
Some existing solutions could be used but they had to be customized some more. The situation was easier when it came to n8n as the node debugging protocol
was available on this server and could be used in a similar way as the frontend.

In order to be able to make statements about the scope of the problem for part 2, the
tests for Artemis will then be extended with the coverage collection solution. After coverage is
collected, it can be compared to the changes that triggered the test and an estimation can be
given if test fails are due to flakiness [4 ]. If a test only covers files that where not changed and
passed before, it is likely that the test is flaky and fails nondeterministically. The quality of the
estimation can then be evaluated by running commits with test fails multiple times to see if the
fail was flaky and comparing the result to our estimation.

Comment: This approach was followed for n8n but due to issues with Artemis, not as much data is available.

Working Plan
1. Research: Review the literature about flaky tests.
2. Research: Learn about Cypress1.
3. Implementation: Setup Artemis locally and run all Cypress tests. (Due to the complexity of the setup, this never fully worked)
4. Implementation: Build a solution to collect coverage information for E2E tests.
4. Implementation: Integrate the solution with the existing CI pipeline.
5. Implementation: Compare coverage information with latest git changes.
6. Evaluation: Identify relevant commits in Artemis by browsing commits and issues. (Here, random selections from the runs of the develop branch where used, as well as new
runs that happened during evaluation.)
7. Evaluation: Apply the tool on relevant commits in Artemis (This happened as part of the CI pipeline).
8. Evaluation: Compare the results to state of the art literature and developer voices to be
able to make a statement about its correctness (correctness is largely estimated by comparing to the results of a run without the flaky failure recognition).
9. Evaluation: Identify limitations of the approach and document them.

Experiences from the project:
The initial approach was to collect coverage on file level and compare that to the changes on file level since the last time all tests passed.
As this approach did not work too well, a decision was made to move to line level coverage.
Sadly, there were two issues with the new implementation that worked on line level. One: not caused by me, the path prefix of the files changed
and the comparison with changed files could never find any files in the backend. Second: I made a mistake in which I did not check if a changed and covered line was also
in the same file, leading to way too many hits.
Due to those two errors, large parts of the evaluation had to be discarded. After finding out, there were multiple problems with the artemis CI that lead to me pursuing a new project.
The n8n project is open source, in active development and has many cypress tests. It took a while to instrument the project properly and get the tests to run in a forked repository.
After getting the instrumented version to run it showed that on github hosted runners, the tests would cause a crash of the testing browser (electron) due to memory issues.
In order to mitigate this, I got a dedicated runner from the chair and ran the tests there. As this did not fix all issues, I also moved to chrome for test execution and
excluded the tests that did not run properly now.

Current Evaluation plan:
I am working on establishing a ground truth of test results for n8n to assess how well the flaky failure detection works. For this I am running the latest five
commits of the latest 30 open pull requests up to five times and collect the results.
Additionally I am running the tests with and without instrumentation on the dedicated runner to see if the instrumentation has significant impact on the failure rate of tests
and the time it takes to execute them.
Next to this, the instumented version of the artemis tests is still running in regular development and I hope to gain some insights into the behavior of the project there.