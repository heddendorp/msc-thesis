% !BIB TS-program = biber

\RequirePackage[l2tabu,orthodox]{nag}

% Add common preamble to the document
\input{common/preamble.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Theses specific packages go here
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[printonlyused]{acronym}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Begin of document
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
% \setlength{\evensidemargin}{22pt}
% \setlength{\oddsidemargin}{22pt}


% \hypersetup{pdfborder={0 0 0}, pdfauthor={\author}, pdftitle={\title}}

\pagenumbering{alph}

%------- Cover and Title setup -------
\include{common/cover}
\frontmatter{}
\include{common/titlepage}

%------- Disclaimer -------
\include{thesis_tex/disclaimer}

%------- Acknowledgements -------
% \newpage
% \thispagestyle{empty}
% \mbox{}
% \include{thesis_tex/acknowledgement}

% \pagenumbering{roman}

%------- Abstracts -------
% \selectlanguage{english}
\include{thesis_tex/abstract_en}
\clearpage
% \selectlanguage{german}
% \include{thesis_tex/abstract_de}
% \clearpage
% \selectlanguage{english}

%------- Table of contents -------
\microtypesetup{protrusion=false}
\tableofcontents{}
\microtypesetup{protrusion=true}

%------- List of todos -------
\listoftodos

\section*{Proofreading info}
\TODO{
	As the evaluation for \textsc{n8n} is still ongoing, the results of RQ\textsubscript{1} are not yet available. This also impacts the discussion.\\
	The thesis is mostly complete content wise, but be on the lookout for cases in which a concept surprises you. Occasionally there are sections in which a concept is mentioned but was not properly introduced yet.\\
	Also, be on the lookout for section that seem too short and should be expanded for better understanding.\\
	Don't worry about tables and figures not being in the right place. They will be moved around during the final formatting.
}
\TODO{Check for \textbf{bugs, defects} and other words used instead of (flaky) \textbf{failures} as some tools for text improvements love to make sure that every word is used only once.}
\feedback{Manchmal schreibst du UI E2E (oder E2E UI) tests.Finde sowohl E2E tests als auch UI tests ok, sollte aber konsistent sein. \\ Decision: use \textbf{\acs{e2e} tests}}

\mainmatter{}

%------- chapter 1 -------
\chapter{Introduction}\label[chapter]{introduction}
Software testing is a critical component of the software development process that ensures the quality, reliability, and overall performance of a system.
By executing test cases, the primary goal of software testing is to identify and fix software defects.
Developers can use test cases to verify that the system behaves as expected and to ensure that changes to the system do not introduce new defects.
This saves time on manual testing, prevents bugs from reaching production, and increases confidence in the system.
Ideally, test cases should be deterministic, that is, they should always produce the same output given the same input.
In practice, however, test cases may be non-deterministic or \emph{flaky}, \autocite{luo_empirical_2014} meaning that they may fail intermittently without any change to the \ac{sut} or the test itself.
\Ac{ui} tests, which often involve asynchronous activities and limited computer resources, are particularly susceptible to flaky behavior \autocite{romano_empirical_2021}.

Flaky tests pose a significant challenge to the software development process because they consume valuable resources, increase development time, and undermine confidence in the testing process.
In addition, flaky tests can cause delays in the development process as developers spend effort investigating false alarms resulting from these tests.
The prevalence of this problem is evident in industry reports, with Google stating that approximately 16\% of their tests exhibit some degree of flakiness \autocite{micco_state_2017}.
In a survey conducted by \citeauthor*{eck_understanding_2019}, a striking 79\% of respondents characterized flaky tests as a \enquote{moderate to serious} concern \autocite{eck_understanding_2019}.

Despite the recognized importance of addressing flaky tests, detecting them remains a major challenge.
The most common method of identifying flaky tests is to rerun them \autocite{lam_idflakies_2019, lam_understanding_2020}.
However, this approach is not without its drawbacks, as it consumes significant computational resources and may still miss certain flaky tests \autocite{bell_deflaker_2018, luo_empirical_2014}.
In addition, \citeauthor*{parry_what_2022} found that rerunning tests detected only 40\% of flaky tests \autocite{parry_what_2022}.
These findings highlight the need for more effective strategies to identify and address flaky tests in order to improve the efficiency and reliability of the software development process.

It is important to distinguish between flaky tests and flaky test failures.
A flaky failure refers to a test failure that does not manifest in every run, but rather occurs by chance.
Consequently, a flaky test is a test that exhibits flaky failures.
While the majority of research has focused on the detection of flaky tests, the approach presented in this thesis is aimed at the detection of flaky failures.

\Citeauthor*{haben_importance_2023} discovered that an alarming 76.2\% of all regression failures, which are new bugs introduced by code changes in previously working system components, are missed when failures of tests considered flaky are ignored \autocite{haben_importance_2023}.
Furthermore, they found that these tests detect approximately one-third of all regression faults \autocite{haben_importance_2023}.
As a result, the practical utility of test defect detectors is limited, as they can only guide developers to tests that need improvement, but provide little value in day-to-day development activities.
To better support the development process and improve the \ac{ci} pipeline, our goal is to detect flaky failures rather than simply identifying flaky tests.
This approach will provide more actionable insights to developers, enabling them to gain insight into each failure and address it accordingly.

To address the issue of flaky failures in \ac{e2e} tests, we adapt a technique originally developed by \citeauthor*{bell_deflaker_2018} for detecting flaky Java unit test failures in \citetitle{bell_deflaker_2018}. \autocite{bell_deflaker_2018}.
They discovered that it is possible to determine whether a failure is flaky by comparing the code covered during test execution to the changes made since the test was last successfully executed.
If the covered code remains unchanged, the failure is likely flaky.
Applying this approach to \ac{e2e} tests requires the instrumentation of code to collect coverage data during test execution.
Because \ac{e2e} tests span the entire \ac{sut} and involve multiple languages, we cannot selectively instrument code as \citeauthor*{bell_deflaker_2018} did.
In addition, we must adapt our collection approach because \ac{e2e} tests do not run in the same context as the \ac{sut}.

To evaluate our methodology, we selected two open source projects as study subjects:
\textsc{ArTEMiS} \autocite{krusche_artemis_2018}, a web-based learning platform for programming exercises that uses Java for server code and JavaScript for client-side code, and \textsc{n8n} \autocite{noauthor_n8n_2023}, a workflow automation tool written in TypeScript for both server and client.
Both projects use \textit{cypress} \autocite{noauthor_cypress-iocypress_2023} for \ac{e2e} testing and have previously reported flaky failures.
We instrumented the test-runners of these projects to collect code coverage data and analyze failures within the \ac{ci} pipeline.

The contributions of this thesis are
\begin{itemize}
	\item \textbf{Methodology.} We propose an approach using change coverage for detecting flaky failures in \ac{e2e} testing, based on previous work in detecting flaky unit test failures \autocite{bell_deflaker_2018}.
	\item \textbf{Study Subjects.} We evaluate our approach on two open-source projects by analyzing our method's impact on execution and evaluating the performance of our approach.
	      During the evaluation, we found that instrumentation can significantly impact test execution, increasing the execution time.
	      Our approach correctly identified flaky failures 42\% of the time \todo{correct number}.
	      During the evaluation, we saw three \todo{Correct number} false positives where our approach identified a failure as flaky when it was not.
	\item \textbf{Guidelines.} Based on our experiences, we derive guidelines for practitioners, these are
	      (1) change coverage-based detection shows very few false positives in our experiments,
	      (2) large spec files combined with instrumentation cause memory issues, and
	      (3) instrumentation can significantly impact the runtime overhead of test execution.
\end{itemize}

The rest of this thesis is organized as follows:
In \cref{background}, we give some background information on \ac{e2e} testing, flaky tests and failures, and coverage collection.
In \cref{related_work}, we review related work.
Our methodology, including instrumentation, data collection, and environment setup, is described in \cref{methodology}.
In \cref{evaluation}, we present the results of our study subjects and answer our research questions.
We discuss the limitations of our approach in \cref{threats} and summarize our findings and potential future work in \cref{conclusion}.



%------- chapter 2 -------

\chapter{Background}\label[chapter]{background}

This chapter provides essential background information for understanding the context of this thesis.
We introduce the concept of \ac{e2e} testing, explain flaky test failures and their causes, and give an overview of coverage collection.

\section{E2E Testing}

When developing software applications, ensuring that the application behaves as expected is essential.
Automated testing is used to ease the manual work of going through the system after changes and checking that everything still works.
Failed tests can indicate regressions in the application, meaning that parts of the application do not behave as expected \autocite{lam_large-scale_2020, luo_empirical_2014,romano_empirical_2021}.

There are several types of tests used in software development.
From \emph{unit tests}, which test small parts of the application in isolation, to \emph{integration tests}, which test the interaction between several application components.
Both of these testing types are testing technical aspects of the application.
\emph{\ac{e2e} tests} are used to test the application as a whole from the users point of view, simulating the interaction of a user with the application \autocite{jacob_schmitt_what_2022}.

\Ac{e2e} tests also include the \ac{ui} of the application.
They simulate a user's interaction with the application and check if the application behaves as expected.
\ac{e2e} tests for web applications are usually written to run in a browser, simulating a user's interaction with the application.
The interactions are realized using an \ac{ui} testing framework that simulates user interaction with the application.
This thesis focuses on \ac{e2e} tests executed in a browser.


\section{Flaky Tests and Failures}

Following the definition of \citeauthor*{luo_empirical_2014} \autocite{luo_empirical_2014}, a test is flaky if its \enquote{outcome is not deterministic with respect to a given software version} \autocite{luo_empirical_2014}.
Based on this definition, we define flaky failures.
In this thesis, we will only mention flaky failures, but their existence also means that the corresponding test is flaky.
\begin{definition}[Flaky Failure]
	\label[definition]{flaky_failure}
	A \textbf{flaky failure} is a test failure that occurs non-deterministically for a particular version of the \ac{sut}.
	The test may pass for the same version of the \ac{sut}.
\end{definition}

Failures of this type are problematic because they are difficult to reproduce due to their non-deterministic nature.
They can also cause developers to waste time identifying a bug in the \ac{sut} that does not exist \autocite{ziftci_-flake_2020}.
Flaky failures are common in software testing, \citeauthor*{harman_start-ups_2018} even suggesting that all tests may have flaky failures \autocite{harman_start-ups_2018}.
A survey of developers at Pivotal by \citeauthor*{hilton_trade-offs_2017} found that about 50\% of \ac{ci} builds fail due to flaky failures \autocite{hilton_trade-offs_2017}.

Due to the asynchronous nature of \ac{ui} interactions, many events and tasks are executed in a non-deterministic order.
When opening a website, many requests are sent at the same time to load the page's resources.
Therefore, flaky \ac{ui} test failures are more challenging to reproduce than flaky unit test failures.
\Citeauthor*{romano_empirical_2021} found common causes for flaky failures based on the work of \citeauthor*{luo_empirical_2014}, but extended for \ac{ui} tests \autocite{luo_empirical_2014,romano_empirical_2021}.

\begin{itemize}
	\item \textbf{Timing Issues.} Responsible for about 45\% of all flakiness. The common root cause is timing issues, where tests do not schedule tasks properly.
	      Timing issues can occur, for example, when the test tries to interact with the \ac{ui} before it is ready to \autocite{romano_empirical_2021}.
	\item \textbf{Environment.} Environment problems can occur when the test is run in different environments.
	      Environment problems include issues with the \ac{ui} rendering differently on different screen sizes \autocite{romano_empirical_2021}.
	\item \textbf{Test Runner \acs{api}.} Issues in this category stem from misusing the test runner \ac{api}, causing unexpected behavior.
	      Another common issue is \ac{dom} selector issues. For example, a selector may not work if stale elements block the focus \ac{ui} \autocite{romano_empirical_2021}.
	\item \textbf{Test Script Logic.} This category of issues describes problems in the test logic, such as the misuse of a random data generator or data leaks between tests \autocite{romano_empirical_2021}.
\end{itemize}


\section{Code Coverage}

Code coverage is a record of what code was executed by a test suite.
A test suite is a collection of tests that are executed together to test the system.
It is often used to identify parts of the code not executed by the test suite \autocite{marick_brian_and_smith_john_and_jones_mark_how_1999}.
These identified parts of the code can be a good start for the development of new tests to ensure the test suite covers the entire system.
It is also sometimes misused as a measure of the quality of a test suite \autocite{martin_fowler_bliki_2012}.
However, \citeauthor*{martin_fowler_bliki_2012} also suggests that the quality of a test suite can be measured by the number of bugs that make it into production and willingness of developers changing code that could lead to bugs \autocite{martin_fowler_bliki_2012}.
\Citeauthor*{kochhar_code_2017} even show that code coverage has no significant correlation with the fault detection rate of a test suite \autocite{kochhar_code_2017}.
That is, a test suite with a high code coverage does not necessarily detect more bugs than a test suite with a low code coverage.

Code coverage is usually measured in terms of \emph{lines} or \emph{branches} covered by the test suite.
A line is considered covered if it is executed by the test suite at least once.
Coverage data is also used for regression test selection to reduce the number of tests to run for a code change \autocite{rothermel_empirical_1998}.
The idea is to only run tests that cover the changed code.
This limits the time needed to test the system while still ensuring that the changed code is tested.
\Citeauthor*{bell_deflaker_2018} use code coverage to identify flaky failures in unit tests \autocite{bell_deflaker_2018}.

%------- chapter 3 -------

\chapter{Related Work}\label[chapter]{related_work}
This chapter provides an overview of existing work related to this thesis.
First, we show some approaches to detecting flaky tests.
Then, we motivate why it is important to detect flaky failures, not just to determine whether a test is flaky.
Then, we will present existing work on flaky failure detection and the effect of instrumentation on flakiness.
Finally, we present the research gap that this thesis aims to fill.

\section{Approaches for Flaky Test Detection}
The simplest and most common way to detect flaky failures is to rerun the test on a given version of \ac{sut} multiple times.
The idea is that if the test is prone to flaky behavior, it will fail on some reruns but pass on others.
The ideal number of reruns is unclear \autocite{parry_survey_2021}, and practitioners must decide how many reruns to perform.
Some suggest using up to five reruns \autocite{lam_understanding_2020} to confirm a failure is not flaky or as many as 10,000 \autocite{alshammari_flakeflagger_2021} to make sure a test does not show flaky failures.

\Citeauthor*{silva_shake_2020} propose a lightweight approach to provoke flaky behavior in tests called \textsc{SHAKER}.
\textsc{SHAKER} adds noise in the execution environment (i.e., adding stressor tasks to compete for the CPU or memory).
It builds on the observations that concurrency is an important source of flakiness and that adding noise in the environment can interfere with the ordering of events and, consequently, influence the test outputs.
They report better and faster detection than rerunning and also found additional flaky tests \autocite{silva_shake_2020}.

\Citeauthor*{lam_idflakies_2019} introduce a framework called \textsc{iDFlakies} that uses a dynamic analysis approach to detect flaky tests.
The framework automates experimentation in Maven-based Java projects.
It uses a combination of test reruns and reordering of tests to detect flakiness.
Additionally, it classifies found flakes into \ac{od} and \ac{nod} flaky tests.
They report that \textsc{iDFlakies} that both classes occur at about the same rate, but note that their approach is very time and resource-consuming \autocite{lam_idflakies_2019}.

\Citeauthor*{alshammari_flakeflagger_2021} present a tool called \textsc{FlakeFlagger} predicts if tests are likely flaky without rerunning.
They present an approach that collects a set of features describing the behavior of each test and then predicts tests that are likely to be flaky based on similar behavioral features.
The authors found that \textsc{FlakeFlagger} correctly labeled as flaky on at least as many tests as a state-of-the-art flaky test classifier but that \textsc{FlakeFlagger} reported far fewer false positives.
Evaluated on their dataset of 23 projects with flaky tests, \textsc{FlakeFlagger} outperformed the prior approach (by F1 score) on 16 projects and tied on four projects.
Their results indicate that this approach can effectively identify likely flaky tests before running time-consuming flaky test detectors \autocite{alshammari_flakeflagger_2021}.
While \textsc{FlakeFlagger} can analyze a test without reruns, it is not able to categorize a single failure of a test as flaky or not.
This weakness is shared by the other mentioned approaches as well.
But as we motivate in the following section, detecting flaky failures is an essential feature for an approach that adds value in a \ac{ci} environment.

\section{Importance of Detecting Flaky Failures}
\Citeauthor*{haben_importance_2023} have studied the difference between detecting flaky tests and flaky test failures, highlighting the importance of the latter for a more effective \ac{ci} process.
In recent years, several methods have been proposed to identify flaky tests.
However, these methods have not been evaluated within a \ac{ci} process, leaving their actual utility unclear \autocite{haben_importance_2023}.

\Citeauthor*{haben_importance_2023} conducted a case study on the Chromium \ac{ci}, using state-of-the-art flakiness prediction methods to examine their performance.
They found that despite the high precision of the methods (\SI{99.2}{\percent}), their application resulted in many missed failures, which were about \SI{76.2}{\percent} of all regression failures (\SI{56.2}{\percent} of which were due to model misclassifications).
To investigate this finding, the authors analyzed the fault-triggering failures. They discovered that flaky tests have a strong fault-revealing capability, revealing over one-third of all regression faults \autocite{haben_importance_2023}.

This result highlights an inherent limitation of methods that focus on identifying flaky tests rather than flaky test failures.
To address this issue, \Citeauthor*{haben_importance_2023} proposed failure-focused prediction methods and optimized them by considering new features.
The new features included test execution characteristics such as duration and history.
Interestingly, they found that these methods performed better than the test-focused ones, with the \ac{mcc} increasing from \num{0.20} to \num{0.42} \autocite{haben_importance_2023}.

Overall, \citeauthor*{haben_importance_2023}'s study suggests that future research should focus on predicting flaky test failures rather than flaky tests.
In addition, it emphasizes the need for more thorough experimental methods when evaluating flakiness prediction methods \autocite{haben_importance_2023}.
This finding underscores the importance of distinguishing flaky from fault-triggering test failures to improve the efficiency of \ac{ci} processes and minimize the waste of valuable developer time investigating false alarms caused by flaky test failures.

\Citeauthor*{rasheed_test_2022} reviewed the literature on flaky test detection and found that most tools can only detect flaky tests, not flaky test failures.
They found that the most common techniques are either static analysis of the test code or dynamic analysis of the test execution, where the tool attempts to trigger flaky behavior \autocite{rasheed_test_2022}.
\section{Change Coverage Based Flaky Failure Detection}\label[section]{related_deflaker}

\Citeauthor*{bell_deflaker_2018} have addressed the problem of flaky test failures in Java unit tests and proposed a method to detect flaky tests without rerunning them \autocite{bell_deflaker_2018}.

To address this problem, the authors propose a novel technique based on change coverage that detects flaky failures without rerunning them and with minimal runtime overhead.
The technique works by monitoring the coverage of the latest code changes and identifying any new test failures that did not execute the changes as flaky.
This approach significantly reduces the time and resources required to detect flakiness, resulting in a more efficient development cycle \autocite{bell_deflaker_2018}.

The effectiveness was evaluated on 26 Java projects hosted on TravisCI, where it identified 91 previously unknown flaky failures.
In addition, the authors conducted experiments on project histories, where they detected 1,874 flaky failures out of a total of 4,846 failures.
Moreover, they observed a low false alarm rate of only 1.5\%.
The detector also demonstrated superior performance compared to Maven's default flaky test detector, with a 95.5\% recall rate compared to Maven's 23\%.
They also demonstrated great runtime performance as the runtime of tests is only 4.5\% higher on average with their detection active \autocite{bell_deflaker_2018}.

\section{Effect of Instrumentation on Flakiness}
The approach presented by \citeauthor*{bell_deflaker_2018} is based on the instrumentation of the code.
As instrumentation can affect the code's behavior, \citeauthor*{rasheed_effect_2023} investigated the effect of instrumentation on flakiness \autocite{rasheed_effect_2023}.
By running the identical test suites without and with different instrumentation agents, they collected data about the impact of instrumentation on flakiness.
They did not find any significant effect of instrumentation on the flakiness score of their study subjects \autocite{rasheed_effect_2023}.


\section{Research Gap}

Considering the importance of detecting flaky failures instead of flaky tests \autocite{haben_importance_2023}, the approach presented by \citeauthor*{bell_deflaker_2018} shows great promise for Java unit tests \autocite{bell_deflaker_2018}.
Flaky failures are more common in \ac{e2e} tests due to the high complexity of the test interactions \autocite{romano_empirical_2021}.
Additionally, reruns are more expensive for \ac{e2e} tests as they typically take much longer to execute than unit tests.
Developers could benefit greatly from immediate and reliable feedback on any failures encountered.
This sentiment is also shared by \citeauthor*{ngo_research_2023} who found that most academic studies only concern themselves with unit tests while the industry interest is focused on \ac{e2e} tests \autocite{ngo_research_2023}.
In this thesis we adopt the approach of \citeauthor*{bell_deflaker_2018} to the \ac{e2e} tests of two open source web projects, \textsc{ArTEMiS} and \textsc{n8n}.
Both projects have client and server-side code for which we collect coverage.
We will evaluate the performance of the detection and the effect of instrumentation on the test behavior.

%------- chapter 4 -------

\chapter{Methodology}\label[chapter]{methodology}
Based on the work by \citeauthor*{bell_deflaker_2018} we want to collect coverage for the executed \ac{e2e} tests.
We want to know which parts of the code of the \ac{sut} were executed by what test.
If a test fails, we can check for the last version of the \ac{sut} for which the tests passed.
By comparing the code that changed since the last time the tests passed and the code for which we know that the failing test executed it, we can judge if the failure is likely to be flaky.
If there is no interception of the code that was covered by the failing test execution and the code that changed, the failure is assumed to be flaky.
\Cref{detection_process} shows the process of detecting flaky failures.

\begin{figure}[h]
	\begin{adjustbox}{max width=\textwidth}
		\begin{tikzpicture}[
				node distance=2cm,
				block/.style={rectangle, rounded corners, draw, minimum width=3cm, minimum height=1cm, align=center},
				arrow/.style={thick, ->, >=stealth}
			]

			\node[block, draw=none] (tests) {
				\includegraphics[width=1cm]{icons8-code-file}
				\\ \ac{e2e} Tests};
			\node[block, shape=rectangle split, rectangle split parts=2, below=of tests, align=center] (runner) {Test Exectution \nodepart{two} Coverage collection};
			\node[block, shape=rectangle split, rectangle split parts=2, right=of runner, align=center] (results) { Test Results\nodepart{two} Coverage Information};

			\node[block, right=of tests, draw=none] (vcs) {
				\includegraphics[width=1cm]{icons8-cloud-database}
				\\ \ac{vcs}};
			\node[block, right=of vcs] (passing-version) {
				Last passing version};
			\node[block] at ($(passing-version.north)!(results.center)!(passing-version.south)$) (changes) {
				Changes compared to\\ tested version};

			\node[block, below=of $(results)!0.5!(changes)$] (comparison) {
				Analysis of changes\\ and coverage};
			\node[block, left=of comparison] (flaky) {
				Flaky failure?};

			\draw[arrow] (tests) -- (runner);
			\draw[arrow] (runner) -- (results);
			\draw[arrow] (vcs) -- (passing-version);
			\draw[arrow] (passing-version) -- (changes);
			\draw[arrow] (results) -| (comparison);
			\draw[arrow] (changes) -| (comparison);
			\draw[arrow] (comparison) -- (flaky);

		\end{tikzpicture}
	\end{adjustbox}
	\caption{Process to determine if a test failure is flaky}
	\label[figure]{detection_process}
\end{figure}

To achieve this, we need to know what code was executed by a test and what code changed since the last time the tests passed.
In this chapter we explain the coverage and change collection and also the implementation of our approach.
Different from \citeauthor*{bell_deflaker_2018}, we face new challenges as we have to collect coverage across multiple processes and multiple programming languages.
This means, our implementation has to be tailored to the studied project and can rely on a standardized framework.
Also, \ac{e2e} tests have longer runtimes and bigger scopes than unit tests, which can impact the runtime overhead introduced by our approach and the detection performance. \todo{improve sentence}

\section{Coverage Collection}
\Citeauthor*{bell_deflaker_2018} use differential coverage in their approach.
That is, they analyze the changes made to the code and instrument only the parts of the \ac{sut} that were changed.
They analyze each change and decide how to track coverage for that particular change by analyzing the code \autocite{bell_deflaker_2018}.
Because our approach works across the entire stack, the changes detected can span multiple languages and be very complex.
Implementing differential coverage for each specific setup was not feasible within the scope of this thesis.
Instead, we collect coverage for all code executed by any test case, which saves the time of analyzing the changes to perform precise instrumentation and is applicable to more setups.

The experiments of \citeauthor*{bell_deflaker_2018} were performed on Java projects, where they also checked the runtime overhead of the JaCoCo instrumentation agent.
They found that the effect of the JaCoCo agent was much larger (\SI{32.9}{\percent} on average) than the effect of their differential coverage (\SI{4.5}{\percent} on average) \autocite{bell_deflaker_2018}.
Since we use JaCoCo for coverage collection in the \textsc{ArTEMiS} project, we expect the runtime overhead of our approach to be greater than the overhead of differential coverage.

To analyze the result of a test execution, it is necessary to know which parts of the code were executed by which test case.
Due to limitations in cypress, we can only collect coverage data per spec file, not per individual test case.
A spec file, short for specification, is meant to hold multiple tests that concern the same area of the \ac{sut} \autocite{noauthor_writing_nodate}.
Cypress only allows plugins to execute before and after running a spec file, but not before and after every single test case that is included.
We evaluated two approaches to full stack code coverage.

\paragraph{Propagation.} The first option is to propagate data about the executed test cases through the code base.
Since we are collecting coverage data on the entire code base, it is necessary to propagate the data from the test framework to both the client and server code.
In addition, the coverage collection must be reset after each test case to avoid mixing coverage data from different test cases.
This approach is similar to distributed tracing, were a trace ID has to be propagated through the code base to link all log entries to the same trace \autocite{noauthor_distributed_nodate}

\paragraph{Timing.} The second option is to match the coverage data to the test cases based on the timing of the test cases.
This more straightforward approach is possible because the test cases are executed sequentially, and the coverage data is collected in the same order.
Instead of propagating the test case information through the code base, we can map the coverage data to the test cases based on timing.
In this approach, coverage is collected and stored after each test case, and the collection is reset.
This is also suggested in cypress documentation \autocite{noauthor_code_nodate}.

To keep our approach more general, we used the second option and matched the coverage data to the test cases based on timing.
Mapping by timing is also more robust and does not require any changes to the code base.
Even parallel execution of tests is possible if each worker has a separate instance of the \ac{sut}.

A limitation of coverage collection is missing out on files that influence the behavior of the \ac{sut} but are not executed.
This can be CSS files to modify the styles of a web application or configuration files that are read by the application but not executed.f
This can lead to false positives, where a test failure is marked as flaky even though it is not.
\section{Change Collection}
In addition to collecting coverage that tells us which code was executed by the tests, we also need to collect changes to the codebase to determine which parts of the code have changed since the last successful test run.
With both sets we can analyze a test failure and determine if it is flaky.
We use git to collect changes because both study subjects use git as their version control system.
After identifying the last commit that passed the tests, we use the git \ac{cli} to collect the changes made between that commit and the commit that failed the tests.

\section{Implementation}
In this section, we describe the implementation of the coverage collection plugins for the two study subjects and the analysis program.
We register a plugin with cypress that starts the coverage collection before each spec file and saves the coverage data after the spec file in case the tests fail.
When a test case fails, the analysis program determines if the failure is flaky.
It is implemented as a \ac{cli} tool\footnote{The tool can be executed with the command \texttt{npx @heddendorp/coverage-git-compare}.}.

\subsection{Coverage Collection Plugins} \label[subsection]{coverage_collection_plugins}
We implemented coverage collection plugins for both study subjects, with specific adjustments to accommodate their different setups. This subsection discusses the implementation details for the Java server, the Node.js server, and the client.

Due to their different setups, the coverage collection plugins are implemented differently for the two study subjects.
The Java server uses the JaCoCo agent to collect coverage data.
The Node.js server uses the V8 coverage \ac{api} to collect coverage data.
Client coverage is collected using V8 coverage for both study subjects since both run their tests in chrome.
The plugins remap all file paths to be relative to the repository root and merge the client and server coverage information.

The cypress plugins are implemented as node modules that can be installed using npm.
They use the cypress plugin \ac{api} \autocite{cypressio_writing_nodate} to hook into the test runner.
Since cypress only provides a \texttt{before:spec} and \texttt{after:spec} hook, coverage can only be collected for each spec file, not for each test case.
Additionally, the plugins use the cypress \texttt{before:browser:launch} hook to identify the port of the chrome debugging protocol.
\subsubsection{Java Server}
Since \textsc{ArTEMiS} uses the JHipster \autocite{jhipster_jhipster_nodate} development platform, the server is implemented in Java.
We use the JaCoCo agent \autocite{noauthor_jacoco_nodate} to collect coverage data in server mode.
The server mode of JaCoCo allows the collection and reset of coverage data from the server without restarting.
To enable the JaCoCo agent, we use the \texttt{-javaagent} option of the java command.

On the \texttt{before:spec} hook, we request the JaCoCo agent to reset the coverage data.
The plugin checks if the spec passed on the \texttt{after:spec} hook.
If the spec passed, the coverage data is not collected because there is no analysis to do.
If the spec fails, the coverage data is collected in JaCoCo binary format.
After collecting the report, the plugin uses the JaCoCo \ac{cli} to transform the binary report into an XML report.
The XML report is transformed using the \textsc{jacoco-parse} library \autocite{noauthor_jacoco-parse_2023}\footnote{We maintain a fork of the library with security updates at \url{https://github.com/heddendorp/jacoco-parse} which is used in this project.}.
The plugin stores the covered lines for each file in a JSON file after merging them with the client's coverage data.

\subsubsection{Node.js Server}
Our second study subject, \textsc{n8n}, is a Node.js-based workflow automation tool.
Coverage in node.js programs is typically collected using the \emph{NYC} tool \autocite{noauthor_nyc_2023}.
NYC is a command-line tool that can collect coverage data for node.js applications.
However, NYC is unsuitable for our use case because it does not allow us to collect coverage data dynamically, because it does not expose an \ac{api} to collect coverage data.
Since we need to collect coverage data from the cypress test-runner after each test case, we decided not to use NYC.
The author of NYC also maintains C8 \autocite{noauthor_bcoec8_nodate}, a similar tool that uses native V8 coverage instead of code instrumentation.
Although C8 does not allow the dynamic collection of coverage data, it served as an inspiration for our implementation.

To collect coverage data we use the V8 native coverage to collect coverage data.
Collecting coverage using the V8 native coverage is the same as for the client described in the following section.
To enable the debugging protocol, we use the \texttt{--inspect} option of the node command.
\subsubsection{Client} \label[subsection]{client_coverage_collection}
The most common way to collect coverage on the client is to instrument the code.
Cypress recommends the \emph{istanbul} library to instrument the code \autocite{noauthor_code_nodate}, which is available via the NYC command line tool \autocite{noauthor_nyc_2023} or as a transpilation plugin.
We also evaluated other istanbul-based coverage collection methods \autocite{noauthor_teamscale_2023} but found that the instrumentation had a significant impact on the performance of the \ac{sut}.
Simply opening the \textsc{ArTEMiS} web application with instrumentation enabled resulted in approximately 6,000 web requests reporting coverage data.
Instead of following the usual approach, we used the native V8 coverage \ac{api} to collect coverage data.

Both study subjects run their tests in chrome.
This allows us to use V8 native coverage to collect coverage data.
The coverage data is available in the chrome developer tools \autocite{kayce_basques_coverage_2020}.
It is also accessible through the Chrome debugging protocol \autocite{noauthor_chrome_nodate}.
To connect to the chrome debugging protocol, we use the \emph{chrome-remote-interface} library \autocite{cardaci_chrome-remote-interface_2023}.
To connect to the debugging protocol, we need to know the port of the debugging protocol.
The port is set by cypress when it launches the browser and can be retrieved with the cypress \texttt{before:browser:launch} hook.
Then, before each spec run, we connect to the debugging protocol and enable coverage collection.
After the spec run completes, we check to see if the spec run failed.
If the spec run failed, we collect the coverage data and reset the coverage data.
If the spec run did not fail, we reset the coverage collection without retrieving it.

We transform the coverage data from the V8 format to the common istanbul format to work with the coverage data.
For this we use the \emph{v8-to-istanbul} library \autocite{noauthor_v8--istanbul_2023}.
Since the library needs the original files to transform the coverage, our plugin maps the paths of the files as reported by chrome to the correct paths in the file system.
The library reads the code files executed in the browser and follows the source maps to map the coverage data to the original source files.
This is necessary because the browser executes the transpiled code, not the original source files.
The coverage data is then simplified to a custom format containing only covered lines and files and saved as JSON.

\subsection{Analysis Program}
The analysis program is run when the tests have at least one failure.
In this subsection, we explain the process of retrieving change data from the git repository and comparing it to the coverage data to identify potentially broken tests.
Since our plugin saves the coverage data for each failed spec in a JSON file, the analysis program can be run after the test run.
A good example is the test script in ArTEMiS \texttt{npm ci \&\& npm run cypress:run || npm run detect:flakies}. The script runs the tests; if they fail, it runs the analysis program.
\cref{coverage_compose} contains the docker-compose file used to run the analysis program.

Since \textsc{ArTEMiS} runs \ac{ci} on bamboo, the analyzer is called with the current plan, build number, and an access token.
The analyzer then checks the bamboo \ac{api} for the latest build that has passed.
If the latest build that passed is not the previous build, the analyzer will fall back to a comparison with the \texttt{develop} branch.
The program can also be invoked in \texttt{compare} mode, passing the commit to compare against as an argument.

After identifying the commit to compare against, the program retrieves the change data from the git repository.
It uses the \texttt{git diff} command to get the changes between the two commits.
It then parses the diff using the \textsc{parse-diff} library \autocite{todyshev_parse-diff_2023}.
The program then extracts the changed lines for each file.
The collected changes are then compared to the coverage data.
The program checks whether the tests cover the changed lines for each spec file.
If the tests do not cover the changed lines, the spec is marked as potentially flaky.

%------- chapter 5 -------

\chapter{Evaluation}\label[chapter]{evaluation}
In this chapter, we put our approach outlined in \cref{methodology} to the test.
First, we describe the study subjects we use to evaluate our approach.
Then we present and discuss the results of our evaluation.
\section{Research Questions}
We want to understand if our adapted approach is effective for \ac{e2e} tests and if it has an effect on the behavior of the tests.
To investigate this, we formulate two research questions.
\begin{enumerate}
	\item[\textbf{\acs{rq}\textsubscript{1}:}] How effective is the change coverage based approach in identifying flaky failures in UI \ac{e2e} tests?

		To understand if our approach is effective at identifying flaky failures we check the recall and precision of our detector.
	\item[\textbf{\acs{rq}\textsubscript{2}:}] How does the instrumentation change the behavior of the \ac{sut} and tests?

		We expect that the instrumentation of the application will affect the behavior of the application and tests.
		Therefore, we further investigate this impact with two subquestions:
		\begin{enumerate}
			\item[\textbf{\acs{rq}\textsubscript{2.1}:}] To what extent does instrumentation affect the runtime overhead of the application during \acs{e2e} testing?

				To evaluate the effect of instrumentation on the runtime of the tests, we will measure the runtime of the tests with and without instrumentation.
			\item[\textbf{\acs{rq}\textsubscript{2.2}:}] How does the instrumentation of coverage collection affect the failure rate of \ac{e2e} tests in the studied projects?

				As the instrumentation could change the failure rate of the \ac{e2e} tests, we will compare the failure rates of both projects with and without instrumentation.
		\end{enumerate}
\end{enumerate}

\section{Study Subjects}
To validate our approach, we chose two study subjects.
Both study subjects are open-source projects that use \textsc{cypress} for \ac{e2e} testing.
They both have a high number of \ac{e2e} tests and are actively maintained.
Also, both projects have experienced issues with flaky tests in the past.

\textsc{ArTEMiS} is a learning management system used at several universities to manage courses and exercises \autocite{krusche_artemis_2018}.
The project is being developed at \ac{tum}, where we conducted our research.
The team has experienced problems with flaky tests in the past, and the project has numerous tests.
This makes it a good candidate for our evaluation.
We also wanted to evaluate our approach on a project that was in active development.

\textsc{n8n} is a workflow automation tool that allows users to connect different services and automate workflows \autocite{noauthor_n8n_2023}.
The project was identified as a study subject by searching GitHub for projects that depend on cypress and use JavaScript as their primary programming language.
The restriction to projects using cypress was set because our approach is only compatible with cypress.
We wanted to find another project that was in active development with plenty of tests.
Another check of the commit history showed that the project had experienced problems with flaky tests.

During the search for study subjects, we also considered \textsc{Gladys} \autocite{noauthor_gladys_2023} and \textsc{ToolJet} \autocite{noauthor_tooljettooljet_2023}. However, we decided against using them as study subjects because \textsc{Galdys} did not show any flakiness in the tests, and \textsc{ToolJet} uses a setup for testing that could not be replicated locally in a reasonable amount of time.

\section{Evaluation Setup}
In this section, we discuss the evaluation setup used for our experiments.
Our evaluation is designed to be part of the study subjects' existing \ac{ci} pipeline.
We use the existing pipeline to ensure that the evaluation is reproducible and that the results are gathered in an environment close to practical development.

\subsection{Evaluation Metrics}
In this section, we will introduce the metrics we use to evaluate our approach.

\paragraph{(True/False) Positives} Any test failure identified as flaky by our approach is considered a positive.
We define a \acfi{tp} as a case where our approach suspects a failure as being flaky, and we know that the tested version of the \ac{sut} has shown flakiness in the past for that test.
A \acfi{fp} is a case where our approach suspects a failure as being flaky, but we know that the tested version of the \ac{sut} has not shown flakiness in the past for that test.

\paragraph{(True/False) Negatives} Any test failure not identified as flaky by our approach is considered a negative.
We define a \acfi{tn} as a case where our approach does not suspect a failure as being flaky, and we know that the tested version of the \ac{sut} has not shown flakiness in the past for that test.
A \acfi{fn} is a case where our approach does not suspect a failure as being flaky, but we know that the tested version of the \ac{sut} has shown flakiness in the past for that test.
These classes are illustrated in \cref{confusion_matrix}.
\begin{figure}[ht]
	\centering
	\begin{tikzpicture}[
			basic/.style = {draw, text centered},
			block/.style = {basic, rectangle, minimum width=3cm, minimum height=1cm, fill=TUMAccentLightBlue, text width=4cm, align=center},
			title/.style = {text width=3cm, align=center}
		]

		% Blocks
		\node[block] (TP) {\acf{tp}};
		\node[block, right=of TP] (FP) {\acf{fp}};
		\node[block, below=of TP] (FN) {\acf{fn}};
		\node[block, right=of FN] (TN) {\acf{tn}};

		% Titles
		\node[title, above=of TP, yshift=-0.5cm] (ActualPositive) {Known Flaky};
		\node[title, above=of FP, yshift=-0.5cm] (ActualNegative) {Not Flaky};
		\node[title, left=of TP, anchor=east] (PredictedPositive) {Suspected Flaky};
		\node[title, left=of FN, anchor=east] (PredictedNegative) {Not suspected Flaky};

		% Border
		\node[draw, fit=(ActualPositive) (PredictedNegative) (TN), inner sep=1em] {};

	\end{tikzpicture}
	\caption{Confusion matrix}
	\label[figure]{confusion_matrix}
\end{figure}

\paragraph{Precision and Recall} The \emph{precision} describes the ratio of correctly identified flaky failures to all identified flaky failures (\ref{precision}).
The \emph{recall} describes the ratio of correctly identified flaky failures to all flaky failures (\ref{recall}).
\begin{equation}
	\text{precision} = \frac{\text{\acs{tp}}}{\text{\acs{tp}} + \text{\acs{fp}}}
	\label[equation]{precision}
\end{equation}
\begin{equation}
	\text{recall} = \frac{\text{\acs{tp}}}{\text{\acs{tp}} + \text{\acs{fn}}}
	\label[equation]{recall}
\end{equation}

\paragraph{F1 Score} The \emph{F1 score} is the harmonic mean of precision and recall (\ref{f1}).
It is used to summarize the performance of a binary classification test and is a value between 0 and 1.
\begin{equation}
	\text{F1 Score} = 2 \cdot \frac{\text{precision} \cdot \text{recall}}{\text{precision} + \text{recall}}
	\label[equation]{f1}
\end{equation}

\subsection{ArTEMiS (Bamboo)}
We start by describing the setup of the \textsc{ArTEMiS} \ac{ci} pipeline.
The \textsc{ArTEMiS} \ac{ci} pipeline is hosted on \textsc{Bamboo} \autocite{atlassian_bamboo_nodate} and consists of multiple plans.
The plans are triggered by commits to the \textsc{ArTEMiS} GitHub repository.
We created a second version of the \ac{e2e} test plan that included our instrumentation.
This plan is running in parallel to the original plan.

Additionally, we selected builds from the regular \ac{e2e} test plan executed before the instrumentation was added.
We reran these builds with our instrumentation to test our approach on historic builds.
Due to the complex nature of the \textsc{ArTEMiS} \ac{ci} pipeline and the project's ongoing development, we had to back port some changes to the historical builds.
While we could run some historic builds, ongoing issues with the \ac{ci} pipeline prevented us from running all historical builds.
Even though we could not use the historic builds for our evaluation, we did see that for the selected commits from the run history, the tests eventually passed for every version of the \ac{sut}.

The data we did use to evaluate our approach was collected from the live \ac{ci} pipeline.
We run the \ac{e2e} tests with and without instrumentation in parallel.
All run data collected during two months of active development comprise the live evaluation data set.
We compare the results of the two runs to determine the impact of the instrumentation on the \ac{e2e} tests.
Additionally, we can check the results of both runs to see if the approach correctly identifies failures as flaky.
If both plans fail, we check if the same tests failed in both runs.
If there is a difference in the failed tests, we can conclude that the instrumentation correctly identified the failure as flaky.

\subsubsection{Data Cleaning}
Since \textsc{ArTEMiS} is actively developing, the \ac{ci} pipeline is also changing.
During our evaluation, multiple changes to the \ac{ci} pipeline led to issues with test execution.
To improve the quality of our evaluation, we filtered out builds that were affected by these issues.
We removed any build that did not have information for passed or failed test cases.
Additionally, we filtered any failed build but reported no failed test cases.
These builds were not correctly run and were unusable for our evaluation.
Lastly, we filtered any build that took less than 20 minutes to run.
We removed these builds because they were likely affected by the issues with the \ac{ci} pipeline.
Even though we filtered out apparent cases of issues with the \ac{ci} pipeline, there were still some cases in which the \ac{ci} pipeline was affected.

\subsection{n8n (GitHub)} \label[section]{evaluation_n8n}

Next, we will discuss the setup of the evaluation of the \textsc{n8n} \ac{ci}.
The \textsc{n8n} \ac{ci} pipeline is hosted on \textsc{GitHub} and consists of several workflows.
We created a fork of the \textsc{n8n} repository to run the evaluation.\footnote{\url{https://github.com/heddendorp/n8n}}
We created a new workflow in the forked repository that runs the \ac{e2e} tests with and without instrumentation from a specific commit.
While our workflow still pulls from the original repository, we had to make some changes to the \ac{e2e} tests to make them work with our instrumentation.
We had to run the tests in chrome instead of electron due to memory issues when running the tests.
In addition, we had to exclude some tests that did not pass after three runs, even after changing the test setup.
To evaluate our approach and answer \textbf{\ac{rq}\textsubscript{1}}, we selected commits from the \textsc{n8n} repository and ran the \ac{e2e} tests against them to establish ground truth.
The commits were selected by taking up to five from the last 50 \acp{pr} that were last updated before \DTMdate{2023-03-20}.
We then ran the \acp{e2e} tests on each commit and collected the results.
If a test failed in one of the runs, we triggered up to four additional runs to see if the test could pass.
In addition, for each selected commit, we also checked to see if it had at least one successful parent.
If a commit did not have a successful parent, we extended the checked commits to include the parent commit.
If we found that even after five runs, the installation step for a commit did not complete successfully, we excluded that commit from our evaluation.
We repeated this process until we had a collection of commits with at least one successful parent.

To find candidates for our evaluation, we looked again at our original set, excluding commits that still needed to have a successful parent.
We also excluded commits that never passed completed the install step.
This selection yielded a set of 71 commits, 3 of which had test cases that failed on all runs.
In addition, 17 commits had broken test cases.
Each selected commit also includes a list of spec files that failed in every run so that the results of our approach can be verified.

To answer \textbf{\ac{rq}\textsubscript{2}}, we also ran another experiment.
We collected the 25 most recent commits from the \textsc{n8n} repository and ran the \ac{e2e} tests on them with and without instrumentation.
To increase our results' validity, we ran the original and instrumented tests on the same machine.
We collected the information from these runs to assess the impact of the instrumentation on the \ac{e2e} tests.

\section{Evaluation Results}\label[section]{results}
In this section, we present the quantitative results of our evaluation.
We also elaborate on any statistical tests we performed on the data to determine whether the results are significant.
For all tests, we use a significance level of $\alpha = 0.05$.
We will discuss the results and their implications in \cref{discussion}.
\subsection{Detection Quality}\label[subsection]{results_rq1}
To answer \textbf{\ac{rq}\textsubscript{1}}, the data from both study subjects were gathered and analyzed.

\begin{table}[h]
	\centering
	\begin{adjustbox}{width=\textwidth, totalheight=\textheight, keepaspectratio}
		\csvautobooktabular{data/evaluationResult.csv}
	\end{adjustbox}
	\caption{Results of running the evaluation for both study subjects}
	\label[table]{evaluation_results}
\end{table}

As we were unable to run historic versions of the \textsc{ArTEMiS} project multiple times to establish ground truth for which versions showed flaky test failures, we only analyzed the data gathered during the live evaluation.
As we had no reliable labels for the runs observed in the live evaluation, we adapted to the limited data that was available to check the detection quality.
We defined \emph{true positives} as versions of the \ac{sut} for which the instrumented run failed, was labeled \emph{suspected flaky}, and the non-instrumented run passed, or there was no overlap in the failed test cases.
In addition, we defined \emph{true negatives} as versions of the \ac{sut} for which both runs failed and the failed tests matched for at least 75\% of the tests.
These definitions lead to overestimating the \emph{false positives} and the \emph{false negatives}.
Thus, the results in \cref{evaluation_results} are the \textbf{worst case} results.
We also calculated the \textbf{best case} results for \textsc{ArTEMiS}, which showed moderate improvements with a precision of \num{0.81879} and a recall of \num{0.4357} which increases the f1 score to \num{0.568751927875}.

The results for the \textsc{n8n} study subject are of better quality, as we could compare the results to ground truth.
As described in \cref{evaluation_n8n}, we ran the \ac{e2e} tests multiple times to establish a ground truth for the selected commits.
To test the performance of our approach, the selected commits were run six times each with instrumentation.
The results of these runs are shown in \cref{evaluation_results}.
We collected coverage on \textit{line level}, but also ran an analysis that worked on \textit{file level}.
Due to a lack more precise information we could not do the same for the \textsc{ArTEMiS} study subject.
In this experiment, there is no difference between the two levels.
This could indicate that the coverage collection could be more coarse without worsening the labeling quality.
Lastly, we calculated the \emph{precision}, \emph{recall}, and \emph{f1 score} for the approach in general by combining the results from both study subjects.

\subsection{Behavior change Impact}
To answer \textbf{\ac{rq}\textsubscript{2}}, we analyzed the data from both study subjects.
As described in \cref{evaluation_n8n}, we ran the \ac{e2e} tests with and without instrumentation on the 25 latest commits.
This data is used together with the data gathered from the \textsc{ArTEMiS} study subject.
\subsubsection{\texorpdfstring{RQ\textsubscript{2.1}:}{RQ2.1:} To what extent does instrumentation affect the runtime overhead of the application during \acs{e2e} testing?}\label[subsubsection]{results_rq2_1}
The results in \cref{duration_results_artemis} and \cref{duration_results} show that the instrumentation introduces a runtime overhead to the test execution.
The tests take on average \SI{21.08}{\percent} to \SI{41.98}{\percent} longer in the \textsc{ArTEMiS} project and \SI{12.33}{\percent} to \SI{14.95}{\percent} longer in the \textsc{n8n} project.
This time can be attributed to the slowdown caused by instrumenting the \ac{sut}.
To make sure that the observed changes in runtime are not caused by random fluctuations, but show a real difference, we checked the statistic significance of the results.
We used a t-test to check whether the difference in runtime is statistically significant.

\Cref{time_boxplots} shows the box plots of the runtimes of the \ac{e2e} tests for both study subjects.
For the \textsc{ArTEMiS} project, this includes all runs from the live evaluation across multiple versions of the \ac{sut}.
In this project, every version was run once with and once without instrumentation.
For the \textsc{n8n} project, this includes four runs per commit of the latest 25 commits of the project.
Here, every commit was run four times with and four times without instrumentation.
The duration considered does not include any build or installation steps of the projects.

As can be seen in \cref{duration_results_artemis} and \cref{artemis_time_boxplot}, the average runtimes of the \acs{e2e} tests for \textsc{ArTEMiS} are higher when using the instrumentation.
A t-test can show whether the difference is statistically significant.
Before running the t-test, some assumptions must be made.
The data contains extreme outliers, shown in \cref{artemis_time_boxplot}.
Since all measurements were taken on the same machine and the runs did not show any concerning behavior, the outliers don't need to be excluded as they represent valid data points.
The Shapiro-Wilk test shows that the data are not normally distributed ($p < .001$) \autocite{shapiro_analysis_1965}.
Since all groups contain more than 30 samples, we can use a t-test, as it has been shown to be robust to non-normality, in this case, \autocite{wilcox_introduction_2011}.
The t-test shows that the difference between the instrumentation and non-instrumentation groups is significant for failed builds ($t(403,769) = -8.350, p = < .001$) and passed builds ($t(20,897) = -2.21, p = .038$).
The mean difference is much bigger for failed builds (23.67 min) than for passed builds (11.92 min).

\begin{figure}[h]
	\centering
	\begin{adjustbox}{max width=\textwidth}
		\begin{subfigure}{0.6\textwidth}
			\begin{tikzpicture}
				\begin{axis}[					boxplot/draw direction=y,					ylabel={Duration (min)},					yticklabel={\pgfmathparse{exp(\tick)}\pgfmathprintnumber[fixed]{\pgfmathresult}},
						xtick={1, 2, 3, 4},
						xticklabels={Failed, Failed (instrumented), Passed, Passed (instrumented)},
						xticklabel style={align=center, font=\small, rotate=-20, anchor=north west},
						ymode=log,
						boxplot/box extend=0.4,
						boxplot/whisker extend=0.2,
					]
					\addplot+[boxplot] table[y index=0]{data/artemis/failedDurationsRegular.csv};
					\addplot+[boxplot] table[y index=0]{data/artemis/failedDurationsFlaky.csv};
					\addplot+[boxplot] table[y index=0]{data/artemis/passedDurationsRegular.csv};
					\addplot+[boxplot] table[y index=0]{data/artemis/passedDurationsFlaky.csv};
				\end{axis}
			\end{tikzpicture}
			\caption{Test runtimes for \textsc{ArTEMiS}}
			\label[figure]{artemis_time_boxplot}
		\end{subfigure}
		\hspace{0.5cm}
		\begin{subfigure}{0.6\textwidth}
			\begin{tikzpicture}
				\begin{axis}[					boxplot/draw direction=y,					ylabel={Duration (min)},					yticklabel={\pgfmathprintnumber[fixed]{\tick}},
						xtick={1, 2, 3, 4},
						xticklabels={Failed, Failed (instrumented), Passed, Passed (instrumented)},
						xticklabel style={align=center, font=\small, rotate=-20, anchor=north west},
						boxplot/box extend=0.4,
						boxplot/whisker extend=0.2,
					]
					\addplot+[boxplot] table[y index=0]{data/n8n/failedDurationsRegular.csv};
					\addplot+[boxplot] table[y index=0]{data/n8n/failedDurationsFlaky.csv};
					\addplot+[boxplot] table[y index=0]{data/n8n/passedDurationsRegular.csv};
					\addplot+[boxplot] table[y index=0]{data/n8n/passedDurationsFlaky.csv};
				\end{axis}
			\end{tikzpicture}
			\caption{Test runtimes for \textsc{n8n}}
			\label[figure]{n8n_time_boxplot}
		\end{subfigure}
	\end{adjustbox}
	\caption{Test runtime plots for both studied projects}
	\label[figure]{time_boxplots}
\end{figure}

\begin{table}[h]
	\centering
	\csvautobooktabular{data/artemis-durationResults.csv}
	\caption{Average test case duration for the \textsc{ArTEMiS} \ac{e2e} tests with and without instrumentation}
	\label[table]{duration_results_artemis}
\end{table}

For the second study subject, \textsc{n8n}, the average runtimes are higher when the instrumentation is used.
The recorded times in \cref{n8n_time_boxplot} show much less variance than for \textsc{ArTEMiS}.
\Cref{duration_results} shows the average duration of the \ac{e2e} tests.
The prerequisites are also met, except for the normality of the data.
The Shapiro-Wilk test shows that the data are not normally distributed in cases where the tests passed ($p < .001$) \autocite{shapiro_analysis_1965}.
In the cases where the tests failed without instrumentation ($p .004$) and with instrumentation ($p < .001$), the data are also not normally distributed.
Since both groups in which the tests passed have more than 30 samples, we can use a t-test, as it has been shown to be robust to non-normality, in this case \autocite{wilcox_introduction_2011}.
In cases where the tests fail because the groups have fewer than 30 samples, we will still use a t-test because it has been shown to be robust to violations of the normality assumption.
The t-test shows that the difference between the instrumentation and non-instrumentation groups is significant for failed builds ($t(7,721) = 11,654, p = < .001$) and passed builds ($t(174,985) = 113,555, p = < .001$).

\begin{table}[h]
	\centering
	\csvautobooktabular{data/durationResults.csv}
	\caption{Average test case duration for the \textsc{n8n} \ac{e2e} tests with and without instrumentation}
	\label[table]{duration_results}
\end{table}

\subsubsection{\texorpdfstring{RQ\textsubscript{2.2}:}{RQ2.2:} How does the instrumentation of coverage collection affect the failure rate of \acs{e2e} tests in the studied projects?}\label[subsubsection]{results_rq2_2}

Our experiment shows that the instrumentation of coverage collection does not significantly impact the failure rate of \ac{e2e} tests.
As \cref{failure_rate_results} shows, the failure rate of the \ac{e2e} tests is almost identical for both runs.
We also collected the failure rate on a test case level and present the statistics in \cref{testcase_results}.
Statistics for every test case that failed at least once during the evaluation are listed in \cref{testcase_details_n8n}.

\begin{table}[h]
	\centering
	\csvautobooktabular{data/runResults.csv}
	\caption{Statistics of the evaluation runs for \textsc{n8n}}
	\label[table]{failure_rate_results}
\end{table}

\Cref{failure_rate_results-rerun_artemis} shows that the failure rate of the \textsc{ArTEMiS} \ac{e2e} tests is higher when instrumentation is present.
The chi-squared test shows that the more frequently reported failures show a significant difference between the experiment with and without instrumentation ($\chi^2(1) = 122.844, p = < .001$).
One of the reasons for this could be that the \ac{ci} system of \textsc{ArTEMiS} had problems and a high load during the evaluation.
Another factor was that the \textsc{ArTEMiS} team focused on regular \ac{e2e} testing without instrumentation.
This led to many more problems in the parallel \ac{ci} plan used for the evaluation.
Meanwhile, the \ac{ci} system of \textsc{n8n} was entirely dedicated to our evaluation.
This increases the internal validity of our experiment for \textsc{n8n}.
The external validity is not limited by the dedicated \ac{ci} system since the \ac{ci} system used to evaluate the approach with \textsc{n8n} is not very unusual.

\begin{table}[h]
	\centering
	\csvautobooktabular{data/artemis-runResults.csv}
	\caption{Statistics of live CI runs for \textsc{ArTEMiS} when considering reruns as failures}
	\label[table]{failure_rate_results-rerun_artemis}
\end{table}

We analyze the failure rate of the tests in the \textsc{n8n} study subject with the results from \cref{failure_rate_results}.
Again, we see a higher failure rate for the instrumented run.
The chi-squared test shows that the more frequently reported failures do not show a significant difference between the instrumented and not instrumented runs ($\chi^2(1) = 2.407, p = .121$).

\section{Discussion} \label[section]{discussion}
In this chapter, we discuss the results of our evaluation and answer our research questions.
The discussion is structured according to the research questions.
Quantitative was reported in \cref{results}.

\subsection{Quality of Failure Categorization (\texorpdfstring{RQ\textsubscript{1}}{RQ1})}

While the results in \cref{results_rq1} show that our approach identifies flaky errors with high precision, \num{0.72} for \textsc{ArTEMiS} and \num{1} for \textsc{n8n}, the recall is low, \num{0.53} for \textsc{ArTEMiS} and \num{0.39} for \textsc{n8n}. \todo{check numbers for \textsc{n8n}}.
While we believe that false positives are a much bigger problem than false negatives, missing almost half of the flaky failures does not go far in solving the problems caused by flaky failures.
\Citeauthor*{bell_deflaker_2018} achieved a much higher recall of \num{0.955} while maintaining a high precision of \num{0.985}. \cite{bell_deflaker_2018}.

We suspect that the low recall is due to the fact that \ac{e2e} tests cover much more code than unit tests.
This leads to a much higher chance that changes in the \ac{sut} causing the failure will be covered by a failing test, even if those changes did not cause the failure.
Since we only consider failures to be flaky if they cover \textbf{none} of the changed lines, this leads to many false negatives.

Due to the low recall of our approach, we do not believe it can go far in solving the problems caused by flaky failures.
Developers will still have to deal with flaky failures that are not detected by our approach.
And while our approach finds about half of the flaky failures, it also comes at a significant cost in terms of runtime.
The results in \cref{results_rq2_1} show that the runtime of \ac{e2e} tests increases significantly when coverage collection is enabled.

\begin{mdframed}
	\textbf{Answer to RQ\textsubscript{1}:} Our coverage-based approach performs well in finding flaky failures, as demonstrated by our statistical analysis.
	Our approach's high precision indicates that most of the failures we identified as flaky are indeed flaky.
	However, the low recall indicates that there are many failures that where flaky which we did not identify as flaky.
\end{mdframed}

\subsection{Behavioral Change Caused by Instrumentation (\texorpdfstring{RQ\textsubscript{2}}{RQ2})}

The results in \cref{results_rq2_1} show that coverage collection instrumentation has a significant impact on the runtime of \ac{e2e} tests.
Execution is significantly slower in both projects, which is to be expected since we are doing additional work during test execution.
The \ac{sut} is also instrumented to collect coverage metrics during test execution.
It shows that runtime overhead increases when tests fail.
This is not surprising since coverage is only processed when tests fail.
As described in \cref{coverage_collection_plugins}, the coverage collection plugins must read all source files to map the coverage data to the source code.
This is a time-consuming process, especially for large projects.

The difference in runtime overhead between passed and failed tests is only visible for \textsc{ArTEMiS}, where the instrumentation adds \SI{21.08}{\percent} to the runtime of passed tests and \SI{41.98}{\percent} to the runtime of failed tests.
For \textsc{n8n}, the difference is very similar for passed and failed tests, \SI{14.95}{\percent} and \SI{12.33}{\percent}, respectively.

This runtime overhead is much higher than the \SI{4.5}{\percent} overhead that \citeauthor*{bell2018determining} reported for their approach.
This is to be expected, since we collect coverage data for the entire \ac{sut}, not just the modified parts of the code.
The smaller runtime impact of \textsc{n8n} suggests that the V8 coverage collection has a smaller runtime impact than JaCoCo.

\begin{mdframed}
	\textbf{Answer to RQ\textsubscript{2.1}:} In both study subjects, we found that coverage collection instrumentation has a significant impact on the runtime of \ac{e2e} tests.
	In the best case, the runtime overhead is \SI{12.33}{\percent} for \textsc{n8n} and \SI{21.08}{\percent} for \textsc{ArTEMiS}.
	There is evidence that instrumentation has a greater impact on the runtime of failed tests.
\end{mdframed}

To answer RQ\textsubscript{2.2}, the results in \cref{results_rq2_2} show the effect of instrumentation on the failure rate of \ac{e2e} tests.
While there is a significant difference in the failure rate for \textsc{ArTEMiS}, the difference is not significant for \textsc{n8n}.
Since the results for \textsc{n8n} are similar to the results of \citeauthor*{rasheed_effect_2023} \autocite{rasheed_effect_2023}, we assume that instrumentation does not have a significant effect on the failure rate of \ac{e2e} tests.

The results for \textsc{ArTEMiS} may be related to the general problems with the \ac{ci} system during evaluation.
During the evaluation period, the \ac{ci} system of \textsc{ArTEMiS} showed multiple issues that led to a high failure rate of \ac{e2e} tests.
Since our parallel \ac{ci} plan was not of major concern for the development team, any issues impacted the instrumented version of the \ac{e2e} plan more than the regular \ac{e2e} plan.
This could explain the higher failure rate of the instrumented version of the \ac{e2e} plan.

\begin{mdframed}
	\textbf{Answer to RQ\textsubscript{2.2}:} We found that applying our approach significantly changed the test result only for \textsc{ArTEMiS}.
	For \textsc{n8n}, the difference was not statistically significant.
	This indicates that the coverage collection instrumentation does not significantly impact the failure rate of \acs{e2e} tests.
\end{mdframed}
%------- chapter 6 -------

\chapter{Threats to Validity}\label[chapter]{threats}
The following chapters discuss the threats to the validity of our results.
We follow the guidelines of \Citeauthor*{wohlin_experimentation_2012} \cite{wohlin_experimentation_2012} and use the categories \textit{internal validity} and \textit{external validity}.

\section{Threats to Internal Validity}\label[section]{threats_internal}
Because of the reliance on test coverage, our approach is limited to the code executed by the tests.
However, other factors are tracked in version control that can affect the behavior of the \ac{sut} during \ac{e2e} testing.
Examples are changes to the configuration of the \ac{sut} or the test cases themselves.
\ac{e2e} \ac{ui} testing of web applications can also be affected by changes to the styles of the \ac{ui} elements.
Since CSS is not executed by the \ac{sut}, our approach does not track these changes.
While we have no evidence that these factors have a significant impact on the behavior of the \ac{sut}, we cannot rule out the possibility that they do.

Another threat to internal validity is the dependence on the \ac{ci} pipeline.
Especially in the case of \textsc{ArTEMiS}, the \ac{ci} pipeline is unreliable.
This is due to the constant changes to the project and the \ac{ci} setup.
Also, the \ac{ci} pipeline is shared with other projects and is not dedicated to \textsc{ArTEMiS}.
Sudden spikes in builds on the system can also cause the \ac{ci} pipeline to fail.
While this can affect the quality of our results, it is a realistic scenario.
Our second study subject, \textsc{n8n}, was evaluated on a dedicated \ac{ci} system under our control.
This reduces the impact of this threat to internal validity.

\section{Threats to External Validity}
The main threat to external validity is the limited number of study subjects.
While the two study subjects are very different in nature, there are many more ways to build a web application and write \ac{e2e} tests.
Therefore, our results cannot be generalized to all web applications and \ac{e2e} tests.
A second indicator is that we used only one test framework (cypress) in both study subjects.
While cypress is a popular test framework, there are others that are used in practice.
The limited applicability also showed in the fact that we had to exclude some test cases from the evaluation because they did not pass after adjusting the \ac{ci} pipeline of \textsc{n8n}.

Another threat to external validity is the limited information about the \emph{true} flakiness of the test cases.
Due to the non-deterministic nature of flaky tests, it is impossible to know if a test case is definitely not flaky.
This limits the quality of our results for RQ\textsubscript{1}, especially considering \textsc{ArTEMiS}.
\Citeauthor*{schallermayer_reducing_2023} found that every test in the project will show flaky failures if the resources of the \ac{sut} are severely limited \cite{schallermayer_reducing_2023}.
While the ground truth for \textsc{n8n} is more reliable than our estimations for \textsc{ArTEMiS}, it is still possible that some test cases are flaky but were not detected.

%------- chapter 7 -------

\chapter{Conclusion}\label[chapter]{conclusion}
In this thesis, we have presented and evaluated an approach for detecting flaky failures in \ac{e2e} tests using change coverage.
Flaky failures are a common problem in \ac{e2e} testing and can lead to a loss of confidence in the test suite.
Flakiness is widespread in practice \autocite{hilton_trade-offs_2017,micco_state_2017}, with some authors even suggesting that all tests should be considered flaky \autocite{harman_start-ups_2018}.
\Ac{e2e} \ac{ui} tests are particularly prone to flakiness due to their complex interactions, which lead to inherent non-determinism \autocite{romano_empirical_2021}.
In addition, they can cause developers to waste time investigating failures that are not caused by changes to the \ac{sut} \autocite{ziftci_-flake_2020}.

While there are many approaches to detecting flaky tests, there are few that focus on detecting flaky failures.
\Citeauthor*{bell_deflaker_2018} propose \textsc{DeFlaker}, which uses change coverage to detect flaky unit test failures without rerunning \cite{bell_deflaker_2018}.
An approach with similar efficiency for \ac{e2e} would go a long way to reducing the impact of flaky failures on developers.
We adapted this approach to \ac{e2e} tests and evaluated it on two study subjects.
To analyze test failures, we implemented a tool that collects coverage information during test execution and compares it to the changes since the last successful build.

We added our tool to the \ac{ci} pipeline of two open-source web applications to evaluate our approach under realistic conditions.
The first study subject, \textsc{ArTEMiS}, is a web application for teaching software engineering.
The second study subject, \textsc{n8n}, is a workflow automation tool.
Both study subjects use cypress as their testing framework.
For \textsc{ArTEMiS}, we collected data by running a parallel build on the \ac{ci} system that runs the tests with our tool.
For \textsc{n8n}, we used a dedicated \ac{ci} system and ran the tests on selected commits for which we had established ground truth.
% \feedback{Muss hier m.M.n. gar nicht so ausfhrlich sein}

While our evaluation shows a significant impact of the instrumentation used on the runtime of the tests, we did not find it to have a significant impact on the flakiness of the tests.
We only found a significant increase in flaky failures for \textsc{ArTEMiS}, which we attribute to the unreliable \ac{ci} pipeline.
This is consistent with the results of other authors \autocite{rasheed_effect_2023}.
The runtime impact we observed is with \SI{12.33}{\percent} in the best case much higher than the \SI{4.5}{\percent} of \textsc{DeFlaker} \autocite{bell_deflaker_2018}.
In the worst case, we saw an increase of \SI{41.98}{\percent} which delays the feedback for developers.
\textsc{ArTEMiS} has an average runtime of \SI{64}{\minute} for their \textsc{e2e} test suite.
This means keeping our approach \textit{always on} would increase the runtime by up to \SI{26.867}{\minute} in the worst case.

We have also evaluated the detection performance of our approach.
The collected data from both study subjects show that our approach can reliably detect flaky failures.
We found that our approach has very low false positives, reducing the risk of missing faults mistakenly labeled as flaky failures.
However, our approach has a low recall, which means that it is not able to detect all flaky failures.
The results we saw are much worse than the results of \textsc{DeFlaker}.
\Citeauthor*{bell_deflaker_2018} report a recall of \num{0.955} and a precision of \num{0.985} \cite{bell_deflaker_2018}.
We observed an average recall of \num{0.42} and an average precision of \num{0.77} during our evaluation\todo{update numbers}.

These results show that while \textsc{DeFlaker} is a promising approach for detecting flaky failures in unit tests, it is not directly applicable to \ac{e2e} tests.
We believe that this is due to the differences between unit and \ac{e2e} tests.
\Ac{e2e} tests are more complex and cover much more code than unit tests which makes it harder to detect flaky failures.

\section{Future Work}
For this thesis, we limited our evaluation to projects that used the cypress framework for \ac{e2e} testing.
While we used our approach with two different study subjects, many more languages and frameworks are used in practice.
Additional evaluations should be conducted with projects that use different tech stacks to determine the applicability of our approach.
In addition, the coverage collection tool could be extended to support other testing frameworks.

As mentioned in \cref{threats_internal}, our approach only considers code changes.
However, other changes can cause failures, such as changes to the test cases, config files, or dependencies.
Even though we saw few \aclp{fp} in our evaluation, we believe this could be improved by considering these changes.

As our evaluation was also limited by the availability of reliable labels for \ac{e2e} tests, we believe that having a larger dataset of flaky and non-flaky tests would be beneficial.
Further research into the flakiness of \ac{e2e} tests could help improve the quality of evaluations.

We experienced a significant increase in test runtime when using our approach.
Some tests of the \textsc{n8n} study subject showed memory issues when run with coverage collection.
To mitigate this issue, we had to change the \ac{ci} pipeline to run the tests in chrome instead of electron.
This change led to some tests not passing, which we had to exclude from the evaluation.
Further research into the impact of instrumentation on test execution could mitigate this issue.
The impact of instrumentation could be reduced by simply limiting the size of spec files.

\appendix

\chapter[Behaviour change results]{Extensive results for the behavior change evaluation}\label[appendix]{behaviour_change_results}

\begin{table}[h]
	\begin{adjustbox}{max width=\textwidth}
		\begin{tabular}{@{}lrrrrr@{}} \toprule
			\multicolumn{2}{c}{} & \multicolumn{2}{c}{Uninstrumented} & \multicolumn{2}{c}{Instrumented}                                                       \\ \cmidrule{3-4} \cmidrule{5-6}
			Test name            & Duration increase \%               & passed                           & failed  & passed              & failed              \\ \midrule
			\csvreader[
				head to column names
			]{data/testcaseStats.csv}{}{%
			\name                & \durationIncrease                  & \passed                          & \failed & \passedInstrumented & \failedInstrumented \\
			}
			\\ \bottomrule
		\end{tabular}
	\end{adjustbox}
	\caption{Detailed results of the behavior change evaluation for \textsc{n8n}}
	\label[table]{testcase_details_n8n}
\end{table}

\begin{table}[h]
	\centering
	\csvautobooktabular{data/testcaseResults.csv}
	\caption{Statistics of test case results during evaluation for \textsc{n8n}}
	\label[table]{testcase_results}
\end{table}

\chapter{Referenced code} \label[appendix]{code}

\lstinputlisting[language=yaml,caption={Docker compose override file for coverage collection},label=coverage_compose]{code/cypress-E2E-tests-coverage-override.yml}

\microtypesetup{protrusion=false}
\include{common/acronyms}
\listoffigures{}
\listoftables{}
\lstlistoflistings{}
\microtypesetup{protrusion=true}
\printbibliography{}

\end{document}
