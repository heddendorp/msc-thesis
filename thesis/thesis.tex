% !BIB TS-program = biber

\RequirePackage[l2tabu,orthodox]{nag}

% Add common preamble to the document
\input{common/preamble.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Theses specific packages go here
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[printonlyused]{acronym}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Begin of document
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
% \setlength{\evensidemargin}{22pt}
% \setlength{\oddsidemargin}{22pt}


% \hypersetup{pdfborder={0 0 0}, pdfauthor={\author}, pdftitle={\title}}

\pagenumbering{alph}

%------- Cover and Title setup -------
\include{common/cover}
\frontmatter{}
\include{common/titlepage}

%------- Disclaimer -------
\include{thesis_tex/disclaimer}

%------- Acknowledgements -------
% \newpage
% \thispagestyle{empty}
% \mbox{}
% \include{thesis_tex/acknowledgement}

% \pagenumbering{roman}

%------- Abstracts -------
% \selectlanguage{english}
\include{thesis_tex/abstract_en}
% \clearpage
% \selectlanguage{german}
\include{thesis_tex/abstract_de}
% \clearpage
% \selectlanguage{english}

%------- Table of contents -------
\microtypesetup{protrusion=false}
\tableofcontents{}
\microtypesetup{protrusion=true}

%------- List of todos -------
\todototoc
\listoftodos

\mainmatter{}
% \pagenumbering{arabic}

% \fancyhead{}
% \pagestyle{fancy}
% \fancyhead[LE]{\slshape \leftmark}
% \fancyhead[RO]{\slshape \rightmark}
% \headheight=15pt


\chapter*{Not yet assigned ideas}
\TODO{Talk about the limited amount of actual failures in both projects for code that is run by the CI}
\TODO{Overview of the ArTEMiS historic data and insight, that all tested fails did eventually pass}
\TODO{Mention the significant effect cypress rerunning had on ArTEMiS}
\TODO{Elaborate how this approach is useful in everyday development contrary to flaky test detection}
\TODO{Talk about the side effect that failures can be located more easily}
\TODO{Limitation: does not work if service does not start}
\TODO{Problems: include bigger issues during development}

%------- chapter 1 -------

\chapter{Introduction}\label[chapter]{introduction}
Software testing is an integral part of the software development process ensuring a system's quality and dependability.
With the execution of test cases, software testing aims to detect software bugs.
Test cases are typically assumed to be deterministic, meaning that given the same input, they would always yield the same output.
In practice, however, test cases can be non-deterministic or \emph{flaky}, \autocite{luo_empirical_2014} meaning they can fail intermittently without any modification to the \ac{sut} \todo{Decide on Acronym casing} or the test itself.
\Ac{ui} tests, which frequently involve asynchronous activities and limited computer resources, might be especially susceptible to flaky behavior \autocite{romano_empirical_2021}.

Flaky tests can waste resources, increase development time and erode confidence in the testing procedure.
Google reports that about 16\% of their tests exhibit some level of flakiness \autocite{micco_state_2017}.
Even given the importance of testing, detecting flaky tests is still challenging.
The most common approach to identify flaky tests is rerunning \autocite{lam_idflakies_2019}.
However, rerunning has several downsides, such as consuming significant computational resources, generating false positives, and potentially missing certain types of flaky tests \autocite{bell_deflaker_2018, luo_empirical_2014}.
\feedback{The paragraph does not really describe the point of the thesis, which is to detect flaky failures. Maybe move from flaky tests to flaky failures?}

An important distinction that has to be made is the difference between flaky tests and flaky test failures \todo{Explain flaky failures better}.
While most research has focused on detecting flaky tests, it is essential to distinguish them from flaky failures.
\Citeauthor*{haben_importance_2023} find that \enquote{approximately 76.2\% of all regression faults} \autocite{haben_importance_2023} are missed when discarding failures of flaky tests.
Instead, we want to focus on detecting flaky failures in this thesis to provide more value to the \ac{ci} during software development.
\todo{Extend this paragraph?}

To achieve this for \ac{e2e} tests, we focus on identifying flaky failures by applying a technique \todo{Give short explanation} for detecting flaky unit test failures proposed by \citeauthor*{bell_deflaker_2018} in \citetitle{bell_deflaker_2018} \autocite{bell_deflaker_2018} to \ac{e2e} tests.
\Citeauthor*{bell_deflaker_2018} found that it is possible to identify if a failure is flaky by comparing the code covered by the test with the changes since the last passing run.
To evaluate our approach, we selected two open-source projects as case studies.
\textsc{ArTEMiS} \autocite{krusche_artemis_2018} is a web-based learning platform for programming exercises.
It uses Java for server code and JavaScript for client-side code.
\textsc{n8n} \autocite{noauthor_n8n_2023} is a workflow automation tool written in TypeScript for server and client.
Both projects use Cypress \autocite{noauthor_cypress-iocypress_2023} for \ac{e2e} testing.

The contributions of this thesis are:
\begin{itemize}
	\item \textbf{Methodology.} We propose a methodology \todo{Explain methodology} for detecting flaky failures in \ac{e2e} tests based on previous work in detecting flaky unit test failures \autocite{bell_deflaker_2018}.
	\item \textbf{Case Studies.} We evaluate our approach on two open-source projects by analyzing the impact on execution of our method has and evaluating the performance of our approach.
	      During the evaluation, we saw that the instrumentation can significantly impact test execution.
	      Our approach correctly identified flaky failures in 90\% \todo{Correct number} of the cases.
	      During the evaluation, we saw three \todo{Correct number} false positives where our approach identified a failure as flaky, but it was not.
	      \todo{Add more info on study and results}
	\item \textbf{Guidelines.} \todo{Add guidelines}
\end{itemize}

The remainder of this thesis is organized as follows:
In \cref{background}, we will give some background information on \ac{e2e} testing, flaky tests and failures, and coverage collection.
We review related work in \cref{related_work}.
Our methodology, including instrumentation, data collection, and environment setup, is described in \cref{methodology}.
In \cref{evaluation}, we present the results of our case studies and answer our research questions.
We discuss the limitations of our approach in \cref{threats} and conclude with a summary of our findings and potential future work in \cref{conclusion}.



%------- chapter 2 -------

\chapter{Background}\label[chapter]{background}

This chapter provides essential background information for understanding the context of this thesis.

\section{E2E Testing}

\TODO{General rough introduction to software testing before moving to \ac{e2e} and \ac{ui} only}
\TODO{Explain what E2E testing is and why it is important}
\TODO{Give an overview of testing frameworks available (also not for browsers?)}
\TODO{Explain what Cypress is}

\section{Flaky Tests and Failures}

\TODO{Mention the root causes for flaky tests and failures}
\TODO{Press the importance and usefulness of identifying flaky failures instead of just flaky tests}
\TODO{Explain why this approach with flaky failures is superior to identifying flaky tests (more common)}
\TODO{Explain what flaky failures are and why they are a problem}

\section{Coverage Collection}

\TODO{Explain what coverage collection is}
\feedback{Coverage collection is expensive?}
\TODO{Mention some use cases for coverage data}

%------- chapter 3 -------

\chapter{Related Work}\label[chapter]{related_work}

\TODO{Give an overview of related work}

\section{Flaky Failure Detection}
\Citeauthor*{haben_importance_2023} have studied the difference between detecting flaky tests and flaky test failures, highlighting the importance of the latter for a more effective \ac{ci} process.
In recent years, various methods have been proposed to identify flaky tests.
However, these methods have not been evaluated within a \ac{ci} process, leaving their actual utility unclear \autocite{haben_importance_2023}.

\Citeauthor*{haben_importance_2023} conducted a case study on the Chromium \ac{ci}, applying state-of-the-art flakiness prediction methods to examine their performance.
They found that despite the high precision of the methods (99.2\%), their application resulted in many missed faults, approximately 76.2\% of all regression faults.
To investigate this finding, the authors analyzed the fault-triggering failures and discovered that flaky tests have a strong fault-revealing capability, revealing more than one-third of all regression faults \autocite{haben_importance_2023}.

This result highlights an inherent limitation of methods that focus on identifying flaky tests rather than flaky test failures.
To address this issue, \Citeauthor*{haben_importance_2023} proposed failure-focused prediction methods and optimized them by considering new features.
Interestingly, they found that these methods performed better than the test-focused ones, with the \ac{mcc} increasing from 0.20 to 0.42 \autocite{haben_importance_2023}.

Overall, the study by \citeauthor*{haben_importance_2023} suggests that future research should concentrate on predicting flaky test failures instead of flaky tests.
Additionally, it emphasizes the need for adopting more thorough experimental methodologies when evaluating flakiness prediction methods \autocite{haben_importance_2023}.
This insight underscores the importance of discerning flaky from fault-triggering test failures in order to improve the efficiency of \ac{ci} processes and minimize the waste of valuable developer time on investigating false alerts caused by flaky test failures.
\TODO{Look at some papers that detect flaky tests}
\TODO{Explain why flaky test detection is not sufficient, but we need flaky failure detection}
\feedback{Fabian Leinen: Den eigenen Gedanken in dem Kapitel möglichst meiden. Z.B. in "The Importance of Discerning Flaky from Fault-triggering Test Failures: A Case Study on the Chromium CI" ist das ja wunderschön dargelegt.
}

\section{Adapting \emph{DeFlaker}}

\Citeauthor*{bell_deflaker_2018} have looked at the problem of flaky tests in unit tests and proposed a method to detect flaky tests without rerunning them \autocite{bell_deflaker_2018}.

To address this issue, the authors propose a novel technique called DeFlaker, which detects flaky tests without the need for rerunning them and incurs minimal runtime overhead.
DeFlaker works by monitoring the coverage of the latest code changes and identifying any newly failing test that did not execute the changes as flaky.
This approach significantly reduces the time and resources required to detect flaky tests, leading to a more efficient development cycle \autocite{bell_deflaker_2018}.

The effectiveness of DeFlaker was evaluated on 96 Java projects hosted on TravisCI, where it identified 87 previously unknown flaky tests in 10 of the projects.
Moreover, the authors conducted experiments on project histories, where DeFlaker detected 1,874 flaky tests from a total of 4,846 failures.
These results highlight the low false alarm rate of DeFlaker, at just 1.5\%.
Additionally, DeFlaker demonstrated superior performance when compared to Maven's default flaky test detector, boasting a 95.5\% recall rate as opposed to Maven's 23\% \autocite{bell_deflaker_2018}.

The approach presented in the \citetitle{bell_deflaker_2018} paper has significant potential for application in \ac{e2e} \ac{ui} testing.
By incorporating coverage analysis into UI testing, developers can more accurately and efficiently detect flaky tests, ultimately leading to improved test quality and a more streamlined development process.
The integration of coverage metrics into the \ac{ui} testing process of two case studies is presented in \cref{methodology}.


\TODO{Explain how we can use the same approach for E2E tests}

\section{Usage of coverage data in testing}

\TODO{Look at some papers that use coverage data}
\TODO{Look at test case selection based on coverage data}

%------- chapter 4 -------

\chapter{Methodology}\label[chapter]{methodology}

\TODO{Overview for the Chapter and introduction to the different parts of the project.}
\TODO{Explain the approach in DeFlaker and how this project has different requirements?}
This chapter presents the methodology used to detect flaky failures in \ac{e2e} tests. We first discuss the coverage collection process, followed by change collection, and finally the implementation of the analysis program and coverage collection plugins.

\section{Coverage Collection}
The approach presented in this thesis is inspired by \citeauthor*{bell_deflaker_2018}'s work on detecting flaky unit test failures \autocite{bell_deflaker_2018}.
The main differences between the two approaches are that we analyze \ac{e2e} test runs instead of unit test runs, and we collect coverage data on the entire code base instead of just the code that changed recently.
\TODO{Info about issues with default instrumentation which is not designed for remote collection for specific test cases.}
\TODO{Explain why the entire coverage is collected and not just change coverage}
\TODO{Explain how the decision was made on which level to collect coverage}
\TODO{Explain how the coverage is matched to test cases: propagation vs timing}
\TODO{Mention the possible impact of instrumentation on the behavior of the application and the tests.}

Coverage collection is a central part of the approach introduced by \citeauthor*{bell_deflaker_2018} in \citetitle{bell_deflaker_2018} \autocite{bell_deflaker_2018}.
While \citeauthor*{bell_deflaker_2018} work with change coverage in unit tests, this option is not feasible in \ac{e2e} tests.
The changes in the code can be very large and span multiple services and languages, which makes it difficult to instrument the specific parts of the code.
To reduce implementation complexity, we decided to collect coverage data on the entire code base in a more generic way.

Another important step is to match the collected coverage with the test cases that were executed.
Here, multiple options are possible.

\paragraph{Propagation.} First option is to propagate data about the executed test cases through the code base.
Since we collect coverage data on the entire code base, it is required to propagate the data from the testing framework to both the client and server code.
Additionally, the coverage collection has to be reset after each test case to avoid mixing coverage data from different test cases.

\paragraph{Timing.} The second option is to match the coverage data with the test cases based on the timing of the test cases.
This simpler approach is possible because the test cases are executed sequentially and the coverage data is collected in the same order.
Instead of propagating the test case information through the code base, we can simply match the coverage data with the test cases based on the timing.
In this approach, the coverage is collected and saved after each test case and the collection is reset.

For simplicity, we decided to use the second approach and match the coverage data with the test cases based on the timing.
This approach is also more robust and does not require any changes to the code base.
Event parallel execution of tests is possible if there is a separate instance of the tested software for each worker.

\section{Change Collection}

In addition to coverage collection, we also need to collect changes in the code base to determine which parts of the code have been modified since the last successful test run.
In this section, we explain how we collect changes using the git history of the code base and match them with the collected coverage data.

\section{Implementation}
\TODO{Explain the implementation of the coverage collection plugins}
\TODO{Explain the implementation of the analysis program}
In this section, we describe the implementation of the coverage collection plugins for the two case studies and the analysis program. Both the case studies use the cypress testing framework, and the analysis program is implemented in TypeScript and Node.js.
It is realized as a \ac{cli} tool that can be executed with the command \texttt{npx @heddendorp/coverage-git-compare}.
Both tools are open source and published to the npm registry.
\begin{figure}[h]
	\centering
	\begin{tikzpicture}[node distance=2cm]
		% Nodes
		\node (cypress) [rectangle, draw] {Cypress};
		\node (tests) [rectangle, draw, right of=cypress, xshift=2cm] {Specs};
		\node (plugin) [rectangle, draw, below of=cypress] {Coverage Collection Plugin};
		\node (client) [rectangle, draw, below left of=plugin, xshift=-2cm] {Client};
		\node (server) [rectangle, draw, below right of=plugin, xshift=2cm] {Server};
		\node (coverage) [rectangle, draw, below of=plugin] {Full Coverage};
		\node (analysis) [rectangle, draw, below of=coverage] {Analysis Program};

		% Edges
		\draw [<-] (tests) --  node[pos=0.5, below] {runs} (cypress);
		\draw [-] (cypress) -- (plugin);
		\draw [<->] (plugin) -- (client);
		\draw [<->] (plugin) -- (server);
		\draw [->] (plugin) -- node[pos=0.5, below] {on fail} (coverage);
		\draw [->] (coverage) -- (analysis);
	\end{tikzpicture}
	\caption[Full stack coverage collection process in e2e testing]{Diagram showing the full stack coverage collection process in \ac{e2e} testing}
	\label[figure]{full_stack_coverage_collection}
\end{figure}

\subsection{Coverage Collection Plugins}
We implemented coverage collection plugins for both case studies, with specific adjustments to accommodate their different setups. In this subsection, we discuss the implementation details for the Java server, the Node.js server, and the client.

Due to their different setups, the coverage collection plugins for the two case studies are implemented differently.
The Java server uses the jacoco agent to collect coverage data.
The Node.js server uses the V8 coverage \ac{api} to collect coverage data.
The client coverage is collected by using the V8 coverage for both case studies as both run their tests in chrome.
The plugins remap all file paths to be relative to the repository root and merge the coverage information of client and server.

The cypress plugins are implemented as node modules that can be installed with npm.
They make use of the cypress plugin \ac{api} \autocite{cypressio_writing_nodate} to hook into the cypress test runner.
As cypress only offers a \texttt{before:spec} and \texttt{after:spec} hook, the coverage can only be collected for each spec file instead of each individual test case.
Additionally, the plugins make use of the cypress \texttt{before:browser:launch} hook to identify the port of the chrome debugging protocol.
\subsubsection{Java server}
\TODO{Explain the use of jacoco and it's setup in server mode}
\TODO{Transform coverage to XML and to JSON}
As \textsc{ArTEMiS} uses the JHipster \autocite{jhipster_jhipster_nodate} development platform, the server is implemented in Java.
To collect coverage data, we use the jacoco agent \autocite{noauthor_jacoco_nodate} in server mode.
The server mode of jacoco allows collecting and reset coverage data from the server without restarting.
To enable the jacoco agent, we use the \texttt{-javaagent} option of the java command.

On the \texttt{before:spec} hook, we send a request to the jacoco agent to reset the coverage data.
On the \texttt{after:spec} hook, the plugin checks if the spec passed.
If the spec passed the coverage data is not collected as there is no analysis to be done.
If the spec failed, the coverage data is collected in the jacoco binary format.
After the report is collected, the plugin uses the jacoco \ac{cli} to transform the binary report to an XML report.
The XML report is transformed using the \textsc{jacoco-parse} library \autocite{noauthor_jacoco-parse_2023}.
\footnote{We maintain a fork of the library with security updates at \url{https://github.com/heddendorp/jacoco-parse} which is used in this project.}
The plugin saves the covered lines for each file in a JSON file after merging them with the coverage data of the client.

\subsubsection{Node.js server}
\TODO{Explain NYC cannot be used as dynamically and would need a reset mechanism}
\TODO{Explain the use of the node debugging protocol to use V8 coverage}
\textsc{n8n} is a Node.js based workflow automation tool.
Coverage in node.js programs is usually collected with the \textsc{NYC} tool \autocite{noauthor_nyc_2023}.
NYC is a command line tool that can be used to collect coverage data for node.js programs.
However, NYC is not suitable for our use case as it does not allow collecting coverage data dynamically.
NYC does not allow collecting coverage data dynamically as it does not expose an \ac{api} to collect coverage data.
Since we have to collect coverage data after each test case from the cypress test runner, we decided against using NYC.
The author of NYC also maintains C8 \autocite{noauthor_bcoec8_nodate}, a similar tool that uses V8 native coverage instead of code instrumentation.
Event though C8 does not allow collecting coverage data dynamically, it served as inspiration for our implementation.

To collect coverage data, we use the V8 native coverage to collect coverage data.
The process of collecting coverage using the V8 native coverage is the same as for the client described in the following section.
To enable the debugging protocol, we use the \texttt{--inspect} option of the node command.
\subsubsection{Client} \label[subsection]{client_coverage_collection}

\TODO{Elaboration of instrumentation ideas that did not work out. Preprocessing with istanbul as suggested by cypress and teamscale.}
\TODO{Elaboration of chrome native based coverage}
\TODO{Small tangent on electron instrumentation which is special as cypress already runs in an electron process}

Both case studies run their tests in chrome.
This allows us to use V8 native coverage to collect coverage data.
Coverage data is available in the chrome developer tools \autocite{kayce_basques_coverage_2020}.
Additionally, it is accessible through the chrome debugging protocol \autocite{noauthor_chrome_nodate}.
To connect to the chrome debugging protocol, we use the \textsc{chrome-remote-interface} library \autocite{cardaci_chrome-remote-interface_2023}.
In order to connect to the debugging protocol, we need to know the port of the debugging protocol.
The port is set by cypress when it starts the browser and can be retrieved from the cypress \texttt{before:browser:launch} hook.
Then, before every spec run we connect to the debugging protocol and enable coverage collection.
After the spec run has finished we check if the spec run failed.
If the spec run failed, we collect the coverage data and reset the coverage data.
If the spec run did not fail, we just reset the coverage collection without retrieving it.

To work with the coverage data it is transformed from the V8 format into the common istanbul format.
For this we use the \textsc{v8-to-istanbul} library \autocite{noauthor_v8--istanbul_2023}.
As the library requires the original files to transform coverage, our plugin maps the paths of the files as reported by chrome to the correct paths in the file system.
The library reads the code files that are executed in the browser and follows the source maps to map the coverage data to the original source files.
This is necessary as the browser executes the transpiled code and not the original source files.
The coverage data is then simplified to a custom format that includes only covered lines and files and saved as JSON.

\paragraph{Code Instrumentation} The common way to collect coverage in JavaScript is to instrument the code with a coverage library.
Cypress uses the \textsc{istanbul} library to instrument the code \autocite{noauthor_code_nodate}, it is available through the nyc command line tool \autocite{noauthor_nyc_2023} or as a transpilation plugin.


\subsubsection{Additional Helper Programs}
\TODO{Elaborate on the historic analysis helper?}
\TODO{Explain the jacoco coverage transform fork?}

\subsection{Analysis Program}
\TODO{Explain how to get file based changes from the git log}
\TODO{Explain how to get line based changes from the git diff and the issues with buffer size}
The analysis program is executed when the tests exhibit at least one failure.
In this subsection, we explain the process of retrieving change data from the git repository and comparing it with the coverage data to identify potentially flaky tests.
As our plugin saves the coverage data for any spec that does not pass in a JSON file, the analysis program can be executed after the test run has finished.
A good example is the testing script in ArTEMiS \texttt{npm ci \&\& npm run cypress:run || npm run detect:flakies} this runs the tests and if they fail, the analysis program is executed.
\cref{coverage_compose} contains the docker compose file that is used to run the analysis program.

Since \textsc{ArTEMiS} is running \ac{ci} on bamboo the analysis is called with the current plan, build number and an access token.
The analysis program then checks the bamboo \ac{api} for the latest build that passed.
If the latest build that passed is not the previous build, the analysis program falls back to a comparison with the \texttt{develop} branch.
The program can also be invoked in the \texttt{compare} mode where the commit for comparison is passed as an argument.

After identifying the commit to compare with, the program retrieves the change data from the git repository.
For this, the program uses the \texttt{git diff} command to retrieve the changes between the two commits.
Afterwards, the diff is parsed using the \textsc{parse-diff} library \autocite{todyshev_parse-diff_2023}.
The program then extracts the changed lines for each file.
The collected changes are then compared to the coverage data.
For each spec file, the program checks if the changed lines are covered by the tests.
If the changed lines are not covered by the tests, the spec is marked as potentially flaky.

%------- chapter 5 -------

\chapter{Evaluation}\label[chapter]{evaluation}
In this chapter we put our approach, outlined in \cref{methodology}, to the test.
We first describe the case studies that we use to evaluate our approach.
Then we present the results of our evaluation and discuss them.
\section{Research Questions}
We have two \acp{rq} that we want to answer with our evaluation.
\begin{enumerate}
	\item[\textbf{\ac{rq}\textsubscript{1}:}] How effective is the adapted DeFlaker approach in identifying flaky failures in UI \ac{e2e} tests?

		We want to determine the effectiveness of our approach in identifying flaky failures in UI \ac{e2e} tests.
		We will evaluate the approach by comparing the results of our approach with an established baseline from rerunning the tests.
	\item[\textbf{\ac{rq}\textsubscript{2}:}] What impact does the instrumentation have on the behavior of the application and tests?

		We recognize that the instrumentation of the application and tests has an impact on the behavior of the application and tests.
		Therefore, we further Investigate this impact with two subquestions:
		\begin{enumerate}
			\item[\textbf{\ac{rq}\textsubscript{2.1}:}] To what extent does instrumentation impact the temporal behavior of the application during \ac{e2e} testing?

				We want to determine the impact of instrumentation on the temporal behavior of the application during \ac{e2e} testing.
				We will evaluate the impact by comparing the temporal behavior of the application during \ac{e2e} testing with and without instrumentation.
			\item[\textbf{\ac{rq}\textsubscript{2.2}:}] How does the instrumentation of coverage collection affect the failure rate of \ac{e2e} tests in the studied projects?

				We want to determine the impact of instrumentation on the failure rate of \ac{e2e} tests in the studied projects.
				We will evaluate the impact by comparing the failure rate of \ac{e2e} tests in the studied projects with and without instrumentation.
		\end{enumerate}
\end{enumerate}

\section{Case Studies}
\TODO{Bigger introduction to ArTEMiS and n8n}
\TODO{\textsc{ArTEMiS} was chosen because it is a large project with a lot of tests and a CI pipeline that experienced issues with flaky tests at the university}
\TODO{\textsc{Gladys} was a contender, but tests did not show any flakiness and there were also no indication in the commit messages}
\TODO{Additional project, \textsc{ToolJet} turned out to have a too complicated setup for their E2E tests}
\TODO{n8n was chosen from the list of projects with cypress and JavaScript and had issues with flaky tests before}
To validate our approach we chose two case studies.
Both case studies are open source projects that use \textsc{cypress} for \ac{e2e} testing.
They both have many tests and are actively maintained.
Also, both projects have experienced issues with flaky tests in the past.

\paragraph{ArTEMiS} is a learning management system that is used at multiple universities to manage courses and exercises \autocite{krusche_artemis_2018}.
The project was a good fit since it is also developed at \ac{tum}.
The team has experienced issues with flaky tests in the past and the project has many tests.

\paragraph{n8n} is a workflow automation tool that allows users to connect different services and automate workflows \autocite{noauthor_n8n_2023}. The project was identified as a case study by searching GitHub for projects that depend on \textsc{cypress} and use JavaScript as the main programming language. A further check of the commit history showed that the project had experienced issues with flaky tests in the past.

\paragraph{} During the search for case studies, we also considered \textsc{Gladys} \autocite{noauthor_gladys_2023} and \textsc{ToolJet} \autocite{noauthor_tooljettooljet_2023}. However, we decided against using them as case studies because \textsc{Galdys} did not show any flakiness in the tests and \textsc{ToolJet} had a too complicated setup for their \ac{e2e} tests.

\section{Evaluation Setup}
\TODO{Setup as part of the existing CI pipeline}
\TODO{Talk about issues with running ArTEMiS tests locally, too much setup needed and too much environment}
In this section, we discuss the evaluation setup used for our experiments.
Our evaluation is designed to be part of the existing \ac{ci} pipeline of the case studies.
Also, \textsc{ArTEMiS} has a very complex setup and is not easily runable locally.

\subsection{ArTEMiS (Bamboo)}
\TODO{Explain the setup of the ArTEMiS CI pipeline}
\TODO{Live and historic evaluation}
\TODO{Elaborate issues with the historic evaluation due to CI and code changes that had to be backported}
\TODO{Explain setup of live evaluation with duplicated plan}
\TODO{Specialty of the ArTEMiS CI pipeline, regular e2e tests as base}
We start by describing the setup of the \textsc{ArTEMiS} \ac{ci} pipeline.
The \textsc{ArTEMiS} \ac{ci} pipeline is hosted on \textsc{Bamboo} \autocite{atlassian_bamboo_nodate} and consists of multiple plans.
The plans are triggered by commits to the \textsc{ArTEMiS} GitHub repository.
We created a second version of the \ac{e2e} test plan that included our instrumentation.
This plan is running in parallel to the original plan.

Additionally, we selected builds from the regular \ac{e2e} test plan that were executed before the instrumentation was added.
We reran these builds with our instrumentation to test our approach on historic builds.
Due to the complex nature of the \textsc{ArTEMiS} \ac{ci} pipeline and ongoing development of the project, we had to backport some changes to the historic builds.
While we were able to run some historic builds, there were ongoing issues with the \ac{ci} pipeline that prevented us from running all historic builds.

The second part of the evaluation is the live evaluation.
We run the \ac{e2e} tests with and without instrumentation in parallel.
We compare the results of the two runs to determine the impact of the instrumentation on the \ac{e2e} tests.
Additionally, we can check the results of both runs to see if the approach correctly identifies failures as flaky.
If both plans fail, we check if the same tests failed in both runs.
Should there be a difference in the failed tests, we can conclude that the instrumentation correctly identified the failure as flaky.

\subsection{n8n (GitHub)} \label[section]{evaluation_n8n}
\TODO{Explain the setup of the n8n CI pipeline}
\TODO{Fork for historic evaluation}
\TODO{Issues with GitHub Runners due to memory limitations}
\TODO{Issues with tests after switching to chrome, exclusion of tests that did not pass in three runs}
\TODO{Setup of historic evaluation with baseline and comparisons}
\TODO{Explain the timing baseline collected on hosted runners}

Next, we discuss the setup of the \textsc{n8n} \ac{ci} evaluation.
The \textsc{n8n} \ac{ci} pipeline is hosted on \textsc{GitHub} and consists of multiple workflows.
To run the evaluation we created a fork of the \textsc{n8n} repository.\footnote{\url{https://github.com/heddendorp/n8n}}
In the forked repository we created a new workflow that runs the \ac{e2e} tests with and without instrumentation from a specific commit.
While our workflow is still pulling from the original repository, we had apply some changes to the \ac{e2e} tests to make them work with our instrumentation.
We had to run the tests in chrome instead of electron due to memory issues when executing the tests.
Additionally, we had to exclude some tests that did not pass in three runs, even after changing the test setup.

To evaluate the historic builds, we created a baseline of runs to know if the tests can pass for a specific commit.
The commits for the baseline were selected by taking up to five commits from the latest 50 \acp{pr} that were opened on the \textsc{n8n} repository.
We then ran the \ac{e2e} tests for each commit in the baseline and collected the results.
If a test failed in one of the runs, we triggered up to four additional runs to see if the test passes in the next run.
Additionally, for every commit in the baseline we also checked if it had at least one successful parent.
If a commit did not have a successful parent, we extended the baseline to include the parent commit.
We repeated this process until we had a baseline of commits that had at least one successful parent.

To answer \textbf{\ac{rq}\textsubscript{2}} we also ran another experiment.
We collected the 25 latest commits from the \textsc{n8n} repository and ran the \ac{e2e} tests with and without instrumentation on them.
We collected the information form these runs to judge the impact of the instrumentation on the \ac{e2e} tests.

\feedback{Fabian Leinen: Zusammenfassend gibt es ja drei Subjects zum Evaluieren: ArTEMiS Live, ArTEMiS Historic, n8n historic. Da musst du glaube ich drauf achten, dass es hier so klar wird und in der Results Section muss klar sein, was du womit beantwortest.}

\section{Evaluation Results}
In this section we show the quantitative results of our evaluation.
You can find the qualitative discussion in \cref{discussion}.
\subsection{\texorpdfstring{RQ\textsubscript{1}:}{RQ1:} How effective is the adapted DeFlaker approach in identifying flaky failures in \acs{ui} \acs{e2e} tests?}
\TODO{Review the gathered data for both case studies}
To answer \textbf{\ac{rq}\textsubscript{1}} we look at both, the \textsc{ArTEMiS} and \textsc{n8n} case studies.

\begin{table}[h]
	\centering
	\begin{adjustbox}{width=\textwidth, totalheight=\textheight, keepaspectratio}
		\csvautobooktabular{data/evaluationResult.csv}
	\end{adjustbox}
	\caption{Results of running the evaluation against the baseline for \textsc{n8n}}
	\label[table]{n8n_evaluation_results}
\end{table}

\begin{table}[h]
	\centering
	\begin{adjustbox}{width=\textwidth, totalheight=\textheight, keepaspectratio}
		\csvautobooktabular{data/artemis-evaluationResult.csv}
	\end{adjustbox}
	\caption{Worst case results of running the evaluation for \textsc{ArTEMiS}}
	\label[table]{artemis_evaluation_results}
\end{table}

As we are missing a baseline for the \textsc{ArTEMiS} case study, we can only compare the results of the live evaluation.
For this reason the results in \cref{artemis_evaluation_results} are the \textbf{worst case} results.
We defined \emph{true positives} as versions of the \ac{sut} for which the instrumented run failed, was labeled as \emph{suspected flaky} and the non-instrumented run passed or there was no overlap in the failed tests.
Additionally, we defined \emph{true negatives} as versions of the \ac{sut} for which the both runs failed, and the failed tests matched for at least 75\% of the tests.
This leads to an overestimation of both, the \emph{false positives} and the \emph{false negatives}.
We did however also calculate the \textbf{best case} results for \textsc{ArTEMiS} which showed moderate improvements with a precision of \num{0.81879} and a recall of \num{0.4357}.

\subsection{\texorpdfstring{RQ\textsubscript{2}:}{RQ2:} What impact does the instrumentation have on the behavior of the application and tests?}
To answer \textbf{\ac{rq}\textsubscript{2}} we look at the \textsc{n8n} case study.
As described in \cref{evaluation_n8n}, we ran the \ac{e2e} tests with and without instrumentation on the 25 latest commits.
\subsubsection{\texorpdfstring{RQ\textsubscript{2.1}:}{RQ2.1:} To what extent does instrumentation impact the temporal behavior of the application during \acs{e2e} testing?}
You can review the full table of test cases that failed during our evaluation in \cref{behaviour_change_results}.

As can be seen in \cref{duration_results_artemis} and \cref{artemis_time_boxplot} the average run times of the \ac{e2e} tests for \textsc{ArTEMiS} are higher when using the instrumentation.
To test for a statistically significant difference between the instrumentation and non-instrumentation groups, we used a t-test.
Before using the t-test, we checked the prerequisites for its use.
When checking for outliers we found that there were several extreme outliers, we decided to remove only the most extreme outlier from the \emph{failed build with instrumentation} group.
All other outliers were kept as they were valid results.
To assess the normality of the measurement results, we ran both the Kolmogorov-Smirnov test and the Shapiro-Wilk test.
Both tests showed that all groups of results were not normally distributed ($p < .001$).
As all groups contain more than 30 samples, we can use a t-test, as it has been shown to be robust against non-normality in this case. \addref
The t-test shows that the difference between the instrumentation and non-instrumentation groups is significant for failed builds ($t(424,807) = -9.155, p = < .001$) and passed builds ($t(20,897) = -2.21, p = .038$).

\begin{table}[h]
	\centering
	\csvautobooktabular{data/durationResults.csv}
	\caption{Average test case duration for the n8n \ac{e2e} tests with and without instrumentation}
	\label[table]{duration_results}
\end{table}

\begin{table}[h]
	\centering
	\csvautobooktabular{data/artemis-durationResults.csv}
	\caption{Average test case duration for the ArTEMiS \ac{e2e} tests with and without instrumentation}
	\label[table]{duration_results_artemis}
\end{table}

\begin{figure}[h]
	\centering
	\begin{tikzpicture}
		\begin{axis}[
				boxplot/draw direction=y,
				ylabel={Duration (min)},
				yticklabel={\pgfmathparse{exp(\tick)}\pgfmathprintnumber[fixed]{\pgfmathresult}},
				xtick={1, 2, 3, 4},
				xticklabels={Failed, Failed (instrumented), Passed, Passed (instrumented)},
				xticklabel style={align=center, font=\small, rotate=-20, anchor=north west},
				ymode=log,
				boxplot/box extend=0.4,
				boxplot/whisker extend=0.2,
			]
			\addplot+[boxplot] table[y index=0]{data/artemis/failedDurationsRegular.csv};
			\addplot+[boxplot] table[y index=0]{data/artemis/failedDurationsFlaky.csv};
			\addplot+[boxplot] table[y index=0]{data/artemis/passedDurationsRegular.csv};
			\addplot+[boxplot] table[y index=0]{data/artemis/passedDurationsFlaky.csv};
		\end{axis}
	\end{tikzpicture}
	\caption{Box plots of test runtimes for ArTEMiS}
	\label[figure]{artemis_time_boxplot}
\end{figure}

% \begin{figure}
% 	\centering
% 	\begin{tikzpicture}
% 		\begin{axis}[
% 				boxplot/draw direction=y,
% 				ylabel={Duration (min)},
% 				yticklabel={\pgfmathparse{exp(\tick)}\pgfmathprintnumber[fixed]{\pgfmathresult}},
% 				xtick={1, 2, 3, 4},
% 				xticklabels={Failed, Failed (instrumented), Passed, Passed (instrumented)},
% 				xticklabel style={align=center, font=\small, rotate=-20, anchor=north west},
% 				ymode=log,
% 				boxplot/box extend=0.4,
% 				boxplot/whisker extend=0.2,
% 			]
% 			\addplot+[boxplot] table[y index=0]{data/n8n/failedDurationsRegular.csv};
% 			\addplot+[boxplot] table[y index=0]{data/n8n/failedDurationsFlaky.csv};
% 			\addplot+[boxplot] table[y index=0]{data/n8n/passedDurationsRegular.csv};
% 			\addplot+[boxplot] table[y index=0]{data/n8n/passedDurationsFlaky.csv};
% 		\end{axis}
% 	\end{tikzpicture}
% 	\caption{Box plots of test runtimes for n8n}
% 	\label{fig:n8n_time_boxplot}
% \end{figure}

\subsubsection{\texorpdfstring{RQ\textsubscript{2.2}:}{RQ2.2:} How does the instrumentation of coverage collection affect the failure rate of \acs{e2e} tests in the studied projects?}
Or experiment shows that the instrumentation of coverage collection does not have a significant impact on the failure rate of \acs{e2e} tests.
As you can see in \cref{failure_rate_results}, the failure rate of the \ac{e2e} tests is almost identical for both runs.
We also collected the failure rate on a test case level and compared the results.
As you can see in \cref{testcase_results}, the failure rate of the test cases is also almost identical for both runs.
\begin{table}[h]
	\centering
	\csvautobooktabular{data/runResults.csv}
	\caption{Statistics of the evaluation runs for n8n}
	\label[table]{failure_rate_results}
\end{table}

\begin{table}[h]
	\centering
	\csvautobooktabular{data/testcaseResults.csv}
	\caption{Statistics of test case results during evaluation for n8n}
	\label[table]{testcase_results}
\end{table}

\begin{table}[h]
	\centering
	\csvautobooktabular{data/artemis-runResults.csv}
	\caption{Statistics of the live CI runs for ArTEMiS}
	\label[table]{failure_rate_results_artemis}
\end{table}

\begin{table}[h]
	\centering
	\csvautobooktabular{data/artemis-runResults-withReruns.csv}
	\caption{Statistics of live CI runs for ArTEMiS when considering reruns as failures}
	\label[table]{failure_rate_results-rerun_artemis}
\end{table}
\TODO{Review the gathered data for both case studies}


% \subsection{Additional Observations}
% \TODO{Issues with memory}
% \feedback{Fabian Leinen: Was kommt alles in diese Subsection? Mein Gefühl ist, dass es nicht viel ist und das auch ganz gut bei RQ\textsubscript{1} oder Discussion rein passt.}

\section{Discussion} \label[section]{discussion}
\TODO{Give an overview of the results}
\feedback{Fabian Leinen: Dies Discussion kann wie hier in einer eigenen Section sein, Dann achte bloß bei den Results etwas drauf nicht zu sehr Interpretationen und so weiter einzubauen sondern möglichst trocken die (quantitativen) Ergebnisse zu präsentieren.}

\subsection{Quality of Failure Categorization}
\TODO{Interpret the results of RQ\textsubscript{1}}

\subsection{Change of Behavior caused by Instrumentation}
\TODO{Interpret the results of RQ\textsubscript{2.1}}
\TODO{Show the impact on RQ\textsubscript{1}}
\TODO{Interpret the results of RQ\textsubscript{2.2} for both case studies}

%------- chapter 6 -------

\chapter{Threats to Validity}\label[chapter]{threats}
\TODO{Write chapter introduction}

\section{Threats to Internal Validity}
\TODO{Dependence on the CI pipeline}
\TODO{Missing confirmed failures}
\TODO{Limited insights to non code changes and test case changes}

\section{Threats to External Validity}
\TODO{Only two case studies}
\TODO{Limited availability of run labels}


%------- chapter 7 -------

\chapter{Conclusion}\label[chapter]{conclusion}

\TODO{Summary of the results}
\TODO{Summary of the implementation and evaluation}


\section{Future Work}

\TODO{Replicate tests with other test frameworks such as Playwright}
\TODO{Extend analysis to other languages}
\TODO{Investigate ways to mitigate the impact of instrumentation on test execution}
\TODO{Make the analysis more robust for non code changes and test case changes}

\appendix

\chapter[Behaviour change results]{Extensive results for the behavior change evaluation}\label[appendix]{behaviour_change_results}

\begin{adjustbox}{max width=\textwidth, caption={Statistics of testcase results during evaluation}, float=table}
	\begin{tabular}{@{}lrrrrr@{}} \toprule
		\multicolumn{2}{c}{} & \multicolumn{2}{c}{Uninstrumented} & \multicolumn{2}{c}{Instrumented}                                                       \\ \cmidrule{3-4} \cmidrule{5-6}
		Test name            & Duration increase \%               & passed                           & failed  & passed              & failed              \\ \midrule
		\csvreader[
			head to column names
		]{data/testcaseStats.csv}{}{%
		\name                & \durationIncrease                  & \passed                          & \failed & \passedInstrumented & \failedInstrumented \\
		}
		\\ \bottomrule
	\end{tabular}
\end{adjustbox}

\chapter{Referenced code} \label[appendix]{code}

\lstinputlisting[language=yaml,caption={Docker compose override file for coverage collection},label=coverage_compose]{code/cypress-E2E-tests-coverage-override.yml}

\microtypesetup{protrusion=false}
\include{common/acronyms}
\listoffigures{}
\listoftables{}
\lstlistoflistings{}
\microtypesetup{protrusion=true}
\printbibliography{}

\end{document}
