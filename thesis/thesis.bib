
@inproceedings{bell_deflaker_2018,
	address = {Gothenburg Sweden},
	title = {{DeFlaker}: automatically detecting flaky tests},
	isbn = {978-1-4503-5638-1},
	shorttitle = {{DeFlaker}},
	url = {https://dl.acm.org/doi/10.1145/3180155.3180164},
	doi = {10.1145/3180155.3180164},
	abstract = {Developers often run tests to check that their latest changes to a code repository did not break any previously working functionality. Ideally, any new test failures would indicate regressions caused by the latest changes. However, some test failures may not be due to the latest changes but due to non-determinism in the tests, popularly called flaky tests. The typical way to detect flaky tests is to rerun failing tests repeatedly. Unfortunately, rerunning failing tests can be costly and can slow down the development cycle.},
	language = {en},
	urldate = {2022-10-31},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Bell, Jonathan and Legunsen, Owolabi and Hilton, Michael and Eloussi, Lamyaa and Yung, Tifany and Marinov, Darko},
	month = may,
	year = {2018},
	pages = {433--444},
	file = {Bell et al. - 2018 - D e .pdf:C\:\\Users\\hedde\\Zotero\\storage\\S9QKTSFZ\\Bell et al. - 2018 - D e .pdf:application/pdf},
}

@inproceedings{luo_empirical_2014,
	address = {Hong Kong China},
	title = {An empirical analysis of flaky tests},
	isbn = {978-1-4503-3056-5},
	url = {https://dl.acm.org/doi/10.1145/2635868.2635920},
	doi = {10.1145/2635868.2635920},
	abstract = {Regression testing is a crucial part of software development. It checks that software changes do not break existing functionality. An important assumption of regression testing is that test outcomes are deterministic: an unmodiﬁed test is expected to either always pass or always fail for the same code under test. Unfortunately, in practice, some tests—often called ﬂaky tests—have non-deterministic outcomes. Such tests undermine the regression testing as they make it diﬃcult to rely on test results.},
	language = {en},
	urldate = {2022-10-31},
	booktitle = {Proceedings of the 22nd {ACM} {SIGSOFT} {International} {Symposium} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Luo, Qingzhou and Hariri, Farah and Eloussi, Lamyaa and Marinov, Darko},
	month = nov,
	year = {2014},
	pages = {643--653},
	file = {Luo et al. - 2014 - An empirical analysis of flaky tests.pdf:C\:\\Users\\hedde\\Zotero\\storage\\HCWMCLQR\\Luo et al. - 2014 - An empirical analysis of flaky tests.pdf:application/pdf},
}

@inproceedings{romano_empirical_2021,
	title = {An {Empirical} {Analysis} of {UI}-{Based} {Flaky} {Tests}},
	doi = {10.1109/ICSE43902.2021.00141},
	abstract = {Flaky tests have gained attention from the research community in recent years and with good reason. These tests lead to wasted time and resources, and they reduce the reliability of the test suites and build systems they affect. However, most of the existing work on flaky tests focus exclusively on traditional unit tests. This work ignores UI tests that have larger input spaces and more diverse running conditions than traditional unit tests. In addition, UI tests tend to be more complex and resource-heavy, making them unsuited for detection techniques involving rerunning test suites multiple times. In this paper, we perform a study on flaky UI tests. We analyze 235 flaky UI test samples found in 62 projects from both web and Android environments. We identify the common underlying root causes of flakiness in the UI tests, the strategies used to manifest the flaky behavior, and the fixing strategies used to remedy flaky UI tests. The findings made in this work can provide a foundation for the development of detection and prevention techniques for flakiness arising in UI tests.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Romano, Alan and Song, Zihe and Grandhi, Sampath and Yang, Wei and Wang, Weihang},
	month = may,
	year = {2021},
	note = {ISSN: 1558-1225},
	keywords = {Android, flaky test, flaky UI test, Reliability, Software development management, Software engineering, web},
	pages = {1585--1597},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\hedde\\Zotero\\storage\\I7ERXXND\\9402129.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\L84ZQFR8\\Romano et al. - 2021 - An Empirical Analysis of UI-Based Flaky Tests.pdf:application/pdf},
}

@misc{micco_state_2017,
	title = {The {State} of {Continuous} {Integration} {Testing} @{Google}},
	url = {https://research.google/pubs/pub45880/},
	author = {Micco, John},
	year = {2017},
	file = {Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\U25FP7TS\\Micco - 2017 - The State of Continuous Integration Testing @Googl.pdf:application/pdf},
}

@inproceedings{hilton_large-scale_2018,
	address = {New York, NY, USA},
	series = {{ASE} 2018},
	title = {A large-scale study of test coverage evolution},
	isbn = {978-1-4503-5937-5},
	url = {https://doi.org/10.1145/3238147.3238183},
	doi = {10.1145/3238147.3238183},
	abstract = {Statement coverage is commonly used as a measure of test suite quality. Coverage is often used as a part of a code review process: if a patch decreases overall coverage, or is itself not covered, then the patch is scrutinized more closely. Traditional studies of how coverage changes with code evolution have examined the overall coverage of the entire program, and more recent work directly examines the coverage of patches (changed statements). We present an evaluation much larger than prior studies and moreover consider a new, important kind of change --- coverage changes of unchanged statements. We present a large-scale evaluation of code coverage evolution over 7,816 builds of 47 projects written in popular languages including Java, Python, and Scala. We find that in large, mature projects, simply measuring the change to statement coverage does not capture the nuances of code evolution. Going beyond considering statement coverage as a simple ratio, we examine how the set of statements covered evolves between project revisions. We present and study new ways to assess the impact of a patch on a project's test suite quality that both separates coverage of the patch from coverage of the non-patch, and separates changes in coverage from changes in the set of statements covered.},
	urldate = {2022-11-06},
	booktitle = {Proceedings of the 33rd {ACM}/{IEEE} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Hilton, Michael and Bell, Jonathan and Marinov, Darko},
	month = sep,
	year = {2018},
	keywords = {flaky tests, code coverage, empirical study, Software testing},
	pages = {53--63},
	file = {Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\IUJXS3ZD\\Hilton et al. - 2018 - A large-scale study of test coverage evolution.pdf:application/pdf},
}

@inproceedings{ziftci_-flake_2020,
	title = {De-{Flake} {Your} {Tests} : {Automatically} {Locating} {Root} {Causes} of {Flaky} {Tests} in {Code} {At} {Google}},
	shorttitle = {De-{Flake} {Your} {Tests}},
	doi = {10.1109/ICSME46990.2020.00083},
	abstract = {Regression testing is a critical part of software development and maintenance. It ensures that modifications to existing software do not break existing behavior and functionality.One of the key assumptions about regression tests is that their results are deterministic: when executed without any modifications with the same configuration, either they always fail or they always pass. In practice, however, there exist tests that are non-deterministic, called flaky tests. Flaky tests cause the results of test runs to be unreliable, and they disrupt the software development workflow. In this paper, we present a novel technique to automatically identify the locations of the root causes of flaky tests on the code level to help developers debug and fix them. We study the technique on flaky tests across 428 projects at Google. Based on our case studies, the technique helps identify the location of the root causes of flakiness with 82\% accuracy. Furthermore, our studies show that integration into the appropriate developer workflows, simplicity of debugging aides and fully automated fixes are crucial and preferred components for adoption and usability of flakiness debugging and fixing tools.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Ziftci, Celal and Cavalcanti, Diego},
	month = sep,
	year = {2020},
	note = {ISSN: 2576-3148},
	keywords = {Debuggers, Debugging, Debugging aids, Diagnostics, Flaky tests, Internet, Maintenance engineering, Software, Software maintenance, Test management, Testing, Tools, Tracing, Usability},
	pages = {736--745},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\hedde\\Zotero\\storage\\NZMJNENF\\9240685.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\8MRI7TL6\\Ziftci and Cavalcanti - 2020 - De-Flake Your Tests  Automatically Locating Root .pdf:application/pdf},
}

@inproceedings{moran_debugging_2019,
	address = {Vienna, Austria},
	title = {Debugging {Flaky} {Tests} on {Web} {Applications}:},
	isbn = {978-989-758-386-5},
	shorttitle = {Debugging {Flaky} {Tests} on {Web} {Applications}},
	url = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0008559004540461},
	doi = {10.5220/0008559004540461},
	language = {en},
	urldate = {2022-11-06},
	booktitle = {Proceedings of the 15th {International} {Conference} on {Web} {Information} {Systems} and {Technologies}},
	publisher = {SCITEPRESS - Science and Technology Publications},
	author = {Morán, Jesus and Augusto, Cristian and Bertolino, Antonia and de la Riva, Claudio and Tuya, Javier},
	year = {2019},
	pages = {454--461},
	file = {Morán et al. - 2019 - Debugging Flaky Tests on Web Applications.pdf:C\:\\Users\\hedde\\Zotero\\storage\\ZE66V642\\Morán et al. - 2019 - Debugging Flaky Tests on Web Applications.pdf:application/pdf},
}

@inproceedings{kowalczyk_modeling_2020,
	address = {New York, NY, USA},
	series = {{ICSE}-{SEIP} '20},
	title = {Modeling and ranking flaky tests at {Apple}},
	isbn = {978-1-4503-7123-0},
	url = {http://doi.org/10.1145/3377813.3381370},
	doi = {10.1145/3377813.3381370},
	abstract = {Test flakiness---inability to reliably repeat a test's Pass/Fail outcome---continues to be a significant problem in Industry, adversely impacting continuous integration and test pipelines. Completely eliminating flaky tests is not a realistic option as a significant fraction of system tests (typically non-hermetic) for services-based implementations exhibit some level of flakiness. In this paper, we view the flakiness of a test as a rankable value, which we quantify, track and assign a confidence. We develop two ways to model flakiness, capturing the randomness of test results via entropy, and the temporal variation via flipRate, and aggregating these over time. We have implemented our flakiness scoring service and discuss how its adoption has impacted test suites of two large services at Apple. We show how flakiness is distributed across the tests in these services, including typical score ranges and outliers. The flakiness scores are used to monitor and detect changes in flakiness trends. Evaluation results demonstrate near perfect accuracy in ranking, identification and alignment with human interpretation. The scores were used to identify 2 causes of flakiness in the dataset evaluated, which have been confirmed, and where fixes have been implemented or are underway. Our models reduced flakiness by 44\% with less than 1\% loss in fault detection.},
	urldate = {2022-11-06},
	booktitle = {Proceedings of the {ACM}/{IEEE} 42nd {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice}},
	publisher = {Association for Computing Machinery},
	author = {Kowalczyk, Emily and Nair, Karan and Gao, Zebao and Silberstein, Leo and Long, Teng and Memon, Atif},
	month = sep,
	year = {2020},
	pages = {110--119},
	file = {Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\S7DQAA2G\\Kowalczyk et al. - 2020 - Modeling and ranking flaky tests at Apple.pdf:application/pdf},
}

@inproceedings{machalica_predictive_2019,
	title = {Predictive {Test} {Selection}},
	doi = {10.1109/ICSE-SEIP.2019.00018},
	abstract = {Change-based testing is a key component of continuous integration at Facebook. However, a large number of tests coupled with a high rate of changes committed to our monolithic repository make it infeasible to run all potentially-impacted tests on each change. We propose a new predictive test selection strategy which selects a subset of tests to exercise for each change submitted to the continuous integration system. The strategy is learned from a large dataset of historical test outcomes using basic machine learning techniques. Deployed in production, the strategy reduces the total infrastructure cost of testing code changes by a factor of two, while guaranteeing that over 95\% of individual test failures and over 99.9\% of faulty changes are still reported back to developers. The method we present here also accounts for the non-determinism of test outcomes, also known as test flakiness.},
	booktitle = {2019 {IEEE}/{ACM} 41st {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} ({ICSE}-{SEIP})},
	author = {Machalica, Mateusz and Samylkin, Alex and Porth, Meredith and Chandra, Satish},
	month = may,
	year = {2019},
	keywords = {flaky tests, Testing, Tools, continuous integration, Diamond, Facebook, Libraries, machine learning, Machine learning, Metadata, test selection},
	pages = {91--100},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\hedde\\Zotero\\storage\\GRH39I95\\8804462.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\9TQM863C\\Machalica et al. - 2019 - Predictive Test Selection.pdf:application/pdf},
}

@inproceedings{yu_terminator_2019,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2019},
	title = {{TERMINATOR}: better automated {UI} test case prioritization},
	isbn = {978-1-4503-5572-8},
	shorttitle = {{TERMINATOR}},
	url = {http://doi.org/10.1145/3338906.3340448},
	doi = {10.1145/3338906.3340448},
	abstract = {Automated UI testing is an important component of the continuous integration process of software development. A modern web-based UI is an amalgam of reports from dozens of microservices written by multiple teams. Queries on a page that opens up another will fail if any of that page's microservices fails. As a result, the overall cost for automated UI testing is high since the UI elements cannot be tested in isolation. For example, the entire automated UI testing suite at LexisNexis takes around 30 hours (3-5 hours on the cloud) to execute, which slows down the continuous integration process. To mitigate this problem and give developers faster feedback on their code, test case prioritization techniques are used to reorder the automated UI test cases so that more failures can be detected earlier. Given that much of the automated UI testing is "black box" in nature, very little information (only the test case descriptions and testing results) can be utilized to prioritize these automated UI test cases. Hence, this paper evaluates 17 "black box" test case prioritization approaches that do not rely on source code information. Among these, we propose a novel TCP approach, that dynamically re-prioritizes the test cases when new failures are detected, by applying and adapting a state of the art framework from the total recall problem. Experimental results on LexisNexis automated UI testing data show that our new approach (which we call TERMINATOR), outperformed prior state of the art approaches in terms of failure detection rates with negligible CPU overhead.},
	urldate = {2022-11-06},
	booktitle = {Proceedings of the 2019 27th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Yu, Zhe and Fahid, Fahmid and Menzies, Tim and Rothermel, Gregg and Patrick, Kyle and Cherian, Snehit},
	month = aug,
	year = {2019},
	keywords = {automated UI testing, test case prioritization, total recall},
	pages = {883--894},
	file = {Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\6N3BMGEX\\Yu et al. - 2019 - TERMINATOR better automated UI test case prioriti.pdf:application/pdf},
}

@inproceedings{shi_understanding_2019,
	title = {Understanding and {Improving} {Regression} {Test} {Selection} in {Continuous} {Integration}},
	doi = {10.1109/ISSRE.2019.00031},
	abstract = {Developers rely on regression testing in their continuous integration (CI) environment to find changes that introduce regression faults. While regression testing is widely practiced, it can be costly. Regression test selection (RTS) reduces the cost of regression testing by not running the tests that are unaffected by the changes. Industry has adopted module-level RTS for their CI environment, while researchers have proposed class-level RTS. In this paper, we compare module-and class-level RTS techniques in a cloud-based CI environment, Travis. We also develop and evaluate a hybrid RTS technique that combines aspects of the module-and class-level RTS techniques. We evaluate all the techniques on real Travis builds. We find that the RTS techniques do save testing time compared to running all tests (RetestAll), but the percentage of time for a full build using RTS (76.0\%) is not as low as found in previous work, due to the extra overhead in a cloud-based CI environment. Moreover, we inspect test failures from RetestAll builds, and although we find that RTS techniques can miss to select failed tests, these test failures are almost all flaky test failures. As such, RTS techniques provide additional value in helping developers avoid wasting time debugging failures not related to the recent code changes. Overall, our results show that RTS can be beneficial for the developers in the CI environment, and RTS not only saves time but also avoids misleading developers by flaky test failures.},
	booktitle = {2019 {IEEE} 30th {International} {Symposium} on {Software} {Reliability} {Engineering} ({ISSRE})},
	author = {Shi, August and Zhao, Peiyuan and Marinov, Darko},
	month = oct,
	year = {2019},
	note = {ISSN: 2332-6549},
	keywords = {flaky tests, continuous integration, regression test selection},
	pages = {228--238},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\hedde\\Zotero\\storage\\MNDBAJLG\\8987498.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\YTDAEN4S\\Shi et al. - 2019 - Understanding and Improving Regression Test Select.pdf:application/pdf},
}

@inproceedings{presler-marshall_wait_2019,
	title = {Wait, {Wait}. {No}, {Tell} {Me}. {Analyzing} {Selenium} {Configuration} {Effects} on {Test} {Flakiness}},
	doi = {10.1109/AST.2019.000-1},
	abstract = {Flaky tests are a source of frustration and uncertainty for developers. In an educational environment, flaky tests can create doubts related to software behavior and student grades, especially when the grades depend on tests passing. NC State University's junior-level software engineering course models industrial practice through team-based development and testing of new features on a large electronic health record (EHR) system, iTrust2. Students are expected to maintain and supplement an extensive suite of UI tests using Selenium WebDriver. Team builds are run on the course's continuous integration (CI) infrastructure. Students report, and we confirm, that tests that pass on one build will inexplicably fail on the next, impacting productivity and confidence in code quality and the CI system. The goal of this work is to find and fix the sources of flaky tests in iTrust2. We analyze configurations of Selenium using different underlying web browsers and timeout strategies (waits) for both test stability and runtime performance. We also consider underlying hardware and operating systems. Our results show that HtmlUnit with Thread waits provides the lowest number of test failures and best runtime on poor-performing hardware. When given more resources (e.g., more memory and a faster CPU), Google Chrome with Angular waits is less flaky and faster than HtmlUnit, especially if the browser instance is not restarted between tests. The outcomes of this research are a more stable and substantially faster teaching application and a recommendation on how to configure Selenium for applications similar to iTrust2 that run in a CI environment.},
	booktitle = {2019 {IEEE}/{ACM} 14th {International} {Workshop} on {Automation} of {Software} {Test} ({AST})},
	author = {Presler-Marshall, Kai and Horton, Eric and Heckman, Sarah and Stolee, Kathryn},
	month = may,
	year = {2019},
	keywords = {Browsers, Flaky Tests, GUI Tests, Hardware, Operating systems, Runtime, Selenium, Software Testing, Vehicles, WebDriver},
	pages = {7--13},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\hedde\\Zotero\\storage\\9CJXCPXX\\8821891.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\JD44DVWK\\Presler-Marshall et al. - 2019 - Wait, Wait. No, Tell Me. Analyzing Selenium Config.pdf:application/pdf},
}

@inproceedings{gao_which_2015,
	title = {Which of {My} {Failures} are {Real}? {Using} {Relevance} {Ranking} to {Raise} {True} {Failures} to the {Top}},
	shorttitle = {Which of {My} {Failures} are {Real}?},
	doi = {10.1109/ASEW.2015.7},
	abstract = {GUI reference testing is performed to detect regression errors in a modified or patched GUI software. Test cases are executed on the original and modified GUIs, differences in the states of GUI widgets across versions indicate potential defects. However, various factors (e.g., position, flakiness, resolution) create problems for accurate GUI state collection, leading to spurious state mismatches, and hence false positives, these need to be weeded out manually. In this paper, we show that the problem of false positives is significant, often inundating the tester with a large number of false bug reports, requiring a disproportionate amount of manual effort. We develop an entropy-based approach to rank each GUI widget property, and use it to determine whether a state mismatch (indicative of a bug) is real or a false positive. Our empirical evaluation shows that this ranking helps to percolate real bugs to the top of a set of reported bugs, thereby helping to economize tester time.},
	booktitle = {2015 30th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} {Workshop} ({ASEW})},
	author = {Gao, Zebao and Memon, Atif M.},
	month = nov,
	year = {2015},
	keywords = {Software, Testing, Computer bugs, Computer science, Electronic mail, Entropy, Graphical user interfaces},
	pages = {62--69},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\hedde\\Zotero\\storage\\KNI6UNKA\\7426638.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\NRTT94NP\\Gao and Memon - 2015 - Which of My Failures are Real Using Relevance Ran.pdf:application/pdf},
}

@inproceedings{shi_ifixflakies_2019,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2019},
	title = {{iFixFlakies}: a framework for automatically fixing order-dependent flaky tests},
	isbn = {978-1-4503-5572-8},
	shorttitle = {{iFixFlakies}},
	url = {http://doi.org/10.1145/3338906.3338925},
	doi = {10.1145/3338906.3338925},
	abstract = {Regression testing provides important pass or fail signals that developers use to make decisions after code changes. However, flaky tests, which pass or fail even when the code has not changed, can mislead developers. A common kind of flaky tests are order-dependent tests, which pass or fail depending on the order in which the tests are run. Fixing order-dependent tests is often tedious and time-consuming. We propose iFixFlakies, a framework for automatically fixing order-dependent tests. The key insight in iFixFlakies is that test suites often already have tests, which we call helpers, whose logic resets or sets the states for order-dependent tests to pass. iFixFlakies searches a test suite for helpers that make the order-dependent tests pass and then recommends patches for the order-dependent tests using code from these helpers. Our evaluation on 110 truly orderdependent tests from a public dataset shows that 58 of them have helpers, and iFixFlakies can fix all 58. We opened pull requests for 56 order-dependent tests (2 of 58 were already fixed), and developers have already accepted pull requests for 21 of them, with all the remaining ones still pending.},
	urldate = {2022-11-06},
	booktitle = {Proceedings of the 2019 27th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Shi, August and Lam, Wing and Oei, Reed and Xie, Tao and Marinov, Darko},
	month = aug,
	year = {2019},
	keywords = {flaky test, automated fixing, order-dependent test, patch generation},
	pages = {545--555},
	file = {Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\S3GH48QX\\Shi et al. - 2019 - iFixFlakies a framework for automatically fixing .pdf:application/pdf},
}

@inproceedings{eck_understanding_2019,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2019},
	title = {Understanding flaky tests: the developer’s perspective},
	isbn = {978-1-4503-5572-8},
	shorttitle = {Understanding flaky tests},
	url = {http://doi.org/10.1145/3338906.3338945},
	doi = {10.1145/3338906.3338945},
	abstract = {Flaky tests are software tests that exhibit a seemingly random outcome (pass or fail) despite exercising unchanged code. In this work, we examine the perceptions of software developers about the nature, relevance, and challenges of flaky tests. We asked 21 professional developers to classify 200 flaky tests they previously fixed, in terms of the nature and the origin of the flakiness, as well as of the fixing effort. We also examined developers' fixing strategies. Subsequently, we conducted an online survey with 121 developers with a median industrial programming experience of five years. Our research shows that: The flakiness is due to several different causes, four of which have never been reported before, despite being the most costly to fix; flakiness is perceived as significant by the vast majority of developers, regardless of their team's size and project's domain, and it can have effects on resource allocation, scheduling, and the perceived reliability of the test suite; and the challenges developers report to face regard mostly the reproduction of the flaky behavior and the identification of the cause for the flakiness. Public preprint [http://arxiv.org/abs/1907.01466], data and materials [https://doi-org.eaccess.ub.tum.de/10.5281/zenodo.3265785].},
	urldate = {2022-11-06},
	booktitle = {Proceedings of the 2019 27th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Eck, Moritz and Palomba, Fabio and Castelluccio, Marco and Bacchelli, Alberto},
	month = aug,
	year = {2019},
	keywords = {Flaky Tests, Empirical Studies, Mixed-Method Research},
	pages = {830--840},
	file = {Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\392ZMRZN\\Eck et al. - 2019 - Understanding flaky tests the developer’s perspec.pdf:application/pdf},
}

@inproceedings{gao_making_2015,
	title = {Making {System} {User} {Interactive} {Tests} {Repeatable}: {When} and {What} {Should} {We} {Control}?},
	volume = {1},
	shorttitle = {Making {System} {User} {Interactive} {Tests} {Repeatable}},
	doi = {10.1109/ICSE.2015.28},
	abstract = {System testing and invariant detection is usually conducted from the user interface perspective when the goal is to evaluate the behavior of an application as a whole. A large number of tools and techniques have been developed to generate and automate this process, many of which have been evaluated in the literature or internally within companies. Typical metrics for determining effectiveness of these techniques include code coverage and fault detection, however, with the assumption that there is determinism in the resulting outputs. In this paper we examine the extent to which a common set of factors such as the system platform, Java version, application starting state and tool harness configurations impact these metrics. We examine three layers of testing outputs: the code layer, the behavioral (or invariant) layer and the external (or user interaction) layer. In a study using five open source applications across three operating system platforms, manipulating several factors, we observe as many as 184 lines of code coverage difference between runs using the same test cases, and up to 96 percent false positives with respect to fault detection. We also see some a small variation among the invariants inferred. Despite our best efforts, we can reduce, but not completely eliminate all possible variation in the output. We use our findings to provide a set of best practices that should lead to better consistency and smaller differences in test outcomes, allowing more repeatable and reliable testing and experimentation.},
	booktitle = {2015 {IEEE}/{ACM} 37th {IEEE} {International} {Conference} on {Software} {Engineering}},
	author = {Gao, Zebao and Liang, Yalan and Cohen, Myra B. and Memon, Atif M. and Wang, Zhen},
	month = may,
	year = {2015},
	note = {ISSN: 1558-1225},
	keywords = {Testing, Operating systems, Entropy, Graphical user interfaces, benchmarking, Delays, experimentation, graphical user interfaces, Java, software testing},
	pages = {55--65},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\hedde\\Zotero\\storage\\JWPDQR8T\\7194561.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\7ML3PCSZ\\Gao et al. - 2015 - Making System User Interactive Tests Repeatable W.pdf:application/pdf},
}

@inproceedings{lam_dependent-test-aware_2020,
	address = {New York, NY, USA},
	series = {{ISSTA} 2020},
	title = {Dependent-test-aware regression testing techniques},
	isbn = {978-1-4503-8008-9},
	url = {http://doi.org/10.1145/3395363.3397364},
	doi = {10.1145/3395363.3397364},
	abstract = {Developers typically rely on regression testing techniques to ensure that their changes do not break existing functionality. Unfortunately, these techniques suffer from flaky tests, which can both pass and fail when run multiple times on the same version of code and tests. One prominent type of flaky tests is order-dependent (OD) tests, which are tests that pass when run in one order but fail when run in another order. Although OD tests may cause flaky-test failures, OD tests can help developers run their tests faster by allowing them to share resources. We propose to make regression testing techniques dependent-test-aware to reduce flaky-test failures. To understand the necessity of dependent-test-aware regression testing techniques, we conduct the first study on the impact of OD tests on three regression testing techniques: test prioritization, test selection, and test parallelization. In particular, we implement 4 test prioritization, 6 test selection, and 2 test parallelization algorithms, and we evaluate them on 11 Java modules with OD tests. When we run the orders produced by the traditional, dependent-test-unaware regression testing algorithms, 82\% of human-written test suites and 100\% of automatically-generated test suites with OD tests have at least one flaky-test failure. We develop a general approach for enhancing regression testing algorithms to make them dependent-test-aware, and apply our approach to 12 algorithms. Compared to traditional, unenhanced regression testing algorithms, the enhanced algorithms use provided test dependencies to produce orders with different permutations or extra tests. Our evaluation shows that, in comparison to the orders produced by unenhanced algorithms, the orders produced by enhanced algorithms (1) have overall 80\% fewer flaky-test failures due to OD tests, and (2) may add extra tests but run only 1\% slower on average. Our results suggest that enhancing regression testing algorithms to be dependent-test-aware can substantially reduce flaky-test failures with only a minor slowdown to run the tests.},
	urldate = {2022-11-06},
	booktitle = {Proceedings of the 29th {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Lam, Wing and Shi, August and Oei, Reed and Zhang, Sai and Ernst, Michael D. and Xie, Tao},
	month = jul,
	year = {2020},
	keywords = {flaky test, order-dependent test, regression testing},
	pages = {298--311},
	file = {Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\NZ68F9S8\\Lam et al. - 2020 - Dependent-test-aware regression testing techniques.pdf:application/pdf},
}

@inproceedings{gambi_practical_2018,
	title = {Practical {Test} {Dependency} {Detection}},
	doi = {10.1109/ICST.2018.00011},
	abstract = {Regression tests should consistently produce the same outcome when executed against the same version of the system under test. Recent studies, however, show a different picture: in many cases simply changing the order in which tests execute is enough to produce different test outcomes. These studies also identify the presence of dependencies between tests as one likely cause of this behavior. Test dependencies affect the quality of tests and of the correlated development activities, like regression test selection, prioritization, and parallelization, which assume that tests are independent. Therefore, developers must promptly identify and resolve problematic test dependencies. This paper presents PRADET, a novel approach for detecting problematic dependencies that is both effective and efficient. PRADET uses a systematic, data-driven process to detect problematic test dependencies significantly faster and more precisely than prior work. PRADET scales to analyze large projects with thousands of tests that existing tools cannot analyze in reasonable amount of time, and found 27 previously unknown dependencies.},
	booktitle = {2018 {IEEE} 11th {International} {Conference} on {Software} {Testing}, {Verification} and {Validation} ({ICST})},
	author = {Gambi, Alessio and Bell, Jonathan and Zeller, Andreas},
	month = apr,
	year = {2018},
	keywords = {flaky tests, empirical study, Testing, Tools, Computer bugs, data-flow, detection algorithm, Minimization, Out of order, Pollution, Test dependence},
	pages = {1--11},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\hedde\\Zotero\\storage\\XJH4KSYU\\8367031.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\UW27Y3MB\\Gambi et al. - 2018 - Practical Test Dependency Detection.pdf:application/pdf},
}

@inproceedings{lam_idflakies_2019,
	title = {{iDFlakies}: {A} {Framework} for {Detecting} and {Partially} {Classifying} {Flaky} {Tests}},
	shorttitle = {{iDFlakies}},
	doi = {10.1109/ICST.2019.00038},
	abstract = {Regression testing is increasingly important with the wide use of continuous integration. A desirable requirement for regression testing is that a test failure reliably indicates a problem in the code under test and not a false alarm from the test code or the testing infrastructure. However, some test failures are unreliable, stemming from flaky tests that can nondeterministically pass or fail for the same code under test. There are many types of flaky tests, with order-dependent tests being a prominent type. To help advance research on flaky tests, we present (1) a framework, iDFlakies, to detect and partially classify flaky tests; (2) a dataset of flaky tests in open-source projects; and (3) a study with our dataset. iDFlakies automates experimentation with our tool for Maven-based Java projects. Using iDFlakies, we build a dataset of 422 flaky tests, with 50.5\% order-dependent and 49.5\% not. Our study of these flaky tests finds the prevalence of two types of flaky tests, probability of a test-suite run to have at least one failure due to flaky tests, and how different test reorderings affect the number of detected flaky tests. We envision that our work can spur research to alleviate the problem of flaky tests.},
	booktitle = {2019 12th {IEEE} {Conference} on {Software} {Testing}, {Validation} and {Verification} ({ICST})},
	author = {Lam, Wing and Oei, Reed and Shi, August and Marinov, Darko and Xie, Tao},
	month = apr,
	year = {2019},
	note = {ISSN: 2159-4848},
	keywords = {flaky tests, Reliability, Testing, Tools, Java, regression testing, Concurrent computing, Open source software, order dependent tests, Servers},
	pages = {312--322},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\hedde\\Zotero\\storage\\CZSVB7VF\\8730188.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\4TNPIMX2\\Lam et al. - 2019 - iDFlakies A Framework for Detecting and Partially.pdf:application/pdf},
}

@inproceedings{biagiola_web_2019,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2019},
	title = {Web test dependency detection},
	isbn = {978-1-4503-5572-8},
	url = {http://doi.org/10.1145/3338906.3338948},
	doi = {10.1145/3338906.3338948},
	abstract = {E2E web test suites are prone to test dependencies due to the heterogeneous multi-tiered nature of modern web apps, which makes it difficult for developers to create isolated program states for each test case. In this paper, we present the first approach for detecting and validating test dependencies present in E2E web test suites. Our approach employs string analysis to extract an approximated set of dependencies from the test code. It then filters potential false dependencies through natural language processing of test names. Finally, it validates all dependencies, and uses a novel recovery algorithm to ensure no true dependencies are missed in the final test dependency graph. Our approach is implemented in a tool called TEDD and evaluated on the test suites of six open-source web apps. Our results show that TEDD can correctly detect and validate test dependencies up to 72\% faster than the baseline with the original test ordering in which the graph contains all possible dependencies. The test dependency graphs produced by TEDD enable test execution parallelization, with a speed-up factor of up to 7×.},
	urldate = {2022-11-06},
	booktitle = {Proceedings of the 2019 27th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Biagiola, Matteo and Stocco, Andrea and Mesbah, Ali and Ricca, Filippo and Tonella, Paolo},
	month = aug,
	year = {2019},
	keywords = {NLP, test dependency, web testing},
	pages = {154--164},
	file = {Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\BF7EJ3Z7\\Biagiola et al. - 2019 - Web test dependency detection.pdf:application/pdf},
}

@article{lam_large-scale_2020,
	title = {A large-scale longitudinal study of flaky tests},
	volume = {4},
	url = {http://doi.org/10.1145/3428270},
	doi = {10.1145/3428270},
	abstract = {Flaky tests are tests that can non-deterministically pass or fail for the same code version. These tests undermine regression testing efficiency, because developers cannot easily identify whether a test fails due to their recent changes or due to flakiness. Ideally, one would detect flaky tests right when flakiness is introduced, so that developers can then immediately remove the flakiness. Some software organizations, e.g., Mozilla and Netflix, run some tools—detectors—to detect flaky tests as soon as possible. However, detecting flaky tests is costly due to their inherent non-determinism, so even state-of-the-art detectors are often impractical to be used on all tests for each project change. To combat the high cost of applying detectors, these organizations typically run a detector solely on newly added or directly modified tests, i.e., not on unmodified tests or when other changes occur (including changes to the test suite, the code under test, and library dependencies). However, it is unclear how many flaky tests can be detected or missed by applying detectors in only these limited circumstances. To better understand this problem, we conduct a large-scale longitudinal study of flaky tests to determine when flaky tests become flaky and what changes cause them to become flaky. We apply two state-of-theart detectors to 55 Java projects, identifying a total of 245 flaky tests that can be compiled and run in the code version where each test was added. We find that 75\% of flaky tests (184 out of 245) are flaky when added, indicating substantial potential value for developers to run detectors specifically on newly added tests. However, running detectors solely on newly added tests would still miss detecting 25\% of flaky tests. The percentage of flaky tests that can be detected does increase to 85\% when detectors are run on newly added or directly modified tests. The remaining 15\% of flaky tests become flaky due to other changes and can be detected only when detectors are always applied to all tests. Our study is the first to empirically evaluate when tests become flaky and to recommend guidelines for applying detectors in the future.},
	number = {OOPSLA},
	urldate = {2022-11-06},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Lam, Wing and Winter, Stefan and Wei, Anjiang and Xie, Tao and Marinov, Darko and Bell, Jonathan},
	month = nov,
	year = {2020},
	keywords = {flaky test, regression testing},
	pages = {202:1--202:29},
	file = {Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\47Q5F6RE\\Lam et al. - 2020 - A large-scale longitudinal study of flaky tests.pdf:application/pdf},
}

@inproceedings{silva_shake_2020,
	title = {Shake {It}! {Detecting} {Flaky} {Tests} {Caused} by {Concurrency} with {Shaker}},
	doi = {10.1109/ICSME46990.2020.00037},
	abstract = {A test is said to be flaky when it non-deterministically passes or fails. Test flakiness negatively affects the effectiveness of regression testing and, consequently, impacts software evolution. Detecting test flakiness is an important and challenging problem. ReRun is the most popular approach in industry to detect test flakiness. It re-executes a test suite on a fixed code version multiple times, looking for inconsistent outputs across executions. Unfortunately, ReRun is costly and unreliable. This paper proposes SHAKER, a lightweight technique to improve the ability of ReRun to detect flaky tests. SHAKER adds noise in the execution environment (e.g., it adds stressor tasks to compete for the CPU or memory). It builds on the observations that concurrency is an important source of flakiness and that adding noise in the environment can interfere in the ordering of events and, consequently, influence on the test outputs. We conducted experiments on a data set with 11 Android apps. Results are very encouraging. SHAKER discovered many more flaky tests than ReRun (95\% and 37.5\% of the total, respectively) and discovered these flaky tests much faster. In addition, SHAKER was able to reveal 61 new flaky tests that went undetected in 50 re-executions with ReRun.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Silva, Denini and Teixeira, Leopoldo and d’Amorim, Marcelo},
	month = sep,
	year = {2020},
	note = {ISSN: 2576-3148},
	keywords = {Software maintenance, Testing, regression testing, Concurrent computing, Conferences, Industries, noise, Task analysis, test flakiness},
	pages = {301--311},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\hedde\\Zotero\\storage\\ZUUJIL5Z\\9240694.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\YHLRFPUW\\Silva et al. - 2020 - Shake It! Detecting Flaky Tests Caused by Concurre.pdf:application/pdf},
}

@inproceedings{lam_study_2020,
	address = {New York, NY, USA},
	series = {{ICSE} '20},
	title = {A study on the lifecycle of flaky tests},
	isbn = {978-1-4503-7121-6},
	url = {http://doi.org/10.1145/3377811.3381749},
	doi = {10.1145/3377811.3381749},
	abstract = {During regression testing, developers rely on the pass or fail outcomes of tests to check whether changes broke existing functionality. Thus, flaky tests, which nondeterministically pass or fail on the same code, are problematic because they provide misleading signals during regression testing. Although flaky tests are the focus of several existing studies, none of them study (1) the reoccurrence, runtimes, and time-before-fix of flaky tests, and (2) flaky tests in-depth on proprietary projects. This paper fills this knowledge gap about flaky tests and investigates whether prior categorization work on flaky tests also apply to proprietary projects. Specifically, we study the lifecycle of flaky tests in six large-scale proprietary projects at Microsoft. We find, as in prior work, that asynchronous calls are the leading cause of flaky tests in these Microsoft projects. Therefore, we propose the first automated solution, called Flakiness and Time Balancer (FaTB), to reduce the frequency of flaky-test failures caused by asynchronous calls. Our evaluation of five such flaky tests shows that FaTB can reduce the running times of these tests by up to 78\% without empirically affecting the frequency of their flaky-test failures. Lastly, our study finds several cases where developers claim they "fixed" a flaky test but our empirical experiments show that their changes do not fix or reduce these tests' frequency of flaky-test failures. Future studies should be more cautious when basing their results on changes that developers claim to be "fixes".},
	urldate = {2022-11-06},
	booktitle = {Proceedings of the {ACM}/{IEEE} 42nd {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Lam, Wing and Muşlu, Kıvanç and Sajnani, Hitesh and Thummalapenta, Suresh},
	month = oct,
	year = {2020},
	keywords = {flaky test, empirical study, lifecycle},
	pages = {1471--1482},
	file = {Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\3EY52P62\\Lam et al. - 2020 - A study on the lifecycle of flaky tests.pdf:application/pdf},
}

@article{verdecchia_know_2021,
	title = {Know {You} {Neighbor}: {Fast} {Static} {Prediction} of {Test} {Flakiness}},
	volume = {9},
	issn = {2169-3536},
	shorttitle = {Know {You} {Neighbor}},
	doi = {10.1109/ACCESS.2021.3082424},
	abstract = {Context: Flaky tests plague regression testing in Continuous Integration environments by slowing down change releases and wasting testing time and effort. Despite the growing interest in mitigating the burden of test flakiness, how to efficiently and effectively detect flaky tests is still an open problem. Objective: In this study, we present and evaluate FLAST, an approach designed to statically predict test flakiness. FLAST leverages vector-space modeling, similarity search, dimensionality reduction, and k-Nearest Neighbor classification in order to timely and efficiently detect test flakiness. Method: In order to gain insights into the efficiency and effectiveness of FLAST, we conduct an empirical evaluation of the approach by considering 13 real-world projects, for a total of 1,383 flaky and 26,702 non-flaky tests. We carry out a quantitative comparison of FLAST with the state-of-the-art methods to detect test flakiness, by considering a balanced dataset comprising 1,402 real-world flaky and as many non-flaky tests. Results: From the results we observe that the effectiveness of FLAST is comparable with the state-of-the-art, while providing considerable gains in terms of efficiency. In addition, the results demonstrate how by tuning the threshold of the approach FLAST can be made more conservative, so to reduce false positives, at the cost of missing more potentially flaky tests. Conclusion: The collected results demonstrate that FLAST provides a fast, low-cost and reliable approach that can be used to guide test rerunning, or to gate the inclusion of new potentially flaky tests.},
	journal = {IEEE Access},
	author = {Verdecchia, Roberto and Cruciani, Emilio and Miranda, Breno and Bertolino, Antonia},
	year = {2021},
	note = {Conference Name: IEEE Access},
	keywords = {flaky tests, Software testing, Testing, Tools, Instruments, prediction, Probabilistic logic, similarity, Social networking (online), static analysis, Static analysis, Vocabulary},
	pages = {76119--76134},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\hedde\\Zotero\\storage\\EZANYDT6\\9437181.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\R66TCUIY\\Verdecchia et al. - 2021 - Know You Neighbor Fast Static Prediction of Test .pdf:application/pdf},
}

@inproceedings{alshammari_flakeflagger_2021,
	title = {{FlakeFlagger}: {Predicting} {Flakiness} {Without} {Rerunning} {Tests}},
	shorttitle = {{FlakeFlagger}},
	doi = {10.1109/ICSE43902.2021.00140},
	abstract = {When developers make changes to their code, they typically run regression tests to detect if their recent changes (re) introduce any bugs. However, many tests are flaky, and their outcomes can change non-deterministically, failing without apparent cause. Flaky tests are a significant nuisance in the development process, since they make it more difficult for developers to trust the outcome of their tests, and hence, it is important to know which tests are flaky. The traditional approach to identify flaky tests is to rerun them multiple times: if a test is observed both passing and failing on the same code, it is definitely flaky. We conducted a very large empirical study looking for flaky tests by rerunning the test suites of 24 projects 10,000 times each, and found that even with this many reruns, some previously identified flaky tests were still not detected. We propose FlakeFlagger, a novel approach that collects a set of features describing the behavior of each test, and then predicts tests that are likely to be flaky based on similar behavioral features. We found that FlakeFlagger correctly labeled as flaky at least as many tests as a state-of-the-art flaky test classifier, but that FlakeFlagger reported far fewer false positives. This lower false positive rate translates directly to saved time for researchers and developers who use the classification result to guide more expensive flaky test detection processes. Evaluated on our dataset of 23 projects with flaky tests, FlakeFlagger outperformed the prior approach (by F1 score) on 16 projects and tied on 4 projects. Our results indicate that this approach can be effective for identifying likely flaky tests prior to running time-consuming flaky test detectors.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Alshammari, Abdulrahman and Morris, Christopher and Hilton, Michael and Bell, Jonathan},
	month = may,
	year = {2021},
	note = {ISSN: 1558-1225},
	keywords = {flaky tests, Software engineering, Testing, Computer bugs, regression testing, Detectors, reliability},
	pages = {1572--1584},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\hedde\\Zotero\\storage\\6XUXBXKA\\9402098.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\CJ6EIRWR\\Alshammari et al. - 2021 - FlakeFlagger Predicting Flakiness Without Rerunni.pdf:application/pdf},
}

@article{zolfaghari_root_2021,
	title = {Root causing, detecting, and fixing flaky tests: {State} of the art and future roadmap},
	volume = {51},
	issn = {1097-024X},
	shorttitle = {Root causing, detecting, and fixing flaky tests},
	url = {http://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2929},
	doi = {10.1002/spe.2929},
	abstract = {A flaky test is a test that may lead to different results in different runs on a single code under test without any change in the test code. Test flakiness is a noxious phenomenon that slows down software deployment, and increases the expenditures in a broad spectrum of platforms such as software-defined networks and Internet of Things environments. Industrial institutes and labs have conducted a whole lot of research projects aiming at tackling this problem. Although this issue has been receiving more attention from academia in recent years, the academic research community is still behind the industry in this area. A systematic review and trend analysis on the existing approaches for detecting and root causing flaky tests can pave the way for future research on this topic. This can help academia keep pace with industrial advancements and even lead the research in this field. This article first presents a comprehensive review of recent achievements of the industry as well as academia regarding the detection and mitigation of flaky tests. In the next step, recent trends in this line of research are analyzed and a roadmap is established for future research.},
	language = {en},
	number = {5},
	urldate = {2022-11-06},
	journal = {Software: Practice and Experience},
	author = {Zolfaghari, Behrouz and Parizi, Reza M. and Srivastava, Gautam and Hailemariam, Yoseph},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/spe.2929},
	keywords = {detection, flaky testing, software, tools},
	pages = {851--867},
	file = {Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\RR5WHG9R\\Zolfaghari et al. - 2021 - Root causing, detecting, and fixing flaky tests S.pdf:application/pdf;Snapshot:C\:\\Users\\hedde\\Zotero\\storage\\ZNCUMQRG\\spe.html:text/html},
}

@inproceedings{choudhary_water_2011,
	address = {New York, NY, USA},
	series = {{ETSE} '11},
	title = {{WATER}: {Web} {Application} {TEst} {Repair}},
	isbn = {978-1-4503-0808-3},
	shorttitle = {{WATER}},
	url = {http://doi.org/10.1145/2002931.2002935},
	doi = {10.1145/2002931.2002935},
	abstract = {Web applications tend to evolve quickly, resulting in errors and failures in test automation scripts that exercise them. Repairing such scripts to work on the updated application is essential for maintaining the quality of the test suite. Updating such scripts manually is a time consuming task, which is often difficult and is prone to errors if not performed carefully. In this paper, we propose a technique to automatically suggest repairs for such web application test scripts. Our technique is based on differential testing and compares the behavior of the test case on two successive versions of the web application: first version in which the test script runs successfully and the second version in which the script results in an error or failure. By analyzing the difference between these two executions, our technique suggests repairs that can be applied to repair the scripts. To evaluate our technique, we implemented it in a tool called WATER and exercised it on real web applications with test cases. Our experiments show that WATER can suggest meaningful repairs for practical test cases, many of which correspond to those made later by developers themselves.},
	urldate = {2023-02-15},
	booktitle = {Proceedings of the {First} {International} {Workshop} on {End}-to-{End} {Test} {Script} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Choudhary, Shauvik Roy and Zhao, Dan and Versee, Husayn and Orso, Alessandro},
	month = jul,
	year = {2011},
	keywords = {web testing, test repair},
	pages = {24--29},
	file = {Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\SZSEDXC9\\Choudhary et al. - 2011 - WATER Web Application TEst Repair.pdf:application/pdf},
}

@inproceedings{stocco_visual_2018,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2018},
	title = {Visual web test repair},
	isbn = {978-1-4503-5573-5},
	url = {http://doi.org/10.1145/3236024.3236063},
	doi = {10.1145/3236024.3236063},
	abstract = {Web tests are prone to break frequently as the application under test evolves, causing much maintenance effort in practice. To detect the root causes of a test breakage, developers typically inspect the test's interactions with the application through the GUI. Existing automated test repair techniques focus instead on the code and entirely ignore visual aspects of the application. We propose a test repair technique that is informed by a visual analysis of the application. Our approach captures relevant visual information from tests execution and analyzes them through a fast image processing pipeline to visually validate test cases as they re-executed for regression purposes. Then, it reports the occurrences of breakages and potential fixes to the testers. Our approach is also equipped with a local crawling mechanism to handle non-trivial breakage scenarios such as the ones that require to repair the test's workflow. We implemented our approach in a tool called Vista. Our empirical evaluation on 2,672 test cases spanning 86 releases of four web applications shows that Vista is able to repair, on average, 81\% of the breakages, a 41\% increment with respect to existing techniques.},
	urldate = {2023-02-15},
	booktitle = {Proceedings of the 2018 26th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Stocco, Andrea and Yandrapally, Rahulkrishna and Mesbah, Ali},
	month = oct,
	year = {2018},
	keywords = {web testing, test repair, computer vision, image analysis},
	pages = {503--514},
	file = {Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\WD9ZW6Q6\\Stocco et al. - 2018 - Visual web test repair.pdf:application/pdf},
}

@inproceedings{hammoudi_why_2016,
	title = {Why do {Record}/{Replay} {Tests} of {Web} {Applications} {Break}?},
	doi = {10.1109/ICST.2016.16},
	abstract = {Software engineers often use record/replay tools to enable the automated testing of web applications. Tests created in this manner can then be used to regression test new versions of the web applications as they evolve. Web application tests recorded by record/replay tools, however, can be quite brittle, they can easily break as applications change. For this reason, researchers have begun to seek approaches for automatically repairing record/replay tests. To date, however, there have been no comprehensive attempts to characterize the causes of breakagesin record/replay tests for web applications. In this work, wepresent a taxonomy classifying the ways in which record/replay tests for web applications break, based on an analysis of 453 versions of popular web applications for which 1065 individual test breakages were recognized. The resulting taxonomy can help direct researchers in their attempts to repair such tests. It can also help practitioners by suggesting best practices when creating tests or modifying programs, and can help researchers with other tasks such as test robustness analysis and IDE design.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Software} {Testing}, {Verification} and {Validation} ({ICST})},
	author = {Hammoudi, Mouna and Rothermel, Gregg and Tonella, Paolo},
	month = apr,
	year = {2016},
	keywords = {Maintenance engineering, Testing, Selenium, test repair, Frequency measurement, HTML, Record/replay testing, Taxonomy, test breakages, web applications, Web pages},
	pages = {180--190},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\hedde\\Zotero\\storage\\C7N6I9HI\\7515470.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\YYJWU7H5\\Hammoudi et al. - 2016 - Why do RecordReplay Tests of Web Applications Bre.pdf:application/pdf},
}

@misc{noauthor_n8n_2023,
	title = {n8n - {Workflow} automation tool},
	url = {https://github.com/n8n-io/n8n},
	abstract = {Free and source-available fair-code licensed workflow automation tool. Easily automate tasks across different services.},
	urldate = {2023-03-19},
	publisher = {n8n - Workflow Automation},
	month = mar,
	year = {2023},
	note = {original-date: 2019-06-22T09:24:21Z},
	keywords = {data-flow, apis, automated, automation, cli, development, docker, iaas, integration-framework, integrations, ipaas, low-code, low-code-development-platform, low-code-platform, n8n, node, self-hosted, typescript, workflow, workflow-automation},
}

@misc{noauthor_jacoco_nodate,
	title = {{JaCoCo} {Java} {Code} {Coverage} {Library}},
	url = {https://www.jacoco.org/},
	urldate = {2023-03-19},
	journal = {JaCoCo Java Code Coverage Library},
	file = {EclEmma - Java Code Coverage for Eclipse:C\:\\Users\\hedde\\Zotero\\storage\\LC555TF8\\www.jacoco.org.html:text/html},
}

@misc{noauthor_cypress-iocypress_2023,
	title = {cypress-io/cypress},
	copyright = {MIT},
	url = {https://github.com/cypress-io/cypress},
	abstract = {Fast, easy and reliable testing for anything that runs in a browser.},
	urldate = {2023-03-19},
	publisher = {Cypress.io},
	month = mar,
	year = {2023},
	note = {original-date: 2015-03-04T00:46:28Z},
	keywords = {cypress, e2e-testing, e2e-tests, end-to-end-testing, javascript-tests, test, test-automation, test-runner, test-suite, testing, testing-tools, tests},
}

@inproceedings{harman_start-ups_2018,
	title = {From {Start}-ups to {Scale}-ups: {Opportunities} and {Open} {Problems} for {Static} and {Dynamic} {Program} {Analysis}},
	shorttitle = {From {Start}-ups to {Scale}-ups},
	doi = {10.1109/SCAM.2018.00009},
	abstract = {This paper describes some of the challenges and opportunities when deploying static and dynamic analysis at scale, drawing on the authors' experience with the Infer and Sapienz Technologies at Facebook, each of which started life as a research-led start-up that was subsequently deployed at scale, impacting billions of people worldwide. The paper identifies open problems that have yet to receive significant attention from the scientific community, yet which have potential for profound real world impact, formulating these as research questions that, we believe, are ripe for exploration and that would make excellent topics for research projects. Note: This paper accompanies the authors' joint keynote at the 18th IEEE International Working Conference on Source Code Analysis and Manipulation, September 23rd-24th, 2018 - Madrid, Spain.},
	booktitle = {2018 {IEEE} 18th {International} {Working} {Conference} on {Source} {Code} {Analysis} and {Manipulation} ({SCAM})},
	author = {Harman, Mark and O'Hearn, Peter},
	month = sep,
	year = {2018},
	note = {ISSN: 2470-6892},
	keywords = {flaky test, Software, Testing, Tools, Facebook, Industries, Static analysis, testing, compositional reasoning, Infer, Prototypes, Sapienz, SBSE, Separation Logic, verification},
	pages = {1--23},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\hedde\\Zotero\\storage\\B8BU9JRP\\8530713.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\7R84XUVS\\Harman and O'Hearn - 2018 - From Start-ups to Scale-ups Opportunities and Ope.pdf:application/pdf},
}

@misc{haben_importance_2023,
	title = {The {Importance} of {Discerning} {Flaky} from {Fault}-triggering {Test} {Failures}: {A} {Case} {Study} on the {Chromium} {CI}},
	shorttitle = {The {Importance} of {Discerning} {Flaky} from {Fault}-triggering {Test} {Failures}},
	url = {http://arxiv.org/abs/2302.10594},
	doi = {10.48550/arXiv.2302.10594},
	abstract = {Flaky tests are tests that pass and fail on different executions of the same version of a program under test. They waste valuable developer time by making developers investigate false alerts (flaky test failures). To deal with this problem, many prediction methods that identify flaky tests have been proposed. While promising, the actual utility of these methods remains unclear since they have not been evaluated within a continuous integration (CI) process. In particular, it remains unclear what is the impact of missed faults, i.e., the consideration of fault-triggering test failures as flaky, at different CI cycles. To fill this gap, we apply state-of-the-art flakiness prediction methods at the Chromium CI and check their performance. Perhaps surprisingly, we find that, despite the high precision (99.2\%) of the methods, their application leads to numerous faults missed, approximately 76.2\% of all regression faults. To explain this result, we analyse the fault-triggering failures and show that flaky tests have a strong fault-revealing capability, i.e., they reveal more than 1/3 of all regression faults, indicating an inherent limitation of all methods focusing on identifying flaky tests, instead of flaky test failures. Going a step further, we build failure-focused prediction methods and optimize them by considering new features. Interestingly, we find that these methods perform better than the test-focused ones, with an MCC increasing from 0.20 to 0.42. Overall, our findings imply that on the one hand future research should focus on predicting flaky test failures instead of flaky tests and the need for adopting more thorough experimental methodologies when evaluating flakiness prediction methods, on the other.},
	urldate = {2023-03-25},
	publisher = {arXiv},
	author = {Haben, Guillaume and Habchi, Sarra and Papadakis, Mike and Cordy, Maxime and Traon, Yves Le},
	month = feb,
	year = {2023},
	note = {arXiv:2302.10594 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:C\:\\Users\\hedde\\Zotero\\storage\\UTH6AUS4\\Haben et al. - 2023 - The Importance of Discerning Flaky from Fault-trig.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hedde\\Zotero\\storage\\JCWAJG8Z\\2302.html:text/html},
}

@misc{rasheed_effect_2023,
	title = {On the {Effect} of {Instrumentation} on {Test} {Flakiness}},
	url = {http://arxiv.org/abs/2303.09755},
	abstract = {Test flakiness is a problem that affects testing and processes that rely on it. Several factors cause or influence the flakiness of test outcomes. Test execution order, randomness and concurrency are some of the more common and well-studied causes. Some studies mention code instrumentation as a factor that causes or affects test flakiness. However, evidence for this issue is scarce. In this study, we attempt to systematically collect evidence for the effects of instrumentation on test flakiness. We experiment with common types of instrumentation for Java programs - namely, application performance monitoring, coverage and profiling instrumentation. We then study the effects of instrumentation on a set of nine programs obtained from an existing dataset used to study test flakiness, consisting of popular GitHub projects written in Java. We observe cases where real-world instrumentation causes flakiness in a program. However, this effect is rare. We also discuss a related issue - how instrumentation may interfere with flakiness detection and prevention.},
	urldate = {2023-03-26},
	publisher = {arXiv},
	author = {Rasheed, Shawn and Dietrich, Jens and Tahir, Amjed},
	month = mar,
	year = {2023},
	note = {arXiv:2303.09755 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:C\:\\Users\\hedde\\Zotero\\storage\\J6HXTM5K\\Rasheed et al. - 2023 - On the Effect of Instrumentation on Test Flakiness.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hedde\\Zotero\\storage\\LFCPXYAU\\2303.html:text/html},
}

@inproceedings{krusche_artemis_2018,
	address = {New York, NY, USA},
	series = {{SIGCSE} '18},
	title = {{ArTEMiS}: {An} {Automatic} {Assessment} {Management} {System} for {Interactive} {Learning}},
	isbn = {978-1-4503-5103-4},
	shorttitle = {{ArTEMiS}},
	url = {https://dl.acm.org/doi/10.1145/3159450.3159602},
	doi = {10.1145/3159450.3159602},
	abstract = {The increasing number of students in computer science courses leads to high efforts in manual assessment of exercises. Existing assessment systems are not designed for exercises with immediate feedback in large classes. In this paper, we present an AuTomated assEssment Management System for interactive learning. ArTEMiS assesses solutions to programming exercises automatically and provides instant feedback so that students can iteratively solve the exercise. It is open source and highly scalable based on version control, regression testing and continuous integration. ArTEMiS offers an online code editor with interactive exercise instructions, is programming language independent and applicable to a variety of computer science courses. By using it, students gain experiences in version control, dependency management and continuous integration. We used ArTEMiS in 3 university and 1 online courses and report about our experiences. We figured out that ArTEMiS is suitable for beginners, helps students to realize their progress and to gradually improve their solutions. It reduces the effort of instructors and enhances the learning experience of students.},
	urldate = {2023-03-26},
	booktitle = {Proceedings of the 49th {ACM} {Technical} {Symposium} on {Computer} {Science} {Education}},
	publisher = {Association for Computing Machinery},
	author = {Krusche, Stephan and Seitz, Andreas},
	month = feb,
	year = {2018},
	keywords = {continuous integration, automated assessment, in-class exercises, instant feedback, interactive exercise instructions, online courses, online editor, programming exercises, version control},
	pages = {284--289},
	file = {Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\2PWCGEBE\\Krusche and Seitz - 2018 - ArTEMiS An Automatic Assessment Management System.pdf:application/pdf},
}

@misc{jhipster_jhipster_nodate,
	title = {{JHipster} - {Full} {Stack} {Platform} for the {Modern} {Developer}!},
	url = {https://www.jhipster.tech/},
	abstract = {JHipster is a development platform to quickly generate, develop, \& deploy modern web applications \& microservice architectures.},
	urldate = {2023-04-01},
	journal = {JHipster - Full Stack Platform for the Modern Developer!},
	author = {{JHipster}},
	file = {JHipster - Full Stack Platform for the Modern Developer!:C\:\\Users\\hedde\\Zotero\\storage\\9FZSLTIE\\www.jhipster.tech.html:text/html},
}

@misc{cypressio_writing_nodate,
	title = {Writing a {Plugin} {\textbar} {Cypress} {Documentation}},
	url = {https://docs.cypress.io/api/plugins/writing-a-plugin},
	abstract = {The Plugins API allows you to hook into and extend Cypress behavior.},
	language = {en},
	urldate = {2023-04-01},
	journal = {Writing a Plugin {\textbar} Cypress Documentation},
	author = {{Cypress.io}},
	file = {Snapshot:C\:\\Users\\hedde\\Zotero\\storage\\T99MUEVC\\writing-a-plugin.html:text/html},
}

@misc{noauthor_nyc_2023,
	title = {nyc},
	copyright = {ISC},
	url = {https://github.com/istanbuljs/nyc},
	abstract = {the Istanbul command line interface},
	urldate = {2023-04-01},
	publisher = {Istanbul Code Coverage},
	month = apr,
	year = {2023},
	note = {original-date: 2015-05-09T02:55:45Z},
	keywords = {code-coverage, istanbul, javascript},
}

@misc{noauthor_bcoec8_nodate,
	title = {bcoe/c8: output coverage reports using {Node}.js' built in coverage},
	url = {https://github.com/bcoe/c8},
	urldate = {2023-04-01},
	file = {bcoe/c8\: output coverage reports using Node.js' built in coverage:C\:\\Users\\hedde\\Zotero\\storage\\EFZAKDH8\\c8.html:text/html},
}

@misc{noauthor_code_nodate,
	title = {Code {Coverage} {\textbar} {Cypress} {Documentation}},
	url = {https://docs.cypress.io/guides/tooling/code-coverage},
	abstract = {What you'll learn},
	language = {en},
	urldate = {2023-04-01},
	journal = {Code Coverage {\textbar} Cypress Documentation},
	file = {Snapshot:C\:\\Users\\hedde\\Zotero\\storage\\YYQG5PDT\\code-coverage.html:text/html},
}

@misc{noauthor_chrome_nodate,
	title = {Chrome {DevTools} {Protocol}},
	url = {https://chromedevtools.github.io/devtools-protocol/tot/Profiler/},
	abstract = {Chrome DevTools Protocol - version tot - Profiler domain},
	language = {en},
	urldate = {2023-04-01},
	journal = {Chrome DevTools Protocol},
	file = {Snapshot:C\:\\Users\\hedde\\Zotero\\storage\\HREWAHXE\\Profiler.html:text/html},
}

@misc{cardaci_chrome-remote-interface_2023,
	title = {chrome-remote-interface},
	copyright = {MIT},
	url = {https://github.com/cyrus-and/chrome-remote-interface},
	abstract = {Chrome Debugging Protocol interface for Node.js},
	urldate = {2023-04-01},
	author = {Cardaci, Andrea},
	month = mar,
	year = {2023},
	note = {original-date: 2013-04-17T18:00:32Z},
	keywords = {javascript, browser, chrome-debugging-protocol, firefox, google-chrome, headless, microsoft-edge, mobile-safari, nodejs, opera},
}

@misc{kayce_basques_coverage_2020,
	title = {Coverage: {Find} unused {JavaScript} and {CSS}},
	shorttitle = {Coverage},
	url = {https://developer.chrome.com/docs/devtools/coverage/},
	abstract = {How to find and analyze unused JavaScript and CSS code in Chrome DevTools.},
	language = {en},
	urldate = {2023-04-01},
	journal = {Chrome Developers},
	author = {{Kayce Basques}},
	month = jul,
	year = {2020},
	file = {Snapshot:C\:\\Users\\hedde\\Zotero\\storage\\FPF9VHSK\\coverage.html:text/html},
}

@misc{noauthor_v8--istanbul_2023,
	title = {v8-to-istanbul},
	copyright = {ISC},
	url = {https://github.com/istanbuljs/v8-to-istanbul},
	abstract = {convert from v8 coverage format to istanbul's format},
	urldate = {2023-04-01},
	publisher = {Istanbul Code Coverage},
	month = mar,
	year = {2023},
	note = {original-date: 2017-11-25T03:36:32Z},
}

@misc{noauthor_jacoco-parse_2023,
	title = {jacoco-parse},
	copyright = {MIT},
	url = {https://github.com/vokal/jacoco-parse},
	abstract = {Javascript parser for Jacoco Coverage reports},
	urldate = {2023-04-01},
	publisher = {Vokal},
	month = jan,
	year = {2023},
	note = {original-date: 2015-07-04T15:37:27Z},
}

@misc{todyshev_parse-diff_2023,
	title = {parse-diff},
	copyright = {MIT},
	url = {https://github.com/sergeyt/parse-diff},
	abstract = {Unified diff parser for nodejs and browser},
	urldate = {2023-04-01},
	author = {Todyshev, Sergey},
	month = mar,
	year = {2023},
	note = {original-date: 2014-01-06T16:18:15Z},
}

@misc{noauthor_gladys_2023,
	title = {Gladys {Assistant}},
	copyright = {Apache-2.0},
	url = {https://github.com/GladysAssistant/Gladys},
	abstract = {A privacy-first, open-source home assistant},
	urldate = {2023-04-02},
	publisher = {Gladys Assistant},
	month = apr,
	year = {2023},
	note = {original-date: 2015-06-05T11:10:33Z},
	keywords = {automation, nodejs, assistant, gladys, home, home-automation, iot, raspberry-pi, smarthome},
}

@misc{noauthor_tooljettooljet_2023,
	title = {{ToolJet}/{ToolJet}},
	copyright = {AGPL-3.0},
	url = {https://github.com/ToolJet/ToolJet},
	abstract = {Extensible low-code framework for building business applications. Connect to databases, cloud storages, GraphQL, API endpoints, Airtable, etc and build apps using drag and drop application builder. Built using JavaScript/TypeScript. 🚀},
	urldate = {2023-04-02},
	publisher = {ToolJet},
	month = apr,
	year = {2023},
	note = {original-date: 2021-03-30T08:51:34Z},
	keywords = {docker, low-code, low-code-development-platform, self-hosted, typescript, hacktoberfest, kubernetes, javascript, nodejs, internal-applications, internal-project, internal-tool, internal-tools, low-code-framework, nestjs, no-code, reactjs, typeorm, web-development-tools},
}

@misc{atlassian_bamboo_nodate,
	title = {Bamboo {Continuous} {Integration} and {Deployment} {Build} {Server}},
	url = {https://www.atlassian.com/software/bamboo},
	abstract = {Bamboo is a continuous integration and deployment tool that ties automated builds, tests and releases together in a single workflow.},
	language = {en},
	urldate = {2023-04-02},
	journal = {Atlassian},
	author = {{Atlassian}},
	file = {Snapshot:C\:\\Users\\hedde\\Zotero\\storage\\F9GPBNE9\\bamboo.html:text/html},
}

@book{wilcox_introduction_2011,
	title = {Introduction to {Robust} {Estimation} and {Hypothesis} {Testing}},
	isbn = {978-0-12-387015-5},
	abstract = {This revised book provides a thorough explanation of the foundation of robust methods, incorporating the latest updates on R and S-Plus, robust ANOVA (Analysis of Variance) and regression. It guides advanced students and other professionals through the basic strategies used for developing practical solutions to problems, and provides a brief background on the foundations of modern methods, placing the new methods in historical context. Author Rand Wilcox includes chapter exercises and many real-world examples that illustrate how various methods perform in different situations. Introduction to Robust Estimation and Hypothesis Testing, Second Edition, focuses on the practical applications of modern, robust methods which can greatly enhance our chances of detecting true differences among groups and true associations among variables.  Covers latest developments in robust regression Covers latest improvements in ANOVA Includes newest rank-based methods Describes and illustrated easy to use software},
	language = {en},
	publisher = {Academic Press},
	author = {Wilcox, Rand R.},
	month = dec,
	year = {2011},
	note = {Google-Books-ID: 8f8nBb4\_\_EYC},
	keywords = {Mathematics / General, Mathematics / Probability \& Statistics / General},
}

@inproceedings{daniel_reassert_2009,
	title = {{ReAssert}: {Suggesting} {Repairs} for {Broken} {Unit} {Tests}},
	shorttitle = {{ReAssert}},
	doi = {10.1109/ASE.2009.17},
	abstract = {Developers often change software in ways that cause tests to fail. When this occurs, developers must determine whether failures are caused by errors in the code under test or in the test code itself. In the latter case, developers must repair failing tests or remove them from the test suite. Repairing tests is time consuming but beneficial, since removing tests reduces a test suite's ability to detect regressions. Fortunately, simple program transformations can repair many failing tests automatically. We present ReAssert, a novel technique and tool that suggests repairs to failing tests' code which cause the tests to pass. Examples include replacing literal values in tests, changing assertion methods, or replacing one assertion with several. If the developer chooses to apply the repairs, ReAssert modifies the code automatically. Our experiments show that ReAssert can repair many common test failures and that its suggested repairs correspond to developers' expectations.},
	booktitle = {2009 {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	author = {Daniel, Brett and Jagannath, Vilas and Dig, Danny and Marinov, Darko},
	month = nov,
	year = {2009},
	note = {ISSN: 1938-4300},
	keywords = {Application software, Automatic testing, Government, Logic testing, Performance evaluation, Programming, Protection, Software engineering, Software maintenance, Software test maintenance, Software testing, Software tools, USA Councils},
	pages = {433--444},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\hedde\\Zotero\\storage\\CRNYVE7L\\5431753.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\Q5XCKKA2\\Daniel et al. - 2009 - ReAssert Suggesting Repairs for Broken Unit Tests.pdf:application/pdf},
}

@book{ammann_introduction_2016,
	title = {Introduction to {Software} {Testing}},
	isbn = {978-1-316-77312-3},
	abstract = {This extensively classroom-tested text takes an innovative approach to explaining software testing that defines it as the process of applying a few precise, general-purpose criteria to a structure or model of the software. The book incorporates cutting-edge developments, including techniques to test modern types of software such as OO, web applications, and embedded software. This revised second edition significantly expands coverage of the basics, thoroughly discussing test automaton frameworks, and it adds new, improved examples and numerous exercises. The theory of coverage criteria is carefully and cleanly explained to help students understand concepts before delving into practical applications, while extensive use of the JUnit test framework gives students practical experience in a test framework popular in the industry. Exercises, meanwhile, feature specifically tailored tools that allow students to check their own work. The book's website also offers an instructor's manual, PowerPoint slides, testing tools for students, and example software programs in Java.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Ammann, Paul and Offutt, Jeff},
	month = dec,
	year = {2016},
	note = {Google-Books-ID: 58LeDQAAQBAJ},
	keywords = {Computers / Software Development \& Engineering / General},
}

@misc{martin_fowler_bliki_2012,
	type = {Blog},
	title = {bliki: {TestCoverage}},
	shorttitle = {bliki},
	url = {https://martinfowler.com/bliki/TestCoverage.html},
	abstract = {Test coverage is useful for finding untested parts of a codebase, but it's of little use as a numeric statement of test quality.},
	language = {en},
	urldate = {2023-04-29},
	journal = {martinfowler.com},
	author = {{Martin Fowler}},
	month = apr,
	year = {2012},
	file = {Snapshot:C\:\\Users\\hedde\\Zotero\\storage\\GYPBMFR2\\TestCoverage.html:text/html},
}

@article{marick_brian_and_smith_john_and_jones_mark_how_1999,
	title = {How to {Misuse} {Code} {Coverage}},
	language = {en},
	journal = {Proceedings of the 16th Interational Conference on Testing Computer Software},
	author = {{Marick, Brian and Smith, John and Jones, Mark}},
	year = {1999},
	pages = {16--18},
	file = {Marick - How to Misuse Code Coverage.pdf:C\:\\Users\\hedde\\Zotero\\storage\\M8A3CJBR\\Marick - How to Misuse Code Coverage.pdf:application/pdf},
}

@inproceedings{yang_survey_2006,
	address = {New York, NY, USA},
	series = {{AST} '06},
	title = {A survey of coverage based testing tools},
	isbn = {978-1-59593-408-6},
	url = {https://dl.acm.org/doi/10.1145/1138929.1138949},
	doi = {10.1145/1138929.1138949},
	abstract = {Test coverage is sometimes used as a way to measure how thoroughly software is tested. Coverage is used by software developers and sometimes by vendors to indicate their confidence in the readiness of their software. This survey studies and compares 17 coverage-based testing tools focusing on, but not restricted to coverage measurement. We also survey additional features, including program prioritization for testing, assistance in debugging, automatic generation of test cases, and customization of test reports. Such features make tools more useful and practical, especially for large-scale, real-life commercial software applications. Our initial motivations were both to understand the available test coverage tools and to compare them to a tool that we have developed, called eXVantage1 (a tool suite that includes code coverage testing, debugging, performance profiling, and reporting). Our study shows that each tool has its unique features tailored to its application domains. Therefore this study can be used to pick the right coverage testing tools depending on various requirements.},
	urldate = {2023-04-29},
	booktitle = {Proceedings of the 2006 international workshop on {Automation} of software test},
	publisher = {Association for Computing Machinery},
	author = {Yang, Qian and Li, J. Jenny and Weiss, David},
	month = may,
	year = {2006},
	keywords = {automate test case generation, code coverage, coverage-based testing tool, dominator analysis, eXVantage, prioritization},
	pages = {99--103},
	file = {Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\ERJXBX6R\\Yang et al. - 2006 - A survey of coverage based testing tools.pdf:application/pdf},
}

@inproceedings{ivankovic_code_2019,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2019},
	title = {Code coverage at {Google}},
	isbn = {978-1-4503-5572-8},
	url = {https://dl.acm.org/doi/10.1145/3338906.3340459},
	doi = {10.1145/3338906.3340459},
	abstract = {Code coverage is a measure of the degree to which a test suite exercises a software system. Although coverage is well established in software engineering research, deployment in industry is often inhibited by the perceived usefulness and the computational costs of analyzing coverage at scale. At Google, coverage information is computed for one billion lines of code daily, for seven programming languages. A key aspect of making coverage information actionable is to apply it at the level of changesets and code review. This paper describes Google’s code coverage infrastructure and how the computed code coverage information is visualized and used. It also describes the challenges and solutions for adopting code coverage at scale. To study how code coverage is adopted and perceived by developers, this paper analyzes adoption rates, error rates, and average code coverage ratios over a five-year period, and it reports on 512 responses, received from surveying 3000 developers. Finally, this paper provides concrete suggestions for how to implement and use code coverage in an industrial setting.},
	urldate = {2023-04-29},
	booktitle = {Proceedings of the 2019 27th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Ivanković, Marko and Petrović, Goran and Just, René and Fraser, Gordon},
	month = aug,
	year = {2019},
	keywords = {coverage, industrial study, test infrastructure},
	pages = {955--963},
	file = {Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\QVNMQCNL\\Ivanković et al. - 2019 - Code coverage at Google.pdf:application/pdf},
}

@misc{jacob_schmitt_what_2022,
	title = {What is end-to-end testing?},
	url = {https://circleci.com/blog/what-is-end-to-end-testing/},
	abstract = {Learn what end-to-end testing is and how to use it for your applications.},
	language = {en},
	urldate = {2023-04-29},
	journal = {CircleCI},
	author = {{Jacob Schmitt}},
	month = apr,
	year = {2022},
	file = {Snapshot:C\:\\Users\\hedde\\Zotero\\storage\\APUDDG5T\\what-is-end-to-end-testing.html:text/html},
}

@inproceedings{hilton_trade-offs_2017,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2017},
	title = {Trade-offs in continuous integration: assurance, security, and flexibility},
	isbn = {978-1-4503-5105-8},
	shorttitle = {Trade-offs in continuous integration},
	url = {https://dl.acm.org/doi/10.1145/3106237.3106270},
	doi = {10.1145/3106237.3106270},
	abstract = {Continuous integration (CI) systems automate the compilation, building, and testing of software. Despite CI being a widely used activity in software engineering, we do not know what motivates developers to use CI, and what barriers and unmet needs they face. Without such knowledge, developers make easily avoidable errors, tool builders invest in the wrong direction, and researchers miss opportunities for improving the practice of CI. We present a qualitative study of the barriers and needs developers face when using CI. We conduct semi-structured interviews with developers from different industries and development scales. We triangulate our findings by running two surveys. We find that developers face trade-offs between speed and certainty (Assurance), between better access and information security (Security), and between more configuration options and greater ease of use (Flexi- bility). We present implications of these trade-offs for developers, tool builders, and researchers.},
	urldate = {2023-04-29},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Hilton, Michael and Nelson, Nicholas and Tunnell, Timothy and Marinov, Darko and Dig, Danny},
	month = aug,
	year = {2017},
	keywords = {Automated Testing, Continuous Integration},
	pages = {197--207},
	file = {Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\S3AV6ZPG\\Hilton et al. - 2017 - Trade-offs in continuous integration assurance, s.pdf:application/pdf},
}

@article{kochhar_code_2017,
	title = {Code {Coverage} and {Postrelease} {Defects}: {A} {Large}-{Scale} {Study} on {Open} {Source} {Projects}},
	volume = {66},
	issn = {1558-1721},
	shorttitle = {Code {Coverage} and {Postrelease} {Defects}},
	doi = {10.1109/TR.2017.2727062},
	abstract = {Testing is a pivotal activity in ensuring the quality of software. Code coverage is a common metric used as a yardstick to measure the efficacy and adequacy of testing. However, does higher coverage actually lead to a decline in postrelease bugs? Do files that have higher test coverage actually have fewer bug reports? The direct relationship between code coverage and actual bug reports has not yet been analyzed via a comprehensive empirical study on real bugs. Past studies only involve a few software systems or artificially injected bugs (mutants). In this empirical study, we examine these questions in the context of open-source software projects based on their actual reported bugs. We analyze 100 large open-source Java projects and measure the code coverage of the test cases that come along with these projects. We collect real bugs logged in the issue tracking system after the release of the software and analyze the correlations between code coverage and these bugs. We also collect other metrics such as cyclomatic complexity and lines of code, which are used to normalize the number of bugs and coverage to correlate with other metrics as well as use these metrics in regression analysis. Our results show that coverage has an insignificant correlation with the number of bugs that are found after the release of the software at the project level, and no such correlation at the file level.},
	number = {4},
	journal = {IEEE Transactions on Reliability},
	author = {Kochhar, Pavneet Singh and Lo, David and Lawall, Julia and Nagappan, Nachiappan},
	month = dec,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Reliability},
	keywords = {Code coverage, Computer bugs, empirical study, Open source software, open-source, postrelease defects, software testing, Software testing, Sonar measurements},
	pages = {1213--1228},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\PBRB8EKC\\Kochhar et al. - 2017 - Code Coverage and Postrelease Defects A Large-Sca.pdf:application/pdf},
}

@article{rothermel_empirical_1998,
	title = {Empirical studies of a safe regression test selection technique},
	volume = {24},
	issn = {1939-3520},
	doi = {10.1109/32.689399},
	abstract = {Regression testing is an expensive testing procedure utilized to validate modified software. Regression test selection techniques attempt to reduce the cost of regression testing by selecting a subset of a program's existing test suite. Safe regression test selection techniques select subsets that, under certain well-defined conditions, exclude no tests (from the original test suite) that if executed would reveal faults in the modified software. Many regression test selection techniques, including several safe techniques, have been proposed, but few have been subjected to empirical validation. This paper reports empirical studies on a particular safe regression test selection technique, in which the technique is compared to the alternative regression testing strategy of running all tests. The results indicate that safe regression test selection can be cost-effective, but that its costs and benefits vary widely based on a number of factors. In particular, test suite design can significantly affect the effectiveness of test selection, and coverage-based test suites may provide test selection results superior to those provided by test suites that are not coverage-based.},
	number = {6},
	journal = {IEEE Transactions on Software Engineering},
	author = {Rothermel, G. and Harrold, M.J.},
	month = jun,
	year = {1998},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {Algorithm design and analysis, Costs, Fault detection, Performance evaluation, Software safety, Software testing},
	pages = {401--419},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\hedde\\Zotero\\storage\\7MYL8WWC\\689399.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\8J2G3738\\Rothermel and Harrold - 1998 - Empirical studies of a safe regression test select.pdf:application/pdf},
}

@misc{rasheed_test_2022,
	title = {Test {Flakiness}' {Causes}, {Detection}, {Impact} and {Responses}: {A} {Multivocal} {Review}},
	shorttitle = {Test {Flakiness}' {Causes}, {Detection}, {Impact} and {Responses}},
	url = {http://arxiv.org/abs/2212.00908},
	doi = {10.48550/arXiv.2212.00908},
	abstract = {Flaky tests (tests with non-deterministic outcomes) pose a major challenge for software testing. They are known to cause significant issues such as reducing the effectiveness and efficiency of testing and delaying software releases. In recent years, there has been an increased interest in flaky tests, with research focusing on different aspects of flakiness, such as identifying causes, detection methods and mitigation strategies. Test flakiness has also become a key discussion point for practitioners (in blog posts, technical magazines, etc.) as the impact of flaky tests is felt across the industry. This paper presents a multivocal review that investigates how flaky tests, as a topic, have been addressed in both research and practice. We cover a total of 651 articles (560 academic articles and 91 grey literature articles/posts), and structure the body of relevant research and knowledge using four different dimensions: causes, detection, impact and responses. For each of those dimensions we provide a categorisation, and classify existing research, discussions, methods and tools. With this, we provide a comprehensive and current snapshot of existing thinking on test flakiness, covering both academic views and industrial practices, and identify limitations and opportunities for future research.},
	urldate = {2023-04-29},
	publisher = {arXiv},
	author = {Rasheed, Shawn and Tahir, Amjed and Dietrich, Jens and Hashemi, Negar and Zhang, Lu},
	month = dec,
	year = {2022},
	note = {arXiv:2212.00908 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:C\:\\Users\\hedde\\Zotero\\storage\\8L53CE4Q\\Rasheed et al. - 2022 - Test Flakiness' Causes, Detection, Impact and Resp.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\hedde\\Zotero\\storage\\4QKPE994\\2212.html:text/html},
}

@misc{noauthor_teamscale_2023,
	title = {Teamscale {JavaScript} {Profiler}},
	copyright = {Apache-2.0},
	url = {https://github.com/cqse/teamscale-javascript-profiler},
	abstract = {Teamscale JavaScript Profiler},
	urldate = {2023-04-30},
	publisher = {CQSE GmbH},
	month = jan,
	year = {2023},
	note = {original-date: 2021-05-06T13:48:02Z},
}

@article{shapiro_analysis_1965,
	title = {An {Analysis} of {Variance} {Test} for {Normality} ({Complete} {Samples})},
	volume = {52},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/2333709},
	doi = {10.2307/2333709},
	number = {3/4},
	urldate = {2023-04-30},
	journal = {Biometrika},
	author = {Shapiro, S. S. and Wilk, M. B.},
	year = {1965},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	pages = {591--611},
	file = {JSTOR Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\PCH5EJE4\\Shapiro and Wilk - 1965 - An Analysis of Variance Test for Normality (Comple.pdf:application/pdf},
}

@book{wohlin_experimentation_2012,
	title = {Experimentation in {Software} {Engineering}},
	isbn = {978-3-642-29044-2},
	abstract = {Like other sciences and engineering disciplines, software engineering requires a cycle of model building, experimentation, and learning. Experiments are valuable tools for all software engineers who are involved in evaluating and choosing between different methods, techniques, languages and tools. The purpose of Experimentation in Software Engineering is to introduce students, teachers, researchers, and practitioners to empirical studies in software engineering, using controlled experiments. The introduction to experimentation is provided through a process perspective, and the focus is on the steps that we have to go through to perform an experiment. The book is divided into three parts. The first part provides a background of theories and methods used in experimentation. Part II then devotes one chapter to each of the five experiment steps: scoping, planning, execution, analysis, and result presentation. Part III completes the presentation with two examples. Assignments and statistical material are provided in appendixes. Overall the book provides indispensable information regarding empirical studies in particular for experiments, but also for case studies, systematic literature reviews, and surveys. It is a revision of the authors’ book, which was published in 2000. In addition, substantial new material, e.g. concerning systematic literature reviews and case study research, is introduced. The book is self-contained and it is suitable as a course book in undergraduate or graduate studies where the need for empirical studies in software engineering is stressed. Exercises and assignments are included to combine the more theoretical material with practical aspects. Researchers will also benefit from the book, learning more about how to conduct empirical studies, and likewise practitioners may use it as a “cookbook” when evaluating new methods or techniques before implementing them in their organization.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Wohlin, Claes and Runeson, Per and Höst, Martin and Ohlsson, Magnus C. and Regnell, Björn and Wesslén, Anders},
	month = jun,
	year = {2012},
	note = {Google-Books-ID: QPVsM1\_U8nkC},
	keywords = {Computers / Information Technology, Computers / Software Development \& Engineering / General, Social Science / Methodology, Social Science / Research},
}

@phdthesis{schallermayer_reducing_2023,
	address = {Munich},
	type = {Bachelor's {Thesis}},
	title = {Reducing {Effort} for {Flaky} {Test} {Detection} {Through} {Resource} {Limitation}},
	abstract = {In software engineering, flaky tests are tests that exhibit inconsistent behavior, meaning
they produce different outputs when executed multiple times under the same conditions.
This behavior causes problems during regression testing, where each failure is supposed
to indicate a bug introduced by the latest revision to the code base.
A common strategy to detect flaky tests is Rerun, which executes each test repeatedly
under the same conditions. The test is marked as flaky if both passing and failing
results are observed. However, since flaky tests often only rarely fail while passing
most times, Rerun is inefficient as it may take many runs to detect a test as flaky.
We present a more efficient approach that focuses on detecting concurrency-related
flakiness by limiting computing resources through Docker. Besides trying to detect
flaky tests more quickly, we also examine how the limitation affects previously stable
tests. As we aim to detect those tests that are most likely to cause problems in practice,
flagging stable tests as flaky is undesirable as it may divert developers’ attention away
from the more critical cases of flakiness.
We achieve promising results as our approach reduces the number of runs needed to
detect flakiness by almost 80\%. When looking at absolute run time, we can provide
a speedup of about 50\%. However, we also observe that some previously stable tests
fail under resource limitations. We find that clearly differentiating between flaky and
stable tests proves challenging.},
	language = {en},
	school = {Technical University of Munich},
	author = {Schallermayer, Max},
	month = apr,
	year = {2023},
}

@article{parry_survey_2021,
	title = {A {Survey} of {Flaky} {Tests}},
	volume = {31},
	issn = {1049-331X},
	url = {https://dl.acm.org/doi/10.1145/3476105},
	doi = {10.1145/3476105},
	abstract = {Tests that fail inconsistently, without changes to the code under test, are described as flaky. Flaky tests do not give a clear indication of the presence of software bugs and thus limit the reliability of the test suites that contain them. A recent survey of software developers found that 59\% claimed to deal with flaky tests on a monthly, weekly, or daily basis. As well as being detrimental to developers, flaky tests have also been shown to limit the applicability of useful techniques in software testing research. In general, one can think of flaky tests as being a threat to the validity of any methodology that assumes the outcome of a test only depends on the source code it covers. In this article, we systematically survey the body of literature relevant to flaky test research, amounting to 76 papers. We split our analysis into four parts: addressing the causes of flaky tests, their costs and consequences, detection strategies, and approaches for their mitigation and repair. Our findings and their implications have consequences for how the software-testing community deals with test flakiness, pertinent to practitioners and of interest to those wanting to familiarize themselves with the research area.},
	number = {1},
	urldate = {2023-05-01},
	journal = {ACM Transactions on Software Engineering and Methodology},
	author = {Parry, Owain and Kapfhammer, Gregory M. and Hilton, Michael and McMinn, Phil},
	month = oct,
	year = {2021},
	keywords = {Flaky tests, software testing},
	pages = {17:1--17:74},
	file = {Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\BYFCLVCU\\Parry et al. - 2021 - A Survey of Flaky Tests.pdf:application/pdf},
}

@inproceedings{lam_understanding_2020,
	title = {Understanding {Reproducibility} and {Characteristics} of {Flaky} {Tests} {Through} {Test} {Reruns} in {Java} {Projects}},
	doi = {10.1109/ISSRE5003.2020.00045},
	abstract = {Flaky tests are tests that can non-deterministically pass and fail. They pose a major impediment to regression testing, because they provide an inconclusive assessment on whether recent code changes contain faults or not. Prior studies of flaky tests have proposed tools to detect flaky tests and identified various sources of flakiness in tests, e.g., order-dependent (OD) tests that deterministically fail for some order of tests in a test suite but deterministically pass for some other orders. Several of these studies have focused on OD tests. We focus on an important and under-explored source of flakiness in tests: non-order-dependent tests that can nondeterministically pass and fail even for the same order of tests. Instead of using specialized tools that aim to detect flaky tests, we run tests using the tool configured by the developers. Specifically, we perform our empirical evaluation on Java projects that rely on the Maven Surefire plugin to run tests. We re-execute each test suite 4000 times, potentially in different test-class orders, and we label tests as flaky if our runs have both pass and fail outcomes across these reruns. We obtain a dataset of 107 flaky tests and study various characteristics of these tests. We find that many tests previously called “non-order-dependent” actually do depend on the order and can fail with very different failure rates for different orders.},
	booktitle = {2020 {IEEE} 31st {International} {Symposium} on {Software} {Reliability} {Engineering} ({ISSRE})},
	author = {Lam, Wing and Winter, Stefan and Astorga, Angello and Stodden, Victoria and Marinov, Darko},
	month = oct,
	year = {2020},
	note = {ISSN: 2332-6549},
	keywords = {Complexity theory, flaky tests, Java, regression testing, reproducibility, Software reliability, Testing, Tools},
	pages = {403--413},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\hedde\\Zotero\\storage\\JMY27AEZ\\9251071.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\ZKLJCMF9\\Lam et al. - 2020 - Understanding Reproducibility and Characteristics .pdf:application/pdf},
}
