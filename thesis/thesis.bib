
@inproceedings{bell_d_2018,
	address = {Gothenburg Sweden},
	title = {D {\textless}span style="font-variant:small-caps;"{\textgreater}e{\textless}/span{\textgreater} {F} {\textless}span style="font-variant:small-caps;"{\textgreater}laker{\textless}/span{\textgreater}: automatically detecting flaky tests},
	isbn = {978-1-4503-5638-1},
	shorttitle = {D {\textless}span style="font-variant},
	url = {https://dl.acm.org/doi/10.1145/3180155.3180164},
	doi = {10.1145/3180155.3180164},
	abstract = {Developers often run tests to check that their latest changes to a code repository did not break any previously working functionality. Ideally, any new test failures would indicate regressions caused by the latest changes. However, some test failures may not be due to the latest changes but due to non-determinism in the tests, popularly called flaky tests. The typical way to detect flaky tests is to rerun failing tests repeatedly. Unfortunately, rerunning failing tests can be costly and can slow down the development cycle.},
	language = {en},
	urldate = {2022-10-31},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Bell, Jonathan and Legunsen, Owolabi and Hilton, Michael and Eloussi, Lamyaa and Yung, Tifany and Marinov, Darko},
	month = may,
	year = {2018},
	pages = {433--444},
	file = {Bell et al. - 2018 - D e .pdf:C\:\\Users\\hedde\\Zotero\\storage\\S9QKTSFZ\\Bell et al. - 2018 - D e .pdf:application/pdf},
}

@inproceedings{luo_empirical_2014,
	address = {Hong Kong China},
	title = {An empirical analysis of flaky tests},
	isbn = {978-1-4503-3056-5},
	url = {https://dl.acm.org/doi/10.1145/2635868.2635920},
	doi = {10.1145/2635868.2635920},
	abstract = {Regression testing is a crucial part of software development. It checks that software changes do not break existing functionality. An important assumption of regression testing is that test outcomes are deterministic: an unmodiﬁed test is expected to either always pass or always fail for the same code under test. Unfortunately, in practice, some tests—often called ﬂaky tests—have non-deterministic outcomes. Such tests undermine the regression testing as they make it diﬃcult to rely on test results.},
	language = {en},
	urldate = {2022-10-31},
	booktitle = {Proceedings of the 22nd {ACM} {SIGSOFT} {International} {Symposium} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Luo, Qingzhou and Hariri, Farah and Eloussi, Lamyaa and Marinov, Darko},
	month = nov,
	year = {2014},
	pages = {643--653},
	file = {Luo et al. - 2014 - An empirical analysis of flaky tests.pdf:C\:\\Users\\hedde\\Zotero\\storage\\HCWMCLQR\\Luo et al. - 2014 - An empirical analysis of flaky tests.pdf:application/pdf},
}

@inproceedings{romano_empirical_2021,
	title = {An {Empirical} {Analysis} of {UI}-{Based} {Flaky} {Tests}},
	doi = {10.1109/ICSE43902.2021.00141},
	abstract = {Flaky tests have gained attention from the research community in recent years and with good reason. These tests lead to wasted time and resources, and they reduce the reliability of the test suites and build systems they affect. However, most of the existing work on flaky tests focus exclusively on traditional unit tests. This work ignores UI tests that have larger input spaces and more diverse running conditions than traditional unit tests. In addition, UI tests tend to be more complex and resource-heavy, making them unsuited for detection techniques involving rerunning test suites multiple times. In this paper, we perform a study on flaky UI tests. We analyze 235 flaky UI test samples found in 62 projects from both web and Android environments. We identify the common underlying root causes of flakiness in the UI tests, the strategies used to manifest the flaky behavior, and the fixing strategies used to remedy flaky UI tests. The findings made in this work can provide a foundation for the development of detection and prevention techniques for flakiness arising in UI tests.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Romano, Alan and Song, Zihe and Grandhi, Sampath and Yang, Wei and Wang, Weihang},
	month = may,
	year = {2021},
	note = {ISSN: 1558-1225},
	keywords = {Android, flaky test, flaky UI test, Reliability, Software development management, Software engineering, web},
	pages = {1585--1597},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\hedde\\Zotero\\storage\\I7ERXXND\\9402129.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\L84ZQFR8\\Romano et al. - 2021 - An Empirical Analysis of UI-Based Flaky Tests.pdf:application/pdf},
}

@misc{micco_state_2017,
	title = {The {State} of {Continuous} {Integration} {Testing} @{Google}},
	url = {https://research.google/pubs/pub45880/},
	author = {Micco, John},
	year = {2017},
	file = {Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\U25FP7TS\\Micco - 2017 - The State of Continuous Integration Testing @Googl.pdf:application/pdf},
}

@inproceedings{hilton_large-scale_2018,
	address = {New York, NY, USA},
	series = {{ASE} 2018},
	title = {A large-scale study of test coverage evolution},
	isbn = {978-1-4503-5937-5},
	url = {https://doi.org/10.1145/3238147.3238183},
	doi = {10.1145/3238147.3238183},
	abstract = {Statement coverage is commonly used as a measure of test suite quality. Coverage is often used as a part of a code review process: if a patch decreases overall coverage, or is itself not covered, then the patch is scrutinized more closely. Traditional studies of how coverage changes with code evolution have examined the overall coverage of the entire program, and more recent work directly examines the coverage of patches (changed statements). We present an evaluation much larger than prior studies and moreover consider a new, important kind of change --- coverage changes of unchanged statements. We present a large-scale evaluation of code coverage evolution over 7,816 builds of 47 projects written in popular languages including Java, Python, and Scala. We find that in large, mature projects, simply measuring the change to statement coverage does not capture the nuances of code evolution. Going beyond considering statement coverage as a simple ratio, we examine how the set of statements covered evolves between project revisions. We present and study new ways to assess the impact of a patch on a project's test suite quality that both separates coverage of the patch from coverage of the non-patch, and separates changes in coverage from changes in the set of statements covered.},
	urldate = {2022-11-06},
	booktitle = {Proceedings of the 33rd {ACM}/{IEEE} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Hilton, Michael and Bell, Jonathan and Marinov, Darko},
	month = sep,
	year = {2018},
	keywords = {code coverage, empirical study, flaky tests, Software testing},
	pages = {53--63},
	file = {Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\IUJXS3ZD\\Hilton et al. - 2018 - A large-scale study of test coverage evolution.pdf:application/pdf},
}

@inproceedings{ziftci_-flake_2020,
	title = {De-{Flake} {Your} {Tests} : {Automatically} {Locating} {Root} {Causes} of {Flaky} {Tests} in {Code} {At} {Google}},
	shorttitle = {De-{Flake} {Your} {Tests}},
	doi = {10.1109/ICSME46990.2020.00083},
	abstract = {Regression testing is a critical part of software development and maintenance. It ensures that modifications to existing software do not break existing behavior and functionality.One of the key assumptions about regression tests is that their results are deterministic: when executed without any modifications with the same configuration, either they always fail or they always pass. In practice, however, there exist tests that are non-deterministic, called flaky tests. Flaky tests cause the results of test runs to be unreliable, and they disrupt the software development workflow. In this paper, we present a novel technique to automatically identify the locations of the root causes of flaky tests on the code level to help developers debug and fix them. We study the technique on flaky tests across 428 projects at Google. Based on our case studies, the technique helps identify the location of the root causes of flakiness with 82\% accuracy. Furthermore, our studies show that integration into the appropriate developer workflows, simplicity of debugging aides and fully automated fixes are crucial and preferred components for adoption and usability of flakiness debugging and fixing tools.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Ziftci, Celal and Cavalcanti, Diego},
	month = sep,
	year = {2020},
	note = {ISSN: 2576-3148},
	keywords = {Debuggers, Debugging, Debugging aids, Diagnostics, Flaky tests, Internet, Maintenance engineering, Software, Software maintenance, Test management, Testing, Tools, Tracing, Usability},
	pages = {736--745},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\hedde\\Zotero\\storage\\NZMJNENF\\9240685.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\8MRI7TL6\\Ziftci and Cavalcanti - 2020 - De-Flake Your Tests  Automatically Locating Root .pdf:application/pdf},
}

@inproceedings{moran_debugging_2019,
	address = {Vienna, Austria},
	title = {Debugging {Flaky} {Tests} on {Web} {Applications}:},
	isbn = {978-989-758-386-5},
	shorttitle = {Debugging {Flaky} {Tests} on {Web} {Applications}},
	url = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0008559004540461},
	doi = {10.5220/0008559004540461},
	language = {en},
	urldate = {2022-11-06},
	booktitle = {Proceedings of the 15th {International} {Conference} on {Web} {Information} {Systems} and {Technologies}},
	publisher = {SCITEPRESS - Science and Technology Publications},
	author = {Morán, Jesus and Augusto, Cristian and Bertolino, Antonia and de la Riva, Claudio and Tuya, Javier},
	year = {2019},
	pages = {454--461},
	file = {Morán et al. - 2019 - Debugging Flaky Tests on Web Applications.pdf:C\:\\Users\\hedde\\Zotero\\storage\\ZE66V642\\Morán et al. - 2019 - Debugging Flaky Tests on Web Applications.pdf:application/pdf},
}

@inproceedings{kowalczyk_modeling_2020,
	address = {New York, NY, USA},
	series = {{ICSE}-{SEIP} '20},
	title = {Modeling and ranking flaky tests at {Apple}},
	isbn = {978-1-4503-7123-0},
	url = {http://doi.org/10.1145/3377813.3381370},
	doi = {10.1145/3377813.3381370},
	abstract = {Test flakiness---inability to reliably repeat a test's Pass/Fail outcome---continues to be a significant problem in Industry, adversely impacting continuous integration and test pipelines. Completely eliminating flaky tests is not a realistic option as a significant fraction of system tests (typically non-hermetic) for services-based implementations exhibit some level of flakiness. In this paper, we view the flakiness of a test as a rankable value, which we quantify, track and assign a confidence. We develop two ways to model flakiness, capturing the randomness of test results via entropy, and the temporal variation via flipRate, and aggregating these over time. We have implemented our flakiness scoring service and discuss how its adoption has impacted test suites of two large services at Apple. We show how flakiness is distributed across the tests in these services, including typical score ranges and outliers. The flakiness scores are used to monitor and detect changes in flakiness trends. Evaluation results demonstrate near perfect accuracy in ranking, identification and alignment with human interpretation. The scores were used to identify 2 causes of flakiness in the dataset evaluated, which have been confirmed, and where fixes have been implemented or are underway. Our models reduced flakiness by 44\% with less than 1\% loss in fault detection.},
	urldate = {2022-11-06},
	booktitle = {Proceedings of the {ACM}/{IEEE} 42nd {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice}},
	publisher = {Association for Computing Machinery},
	author = {Kowalczyk, Emily and Nair, Karan and Gao, Zebao and Silberstein, Leo and Long, Teng and Memon, Atif},
	month = sep,
	year = {2020},
	pages = {110--119},
	file = {Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\S7DQAA2G\\Kowalczyk et al. - 2020 - Modeling and ranking flaky tests at Apple.pdf:application/pdf},
}

@inproceedings{machalica_predictive_2019,
	title = {Predictive {Test} {Selection}},
	doi = {10.1109/ICSE-SEIP.2019.00018},
	abstract = {Change-based testing is a key component of continuous integration at Facebook. However, a large number of tests coupled with a high rate of changes committed to our monolithic repository make it infeasible to run all potentially-impacted tests on each change. We propose a new predictive test selection strategy which selects a subset of tests to exercise for each change submitted to the continuous integration system. The strategy is learned from a large dataset of historical test outcomes using basic machine learning techniques. Deployed in production, the strategy reduces the total infrastructure cost of testing code changes by a factor of two, while guaranteeing that over 95\% of individual test failures and over 99.9\% of faulty changes are still reported back to developers. The method we present here also accounts for the non-determinism of test outcomes, also known as test flakiness.},
	booktitle = {2019 {IEEE}/{ACM} 41st {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} ({ICSE}-{SEIP})},
	author = {Machalica, Mateusz and Samylkin, Alex and Porth, Meredith and Chandra, Satish},
	month = may,
	year = {2019},
	keywords = {continuous integration, Diamond, Facebook, flaky tests, Libraries, machine learning, Machine learning, Metadata, test selection, Testing, Tools},
	pages = {91--100},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\hedde\\Zotero\\storage\\GRH39I95\\8804462.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\9TQM863C\\Machalica et al. - 2019 - Predictive Test Selection.pdf:application/pdf},
}

@inproceedings{yu_terminator_2019,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2019},
	title = {{TERMINATOR}: better automated {UI} test case prioritization},
	isbn = {978-1-4503-5572-8},
	shorttitle = {{TERMINATOR}},
	url = {http://doi.org/10.1145/3338906.3340448},
	doi = {10.1145/3338906.3340448},
	abstract = {Automated UI testing is an important component of the continuous integration process of software development. A modern web-based UI is an amalgam of reports from dozens of microservices written by multiple teams. Queries on a page that opens up another will fail if any of that page's microservices fails. As a result, the overall cost for automated UI testing is high since the UI elements cannot be tested in isolation. For example, the entire automated UI testing suite at LexisNexis takes around 30 hours (3-5 hours on the cloud) to execute, which slows down the continuous integration process. To mitigate this problem and give developers faster feedback on their code, test case prioritization techniques are used to reorder the automated UI test cases so that more failures can be detected earlier. Given that much of the automated UI testing is "black box" in nature, very little information (only the test case descriptions and testing results) can be utilized to prioritize these automated UI test cases. Hence, this paper evaluates 17 "black box" test case prioritization approaches that do not rely on source code information. Among these, we propose a novel TCP approach, that dynamically re-prioritizes the test cases when new failures are detected, by applying and adapting a state of the art framework from the total recall problem. Experimental results on LexisNexis automated UI testing data show that our new approach (which we call TERMINATOR), outperformed prior state of the art approaches in terms of failure detection rates with negligible CPU overhead.},
	urldate = {2022-11-06},
	booktitle = {Proceedings of the 2019 27th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Yu, Zhe and Fahid, Fahmid and Menzies, Tim and Rothermel, Gregg and Patrick, Kyle and Cherian, Snehit},
	month = aug,
	year = {2019},
	keywords = {automated UI testing, test case prioritization, total recall},
	pages = {883--894},
	file = {Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\6N3BMGEX\\Yu et al. - 2019 - TERMINATOR better automated UI test case prioriti.pdf:application/pdf},
}

@inproceedings{shi_understanding_2019,
	title = {Understanding and {Improving} {Regression} {Test} {Selection} in {Continuous} {Integration}},
	doi = {10.1109/ISSRE.2019.00031},
	abstract = {Developers rely on regression testing in their continuous integration (CI) environment to find changes that introduce regression faults. While regression testing is widely practiced, it can be costly. Regression test selection (RTS) reduces the cost of regression testing by not running the tests that are unaffected by the changes. Industry has adopted module-level RTS for their CI environment, while researchers have proposed class-level RTS. In this paper, we compare module-and class-level RTS techniques in a cloud-based CI environment, Travis. We also develop and evaluate a hybrid RTS technique that combines aspects of the module-and class-level RTS techniques. We evaluate all the techniques on real Travis builds. We find that the RTS techniques do save testing time compared to running all tests (RetestAll), but the percentage of time for a full build using RTS (76.0\%) is not as low as found in previous work, due to the extra overhead in a cloud-based CI environment. Moreover, we inspect test failures from RetestAll builds, and although we find that RTS techniques can miss to select failed tests, these test failures are almost all flaky test failures. As such, RTS techniques provide additional value in helping developers avoid wasting time debugging failures not related to the recent code changes. Overall, our results show that RTS can be beneficial for the developers in the CI environment, and RTS not only saves time but also avoids misleading developers by flaky test failures.},
	booktitle = {2019 {IEEE} 30th {International} {Symposium} on {Software} {Reliability} {Engineering} ({ISSRE})},
	author = {Shi, August and Zhao, Peiyuan and Marinov, Darko},
	month = oct,
	year = {2019},
	note = {ISSN: 2332-6549},
	keywords = {continuous integration, flaky tests, regression test selection},
	pages = {228--238},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\hedde\\Zotero\\storage\\MNDBAJLG\\8987498.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\YTDAEN4S\\Shi et al. - 2019 - Understanding and Improving Regression Test Select.pdf:application/pdf},
}

@inproceedings{presler-marshall_wait_2019,
	title = {Wait, {Wait}. {No}, {Tell} {Me}. {Analyzing} {Selenium} {Configuration} {Effects} on {Test} {Flakiness}},
	doi = {10.1109/AST.2019.000-1},
	abstract = {Flaky tests are a source of frustration and uncertainty for developers. In an educational environment, flaky tests can create doubts related to software behavior and student grades, especially when the grades depend on tests passing. NC State University's junior-level software engineering course models industrial practice through team-based development and testing of new features on a large electronic health record (EHR) system, iTrust2. Students are expected to maintain and supplement an extensive suite of UI tests using Selenium WebDriver. Team builds are run on the course's continuous integration (CI) infrastructure. Students report, and we confirm, that tests that pass on one build will inexplicably fail on the next, impacting productivity and confidence in code quality and the CI system. The goal of this work is to find and fix the sources of flaky tests in iTrust2. We analyze configurations of Selenium using different underlying web browsers and timeout strategies (waits) for both test stability and runtime performance. We also consider underlying hardware and operating systems. Our results show that HtmlUnit with Thread waits provides the lowest number of test failures and best runtime on poor-performing hardware. When given more resources (e.g., more memory and a faster CPU), Google Chrome with Angular waits is less flaky and faster than HtmlUnit, especially if the browser instance is not restarted between tests. The outcomes of this research are a more stable and substantially faster teaching application and a recommendation on how to configure Selenium for applications similar to iTrust2 that run in a CI environment.},
	booktitle = {2019 {IEEE}/{ACM} 14th {International} {Workshop} on {Automation} of {Software} {Test} ({AST})},
	author = {Presler-Marshall, Kai and Horton, Eric and Heckman, Sarah and Stolee, Kathryn},
	month = may,
	year = {2019},
	keywords = {Browsers, Flaky Tests, GUI Tests, Hardware, Operating systems, Runtime, Selenium, Software Testing, Vehicles, WebDriver},
	pages = {7--13},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\hedde\\Zotero\\storage\\9CJXCPXX\\8821891.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\JD44DVWK\\Presler-Marshall et al. - 2019 - Wait, Wait. No, Tell Me. Analyzing Selenium Config.pdf:application/pdf},
}

@inproceedings{gao_which_2015,
	title = {Which of {My} {Failures} are {Real}? {Using} {Relevance} {Ranking} to {Raise} {True} {Failures} to the {Top}},
	shorttitle = {Which of {My} {Failures} are {Real}?},
	doi = {10.1109/ASEW.2015.7},
	abstract = {GUI reference testing is performed to detect regression errors in a modified or patched GUI software. Test cases are executed on the original and modified GUIs, differences in the states of GUI widgets across versions indicate potential defects. However, various factors (e.g., position, flakiness, resolution) create problems for accurate GUI state collection, leading to spurious state mismatches, and hence false positives, these need to be weeded out manually. In this paper, we show that the problem of false positives is significant, often inundating the tester with a large number of false bug reports, requiring a disproportionate amount of manual effort. We develop an entropy-based approach to rank each GUI widget property, and use it to determine whether a state mismatch (indicative of a bug) is real or a false positive. Our empirical evaluation shows that this ranking helps to percolate real bugs to the top of a set of reported bugs, thereby helping to economize tester time.},
	booktitle = {2015 30th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} {Workshop} ({ASEW})},
	author = {Gao, Zebao and Memon, Atif M.},
	month = nov,
	year = {2015},
	keywords = {Computer bugs, Computer science, Electronic mail, Entropy, Graphical user interfaces, Software, Testing},
	pages = {62--69},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\hedde\\Zotero\\storage\\KNI6UNKA\\7426638.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\NRTT94NP\\Gao and Memon - 2015 - Which of My Failures are Real Using Relevance Ran.pdf:application/pdf},
}

@inproceedings{shi_ifixflakies_2019,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2019},
	title = {{iFixFlakies}: a framework for automatically fixing order-dependent flaky tests},
	isbn = {978-1-4503-5572-8},
	shorttitle = {{iFixFlakies}},
	url = {http://doi.org/10.1145/3338906.3338925},
	doi = {10.1145/3338906.3338925},
	abstract = {Regression testing provides important pass or fail signals that developers use to make decisions after code changes. However, flaky tests, which pass or fail even when the code has not changed, can mislead developers. A common kind of flaky tests are order-dependent tests, which pass or fail depending on the order in which the tests are run. Fixing order-dependent tests is often tedious and time-consuming. We propose iFixFlakies, a framework for automatically fixing order-dependent tests. The key insight in iFixFlakies is that test suites often already have tests, which we call helpers, whose logic resets or sets the states for order-dependent tests to pass. iFixFlakies searches a test suite for helpers that make the order-dependent tests pass and then recommends patches for the order-dependent tests using code from these helpers. Our evaluation on 110 truly orderdependent tests from a public dataset shows that 58 of them have helpers, and iFixFlakies can fix all 58. We opened pull requests for 56 order-dependent tests (2 of 58 were already fixed), and developers have already accepted pull requests for 21 of them, with all the remaining ones still pending.},
	urldate = {2022-11-06},
	booktitle = {Proceedings of the 2019 27th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Shi, August and Lam, Wing and Oei, Reed and Xie, Tao and Marinov, Darko},
	month = aug,
	year = {2019},
	keywords = {automated fixing, flaky test, order-dependent test, patch generation},
	pages = {545--555},
	file = {Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\S3GH48QX\\Shi et al. - 2019 - iFixFlakies a framework for automatically fixing .pdf:application/pdf},
}

@inproceedings{eck_understanding_2019,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2019},
	title = {Understanding flaky tests: the developer’s perspective},
	isbn = {978-1-4503-5572-8},
	shorttitle = {Understanding flaky tests},
	url = {http://doi.org/10.1145/3338906.3338945},
	doi = {10.1145/3338906.3338945},
	abstract = {Flaky tests are software tests that exhibit a seemingly random outcome (pass or fail) despite exercising unchanged code. In this work, we examine the perceptions of software developers about the nature, relevance, and challenges of flaky tests. We asked 21 professional developers to classify 200 flaky tests they previously fixed, in terms of the nature and the origin of the flakiness, as well as of the fixing effort. We also examined developers' fixing strategies. Subsequently, we conducted an online survey with 121 developers with a median industrial programming experience of five years. Our research shows that: The flakiness is due to several different causes, four of which have never been reported before, despite being the most costly to fix; flakiness is perceived as significant by the vast majority of developers, regardless of their team's size and project's domain, and it can have effects on resource allocation, scheduling, and the perceived reliability of the test suite; and the challenges developers report to face regard mostly the reproduction of the flaky behavior and the identification of the cause for the flakiness. Public preprint [http://arxiv.org/abs/1907.01466], data and materials [https://doi-org.eaccess.ub.tum.de/10.5281/zenodo.3265785].},
	urldate = {2022-11-06},
	booktitle = {Proceedings of the 2019 27th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Eck, Moritz and Palomba, Fabio and Castelluccio, Marco and Bacchelli, Alberto},
	month = aug,
	year = {2019},
	keywords = {Empirical Studies, Flaky Tests, Mixed-Method Research},
	pages = {830--840},
	file = {Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\392ZMRZN\\Eck et al. - 2019 - Understanding flaky tests the developer’s perspec.pdf:application/pdf},
}

@inproceedings{gao_making_2015,
	title = {Making {System} {User} {Interactive} {Tests} {Repeatable}: {When} and {What} {Should} {We} {Control}?},
	volume = {1},
	shorttitle = {Making {System} {User} {Interactive} {Tests} {Repeatable}},
	doi = {10.1109/ICSE.2015.28},
	abstract = {System testing and invariant detection is usually conducted from the user interface perspective when the goal is to evaluate the behavior of an application as a whole. A large number of tools and techniques have been developed to generate and automate this process, many of which have been evaluated in the literature or internally within companies. Typical metrics for determining effectiveness of these techniques include code coverage and fault detection, however, with the assumption that there is determinism in the resulting outputs. In this paper we examine the extent to which a common set of factors such as the system platform, Java version, application starting state and tool harness configurations impact these metrics. We examine three layers of testing outputs: the code layer, the behavioral (or invariant) layer and the external (or user interaction) layer. In a study using five open source applications across three operating system platforms, manipulating several factors, we observe as many as 184 lines of code coverage difference between runs using the same test cases, and up to 96 percent false positives with respect to fault detection. We also see some a small variation among the invariants inferred. Despite our best efforts, we can reduce, but not completely eliminate all possible variation in the output. We use our findings to provide a set of best practices that should lead to better consistency and smaller differences in test outcomes, allowing more repeatable and reliable testing and experimentation.},
	booktitle = {2015 {IEEE}/{ACM} 37th {IEEE} {International} {Conference} on {Software} {Engineering}},
	author = {Gao, Zebao and Liang, Yalan and Cohen, Myra B. and Memon, Atif M. and Wang, Zhen},
	month = may,
	year = {2015},
	note = {ISSN: 1558-1225},
	keywords = {benchmarking, Delays, Entropy, experimentation, graphical user interfaces, Graphical user interfaces, Java, Operating systems, software testing, Testing},
	pages = {55--65},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\hedde\\Zotero\\storage\\JWPDQR8T\\7194561.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\7ML3PCSZ\\Gao et al. - 2015 - Making System User Interactive Tests Repeatable W.pdf:application/pdf},
}

@inproceedings{lam_dependent-test-aware_2020,
	address = {New York, NY, USA},
	series = {{ISSTA} 2020},
	title = {Dependent-test-aware regression testing techniques},
	isbn = {978-1-4503-8008-9},
	url = {http://doi.org/10.1145/3395363.3397364},
	doi = {10.1145/3395363.3397364},
	abstract = {Developers typically rely on regression testing techniques to ensure that their changes do not break existing functionality. Unfortunately, these techniques suffer from flaky tests, which can both pass and fail when run multiple times on the same version of code and tests. One prominent type of flaky tests is order-dependent (OD) tests, which are tests that pass when run in one order but fail when run in another order. Although OD tests may cause flaky-test failures, OD tests can help developers run their tests faster by allowing them to share resources. We propose to make regression testing techniques dependent-test-aware to reduce flaky-test failures. To understand the necessity of dependent-test-aware regression testing techniques, we conduct the first study on the impact of OD tests on three regression testing techniques: test prioritization, test selection, and test parallelization. In particular, we implement 4 test prioritization, 6 test selection, and 2 test parallelization algorithms, and we evaluate them on 11 Java modules with OD tests. When we run the orders produced by the traditional, dependent-test-unaware regression testing algorithms, 82\% of human-written test suites and 100\% of automatically-generated test suites with OD tests have at least one flaky-test failure. We develop a general approach for enhancing regression testing algorithms to make them dependent-test-aware, and apply our approach to 12 algorithms. Compared to traditional, unenhanced regression testing algorithms, the enhanced algorithms use provided test dependencies to produce orders with different permutations or extra tests. Our evaluation shows that, in comparison to the orders produced by unenhanced algorithms, the orders produced by enhanced algorithms (1) have overall 80\% fewer flaky-test failures due to OD tests, and (2) may add extra tests but run only 1\% slower on average. Our results suggest that enhancing regression testing algorithms to be dependent-test-aware can substantially reduce flaky-test failures with only a minor slowdown to run the tests.},
	urldate = {2022-11-06},
	booktitle = {Proceedings of the 29th {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Lam, Wing and Shi, August and Oei, Reed and Zhang, Sai and Ernst, Michael D. and Xie, Tao},
	month = jul,
	year = {2020},
	keywords = {flaky test, order-dependent test, regression testing},
	pages = {298--311},
	file = {Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\NZ68F9S8\\Lam et al. - 2020 - Dependent-test-aware regression testing techniques.pdf:application/pdf},
}

@inproceedings{gambi_practical_2018,
	title = {Practical {Test} {Dependency} {Detection}},
	doi = {10.1109/ICST.2018.00011},
	abstract = {Regression tests should consistently produce the same outcome when executed against the same version of the system under test. Recent studies, however, show a different picture: in many cases simply changing the order in which tests execute is enough to produce different test outcomes. These studies also identify the presence of dependencies between tests as one likely cause of this behavior. Test dependencies affect the quality of tests and of the correlated development activities, like regression test selection, prioritization, and parallelization, which assume that tests are independent. Therefore, developers must promptly identify and resolve problematic test dependencies. This paper presents PRADET, a novel approach for detecting problematic dependencies that is both effective and efficient. PRADET uses a systematic, data-driven process to detect problematic test dependencies significantly faster and more precisely than prior work. PRADET scales to analyze large projects with thousands of tests that existing tools cannot analyze in reasonable amount of time, and found 27 previously unknown dependencies.},
	booktitle = {2018 {IEEE} 11th {International} {Conference} on {Software} {Testing}, {Verification} and {Validation} ({ICST})},
	author = {Gambi, Alessio and Bell, Jonathan and Zeller, Andreas},
	month = apr,
	year = {2018},
	keywords = {Computer bugs, data-flow, detection algorithm, empirical study, flaky tests, Minimization, Out of order, Pollution, Test dependence, Testing, Tools},
	pages = {1--11},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\hedde\\Zotero\\storage\\XJH4KSYU\\8367031.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\UW27Y3MB\\Gambi et al. - 2018 - Practical Test Dependency Detection.pdf:application/pdf},
}

@inproceedings{lam_idflakies_2019,
	title = {{iDFlakies}: {A} {Framework} for {Detecting} and {Partially} {Classifying} {Flaky} {Tests}},
	shorttitle = {{iDFlakies}},
	doi = {10.1109/ICST.2019.00038},
	abstract = {Regression testing is increasingly important with the wide use of continuous integration. A desirable requirement for regression testing is that a test failure reliably indicates a problem in the code under test and not a false alarm from the test code or the testing infrastructure. However, some test failures are unreliable, stemming from flaky tests that can nondeterministically pass or fail for the same code under test. There are many types of flaky tests, with order-dependent tests being a prominent type. To help advance research on flaky tests, we present (1) a framework, iDFlakies, to detect and partially classify flaky tests; (2) a dataset of flaky tests in open-source projects; and (3) a study with our dataset. iDFlakies automates experimentation with our tool for Maven-based Java projects. Using iDFlakies, we build a dataset of 422 flaky tests, with 50.5\% order-dependent and 49.5\% not. Our study of these flaky tests finds the prevalence of two types of flaky tests, probability of a test-suite run to have at least one failure due to flaky tests, and how different test reorderings affect the number of detected flaky tests. We envision that our work can spur research to alleviate the problem of flaky tests.},
	booktitle = {2019 12th {IEEE} {Conference} on {Software} {Testing}, {Validation} and {Verification} ({ICST})},
	author = {Lam, Wing and Oei, Reed and Shi, August and Marinov, Darko and Xie, Tao},
	month = apr,
	year = {2019},
	note = {ISSN: 2159-4848},
	keywords = {Concurrent computing, flaky tests, Java, Open source software, order dependent tests, regression testing, Reliability, Servers, Testing, Tools},
	pages = {312--322},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\hedde\\Zotero\\storage\\CZSVB7VF\\8730188.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\4TNPIMX2\\Lam et al. - 2019 - iDFlakies A Framework for Detecting and Partially.pdf:application/pdf},
}

@inproceedings{biagiola_web_2019,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2019},
	title = {Web test dependency detection},
	isbn = {978-1-4503-5572-8},
	url = {http://doi.org/10.1145/3338906.3338948},
	doi = {10.1145/3338906.3338948},
	abstract = {E2E web test suites are prone to test dependencies due to the heterogeneous multi-tiered nature of modern web apps, which makes it difficult for developers to create isolated program states for each test case. In this paper, we present the first approach for detecting and validating test dependencies present in E2E web test suites. Our approach employs string analysis to extract an approximated set of dependencies from the test code. It then filters potential false dependencies through natural language processing of test names. Finally, it validates all dependencies, and uses a novel recovery algorithm to ensure no true dependencies are missed in the final test dependency graph. Our approach is implemented in a tool called TEDD and evaluated on the test suites of six open-source web apps. Our results show that TEDD can correctly detect and validate test dependencies up to 72\% faster than the baseline with the original test ordering in which the graph contains all possible dependencies. The test dependency graphs produced by TEDD enable test execution parallelization, with a speed-up factor of up to 7×.},
	urldate = {2022-11-06},
	booktitle = {Proceedings of the 2019 27th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Biagiola, Matteo and Stocco, Andrea and Mesbah, Ali and Ricca, Filippo and Tonella, Paolo},
	month = aug,
	year = {2019},
	keywords = {NLP, test dependency, web testing},
	pages = {154--164},
	file = {Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\BF7EJ3Z7\\Biagiola et al. - 2019 - Web test dependency detection.pdf:application/pdf},
}

@article{lam_large-scale_2020,
	title = {A large-scale longitudinal study of flaky tests},
	volume = {4},
	url = {http://doi.org/10.1145/3428270},
	doi = {10.1145/3428270},
	abstract = {Flaky tests are tests that can non-deterministically pass or fail for the same code version. These tests undermine regression testing efficiency, because developers cannot easily identify whether a test fails due to their recent changes or due to flakiness. Ideally, one would detect flaky tests right when flakiness is introduced, so that developers can then immediately remove the flakiness. Some software organizations, e.g., Mozilla and Netflix, run some tools—detectors—to detect flaky tests as soon as possible. However, detecting flaky tests is costly due to their inherent non-determinism, so even state-of-the-art detectors are often impractical to be used on all tests for each project change. To combat the high cost of applying detectors, these organizations typically run a detector solely on newly added or directly modified tests, i.e., not on unmodified tests or when other changes occur (including changes to the test suite, the code under test, and library dependencies). However, it is unclear how many flaky tests can be detected or missed by applying detectors in only these limited circumstances. To better understand this problem, we conduct a large-scale longitudinal study of flaky tests to determine when flaky tests become flaky and what changes cause them to become flaky. We apply two state-of-theart detectors to 55 Java projects, identifying a total of 245 flaky tests that can be compiled and run in the code version where each test was added. We find that 75\% of flaky tests (184 out of 245) are flaky when added, indicating substantial potential value for developers to run detectors specifically on newly added tests. However, running detectors solely on newly added tests would still miss detecting 25\% of flaky tests. The percentage of flaky tests that can be detected does increase to 85\% when detectors are run on newly added or directly modified tests. The remaining 15\% of flaky tests become flaky due to other changes and can be detected only when detectors are always applied to all tests. Our study is the first to empirically evaluate when tests become flaky and to recommend guidelines for applying detectors in the future.},
	number = {OOPSLA},
	urldate = {2022-11-06},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Lam, Wing and Winter, Stefan and Wei, Anjiang and Xie, Tao and Marinov, Darko and Bell, Jonathan},
	month = nov,
	year = {2020},
	keywords = {flaky test, regression testing},
	pages = {202:1--202:29},
	file = {Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\47Q5F6RE\\Lam et al. - 2020 - A large-scale longitudinal study of flaky tests.pdf:application/pdf},
}

@inproceedings{silva_shake_2020,
	title = {Shake {It}! {Detecting} {Flaky} {Tests} {Caused} by {Concurrency} with {Shaker}},
	doi = {10.1109/ICSME46990.2020.00037},
	abstract = {A test is said to be flaky when it non-deterministically passes or fails. Test flakiness negatively affects the effectiveness of regression testing and, consequently, impacts software evolution. Detecting test flakiness is an important and challenging problem. ReRun is the most popular approach in industry to detect test flakiness. It re-executes a test suite on a fixed code version multiple times, looking for inconsistent outputs across executions. Unfortunately, ReRun is costly and unreliable. This paper proposes SHAKER, a lightweight technique to improve the ability of ReRun to detect flaky tests. SHAKER adds noise in the execution environment (e.g., it adds stressor tasks to compete for the CPU or memory). It builds on the observations that concurrency is an important source of flakiness and that adding noise in the environment can interfere in the ordering of events and, consequently, influence on the test outputs. We conducted experiments on a data set with 11 Android apps. Results are very encouraging. SHAKER discovered many more flaky tests than ReRun (95\% and 37.5\% of the total, respectively) and discovered these flaky tests much faster. In addition, SHAKER was able to reveal 61 new flaky tests that went undetected in 50 re-executions with ReRun.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Silva, Denini and Teixeira, Leopoldo and d’Amorim, Marcelo},
	month = sep,
	year = {2020},
	note = {ISSN: 2576-3148},
	keywords = {Concurrent computing, Conferences, Industries, noise, regression testing, Software maintenance, Task analysis, test flakiness, Testing},
	pages = {301--311},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\hedde\\Zotero\\storage\\ZUUJIL5Z\\9240694.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\YHLRFPUW\\Silva et al. - 2020 - Shake It! Detecting Flaky Tests Caused by Concurre.pdf:application/pdf},
}

@inproceedings{lam_study_2020,
	address = {New York, NY, USA},
	series = {{ICSE} '20},
	title = {A study on the lifecycle of flaky tests},
	isbn = {978-1-4503-7121-6},
	url = {http://doi.org/10.1145/3377811.3381749},
	doi = {10.1145/3377811.3381749},
	abstract = {During regression testing, developers rely on the pass or fail outcomes of tests to check whether changes broke existing functionality. Thus, flaky tests, which nondeterministically pass or fail on the same code, are problematic because they provide misleading signals during regression testing. Although flaky tests are the focus of several existing studies, none of them study (1) the reoccurrence, runtimes, and time-before-fix of flaky tests, and (2) flaky tests in-depth on proprietary projects. This paper fills this knowledge gap about flaky tests and investigates whether prior categorization work on flaky tests also apply to proprietary projects. Specifically, we study the lifecycle of flaky tests in six large-scale proprietary projects at Microsoft. We find, as in prior work, that asynchronous calls are the leading cause of flaky tests in these Microsoft projects. Therefore, we propose the first automated solution, called Flakiness and Time Balancer (FaTB), to reduce the frequency of flaky-test failures caused by asynchronous calls. Our evaluation of five such flaky tests shows that FaTB can reduce the running times of these tests by up to 78\% without empirically affecting the frequency of their flaky-test failures. Lastly, our study finds several cases where developers claim they "fixed" a flaky test but our empirical experiments show that their changes do not fix or reduce these tests' frequency of flaky-test failures. Future studies should be more cautious when basing their results on changes that developers claim to be "fixes".},
	urldate = {2022-11-06},
	booktitle = {Proceedings of the {ACM}/{IEEE} 42nd {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Lam, Wing and Muşlu, Kıvanç and Sajnani, Hitesh and Thummalapenta, Suresh},
	month = oct,
	year = {2020},
	keywords = {empirical study, flaky test, lifecycle},
	pages = {1471--1482},
	file = {Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\3EY52P62\\Lam et al. - 2020 - A study on the lifecycle of flaky tests.pdf:application/pdf},
}

@article{verdecchia_know_2021,
	title = {Know {You} {Neighbor}: {Fast} {Static} {Prediction} of {Test} {Flakiness}},
	volume = {9},
	issn = {2169-3536},
	shorttitle = {Know {You} {Neighbor}},
	doi = {10.1109/ACCESS.2021.3082424},
	abstract = {Context: Flaky tests plague regression testing in Continuous Integration environments by slowing down change releases and wasting testing time and effort. Despite the growing interest in mitigating the burden of test flakiness, how to efficiently and effectively detect flaky tests is still an open problem. Objective: In this study, we present and evaluate FLAST, an approach designed to statically predict test flakiness. FLAST leverages vector-space modeling, similarity search, dimensionality reduction, and k-Nearest Neighbor classification in order to timely and efficiently detect test flakiness. Method: In order to gain insights into the efficiency and effectiveness of FLAST, we conduct an empirical evaluation of the approach by considering 13 real-world projects, for a total of 1,383 flaky and 26,702 non-flaky tests. We carry out a quantitative comparison of FLAST with the state-of-the-art methods to detect test flakiness, by considering a balanced dataset comprising 1,402 real-world flaky and as many non-flaky tests. Results: From the results we observe that the effectiveness of FLAST is comparable with the state-of-the-art, while providing considerable gains in terms of efficiency. In addition, the results demonstrate how by tuning the threshold of the approach FLAST can be made more conservative, so to reduce false positives, at the cost of missing more potentially flaky tests. Conclusion: The collected results demonstrate that FLAST provides a fast, low-cost and reliable approach that can be used to guide test rerunning, or to gate the inclusion of new potentially flaky tests.},
	journal = {IEEE Access},
	author = {Verdecchia, Roberto and Cruciani, Emilio and Miranda, Breno and Bertolino, Antonia},
	year = {2021},
	note = {Conference Name: IEEE Access},
	keywords = {flaky tests, Instruments, prediction, Probabilistic logic, similarity, Social networking (online), Software testing, static analysis, Static analysis, Testing, Tools, Vocabulary},
	pages = {76119--76134},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\hedde\\Zotero\\storage\\EZANYDT6\\9437181.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\R66TCUIY\\Verdecchia et al. - 2021 - Know You Neighbor Fast Static Prediction of Test .pdf:application/pdf},
}

@inproceedings{alshammari_flakeflagger_2021,
	title = {{FlakeFlagger}: {Predicting} {Flakiness} {Without} {Rerunning} {Tests}},
	shorttitle = {{FlakeFlagger}},
	doi = {10.1109/ICSE43902.2021.00140},
	abstract = {When developers make changes to their code, they typically run regression tests to detect if their recent changes (re) introduce any bugs. However, many tests are flaky, and their outcomes can change non-deterministically, failing without apparent cause. Flaky tests are a significant nuisance in the development process, since they make it more difficult for developers to trust the outcome of their tests, and hence, it is important to know which tests are flaky. The traditional approach to identify flaky tests is to rerun them multiple times: if a test is observed both passing and failing on the same code, it is definitely flaky. We conducted a very large empirical study looking for flaky tests by rerunning the test suites of 24 projects 10,000 times each, and found that even with this many reruns, some previously identified flaky tests were still not detected. We propose FlakeFlagger, a novel approach that collects a set of features describing the behavior of each test, and then predicts tests that are likely to be flaky based on similar behavioral features. We found that FlakeFlagger correctly labeled as flaky at least as many tests as a state-of-the-art flaky test classifier, but that FlakeFlagger reported far fewer false positives. This lower false positive rate translates directly to saved time for researchers and developers who use the classification result to guide more expensive flaky test detection processes. Evaluated on our dataset of 23 projects with flaky tests, FlakeFlagger outperformed the prior approach (by F1 score) on 16 projects and tied on 4 projects. Our results indicate that this approach can be effective for identifying likely flaky tests prior to running time-consuming flaky test detectors.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Alshammari, Abdulrahman and Morris, Christopher and Hilton, Michael and Bell, Jonathan},
	month = may,
	year = {2021},
	note = {ISSN: 1558-1225},
	keywords = {Computer bugs, Detectors, flaky tests, regression testing, reliability, Software engineering, Testing},
	pages = {1572--1584},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\hedde\\Zotero\\storage\\6XUXBXKA\\9402098.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\CJ6EIRWR\\Alshammari et al. - 2021 - FlakeFlagger Predicting Flakiness Without Rerunni.pdf:application/pdf},
}

@article{zolfaghari_root_2021,
	title = {Root causing, detecting, and fixing flaky tests: {State} of the art and future roadmap},
	volume = {51},
	issn = {1097-024X},
	shorttitle = {Root causing, detecting, and fixing flaky tests},
	url = {http://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2929},
	doi = {10.1002/spe.2929},
	abstract = {A flaky test is a test that may lead to different results in different runs on a single code under test without any change in the test code. Test flakiness is a noxious phenomenon that slows down software deployment, and increases the expenditures in a broad spectrum of platforms such as software-defined networks and Internet of Things environments. Industrial institutes and labs have conducted a whole lot of research projects aiming at tackling this problem. Although this issue has been receiving more attention from academia in recent years, the academic research community is still behind the industry in this area. A systematic review and trend analysis on the existing approaches for detecting and root causing flaky tests can pave the way for future research on this topic. This can help academia keep pace with industrial advancements and even lead the research in this field. This article first presents a comprehensive review of recent achievements of the industry as well as academia regarding the detection and mitigation of flaky tests. In the next step, recent trends in this line of research are analyzed and a roadmap is established for future research.},
	language = {en},
	number = {5},
	urldate = {2022-11-06},
	journal = {Software: Practice and Experience},
	author = {Zolfaghari, Behrouz and Parizi, Reza M. and Srivastava, Gautam and Hailemariam, Yoseph},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/spe.2929},
	keywords = {detection, flaky testing, software, tools},
	pages = {851--867},
	file = {Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\RR5WHG9R\\Zolfaghari et al. - 2021 - Root causing, detecting, and fixing flaky tests S.pdf:application/pdf;Snapshot:C\:\\Users\\hedde\\Zotero\\storage\\ZNCUMQRG\\spe.html:text/html},
}

@inproceedings{choudhary_water_2011,
	address = {New York, NY, USA},
	series = {{ETSE} '11},
	title = {{WATER}: {Web} {Application} {TEst} {Repair}},
	isbn = {978-1-4503-0808-3},
	shorttitle = {{WATER}},
	url = {http://doi.org/10.1145/2002931.2002935},
	doi = {10.1145/2002931.2002935},
	abstract = {Web applications tend to evolve quickly, resulting in errors and failures in test automation scripts that exercise them. Repairing such scripts to work on the updated application is essential for maintaining the quality of the test suite. Updating such scripts manually is a time consuming task, which is often difficult and is prone to errors if not performed carefully. In this paper, we propose a technique to automatically suggest repairs for such web application test scripts. Our technique is based on differential testing and compares the behavior of the test case on two successive versions of the web application: first version in which the test script runs successfully and the second version in which the script results in an error or failure. By analyzing the difference between these two executions, our technique suggests repairs that can be applied to repair the scripts. To evaluate our technique, we implemented it in a tool called WATER and exercised it on real web applications with test cases. Our experiments show that WATER can suggest meaningful repairs for practical test cases, many of which correspond to those made later by developers themselves.},
	urldate = {2023-02-15},
	booktitle = {Proceedings of the {First} {International} {Workshop} on {End}-to-{End} {Test} {Script} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Choudhary, Shauvik Roy and Zhao, Dan and Versee, Husayn and Orso, Alessandro},
	month = jul,
	year = {2011},
	keywords = {test repair, web testing},
	pages = {24--29},
	file = {Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\SZSEDXC9\\Choudhary et al. - 2011 - WATER Web Application TEst Repair.pdf:application/pdf},
}

@inproceedings{stocco_visual_2018,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2018},
	title = {Visual web test repair},
	isbn = {978-1-4503-5573-5},
	url = {http://doi.org/10.1145/3236024.3236063},
	doi = {10.1145/3236024.3236063},
	abstract = {Web tests are prone to break frequently as the application under test evolves, causing much maintenance effort in practice. To detect the root causes of a test breakage, developers typically inspect the test's interactions with the application through the GUI. Existing automated test repair techniques focus instead on the code and entirely ignore visual aspects of the application. We propose a test repair technique that is informed by a visual analysis of the application. Our approach captures relevant visual information from tests execution and analyzes them through a fast image processing pipeline to visually validate test cases as they re-executed for regression purposes. Then, it reports the occurrences of breakages and potential fixes to the testers. Our approach is also equipped with a local crawling mechanism to handle non-trivial breakage scenarios such as the ones that require to repair the test's workflow. We implemented our approach in a tool called Vista. Our empirical evaluation on 2,672 test cases spanning 86 releases of four web applications shows that Vista is able to repair, on average, 81\% of the breakages, a 41\% increment with respect to existing techniques.},
	urldate = {2023-02-15},
	booktitle = {Proceedings of the 2018 26th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Stocco, Andrea and Yandrapally, Rahulkrishna and Mesbah, Ali},
	month = oct,
	year = {2018},
	keywords = {computer vision, image analysis, test repair, web testing},
	pages = {503--514},
	file = {Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\WD9ZW6Q6\\Stocco et al. - 2018 - Visual web test repair.pdf:application/pdf},
}

@inproceedings{hammoudi_why_2016,
	title = {Why do {Record}/{Replay} {Tests} of {Web} {Applications} {Break}?},
	doi = {10.1109/ICST.2016.16},
	abstract = {Software engineers often use record/replay tools to enable the automated testing of web applications. Tests created in this manner can then be used to regression test new versions of the web applications as they evolve. Web application tests recorded by record/replay tools, however, can be quite brittle, they can easily break as applications change. For this reason, researchers have begun to seek approaches for automatically repairing record/replay tests. To date, however, there have been no comprehensive attempts to characterize the causes of breakagesin record/replay tests for web applications. In this work, wepresent a taxonomy classifying the ways in which record/replay tests for web applications break, based on an analysis of 453 versions of popular web applications for which 1065 individual test breakages were recognized. The resulting taxonomy can help direct researchers in their attempts to repair such tests. It can also help practitioners by suggesting best practices when creating tests or modifying programs, and can help researchers with other tasks such as test robustness analysis and IDE design.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Software} {Testing}, {Verification} and {Validation} ({ICST})},
	author = {Hammoudi, Mouna and Rothermel, Gregg and Tonella, Paolo},
	month = apr,
	year = {2016},
	keywords = {Frequency measurement, HTML, Maintenance engineering, Record/replay testing, Selenium, Taxonomy, test breakages, test repair, Testing, web applications, Web pages},
	pages = {180--190},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\hedde\\Zotero\\storage\\C7N6I9HI\\7515470.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\hedde\\Zotero\\storage\\YYJWU7H5\\Hammoudi et al. - 2016 - Why do RecordReplay Tests of Web Applications Bre.pdf:application/pdf},
}

@book{brugge_object-oriented_2014,
	address = {Harlow, Essex},
	edition = {3. ed., international ed},
	title = {Object-oriented software engineering: using {UML}, patterns, and {Java}},
	isbn = {978-1-292-02401-1},
	shorttitle = {Object-oriented software engineering},
	language = {eng},
	publisher = {Pearson},
	author = {Brügge, Bernd and Dutoit, Allen Henry},
	year = {2014},
}

@misc{noauthor_n8n_2023,
	title = {n8n - {Workflow} automation tool},
	url = {https://github.com/n8n-io/n8n},
	abstract = {Free and source-available fair-code licensed workflow automation tool. Easily automate tasks across different services.},
	urldate = {2023-03-19},
	publisher = {n8n - Workflow Automation},
	month = mar,
	year = {2023},
	note = {original-date: 2019-06-22T09:24:21Z},
	keywords = {apis, automated, automation, cli, data-flow, development, docker, iaas, integration-framework, integrations, ipaas, low-code, low-code-development-platform, low-code-platform, n8n, node, self-hosted, typescript, workflow, workflow-automation},
}

@inproceedings{krusche_artemis_2018,
	address = {Baltimore Maryland USA},
	title = {{ArTEMiS}: {An} {Automatic} {Assessment} {Management} {System} for {Interactive} {Learning}},
	isbn = {978-1-4503-5103-4},
	shorttitle = {{ArTEMiS}},
	url = {https://dl.acm.org/doi/10.1145/3159450.3159602},
	doi = {10.1145/3159450.3159602},
	language = {en},
	urldate = {2023-03-19},
	booktitle = {Proceedings of the 49th {ACM} {Technical} {Symposium} on {Computer} {Science} {Education}},
	publisher = {ACM},
	author = {Krusche, Stephan and Seitz, Andreas},
	month = feb,
	year = {2018},
	pages = {284--289},
}

@misc{noauthor_jacoco_nodate,
	title = {{JaCoCo} {Java} {Code} {Coverage} {Library}},
	url = {https://www.jacoco.org/},
	urldate = {2023-03-19},
	journal = {JaCoCo Java Code Coverage Library},
	file = {EclEmma - Java Code Coverage for Eclipse:C\:\\Users\\hedde\\Zotero\\storage\\LC555TF8\\www.jacoco.org.html:text/html},
}

@misc{noauthor_cypress-iocypress_2023,
	title = {cypress-io/cypress},
	copyright = {MIT},
	url = {https://github.com/cypress-io/cypress},
	abstract = {Fast, easy and reliable testing for anything that runs in a browser.},
	urldate = {2023-03-19},
	publisher = {Cypress.io},
	month = mar,
	year = {2023},
	note = {original-date: 2015-03-04T00:46:28Z},
	keywords = {cypress, e2e-testing, e2e-tests, end-to-end-testing, javascript-tests, test, test-automation, test-runner, test-suite, testing, testing-tools, tests},
}
